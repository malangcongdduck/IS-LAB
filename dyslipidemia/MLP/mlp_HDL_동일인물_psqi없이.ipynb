{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FONT 깨질때 폰트깨질때\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname = \"C:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font',family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sample_ID GROUP  COHORT SEX  AGE     Trait FitbitOX      Fitbit_ID  HTN  \\\n",
      "0         S0001   SMI       2   M   60  fitbit_O        O  sevrance00001  NaN   \n",
      "1         S0002   SMI       2   M   61  fitbit_O        O  sevrance00002  NaN   \n",
      "2         S0003   SMI       2   F   52  fitbit_O        O  sevrance00003  NaN   \n",
      "3         S0004   SMI       2   F   41  fitbit_O        O  sevrance00004  NaN   \n",
      "4         S0005   SMI       2   F   41  fitbit_O        O  sevrance00005  NaN   \n",
      "..          ...   ...     ...  ..  ...       ...      ...            ...  ...   \n",
      "383  MetS_S0280  MetS       1   F   24  fitbit_O        O   gnfmmets+139  NaN   \n",
      "384  MetS_S0281  MetS       1   F   44  fitbit_O        O   gnfmmets+140  NaN   \n",
      "385  MetS_S0282  MetS       1   F   37  fitbit_O        O   gnfmmets+141  1.0   \n",
      "386  MetS_S0283  MetS       1   M   51  fitbit_X        X              X  NaN   \n",
      "387  MetS_S0284  MetS       1   F   42  fitbit_X        X              X  NaN   \n",
      "\n",
      "      DM  ...  BDI_Q13_2 BDI_Q14_2  BDI_Q15_2  BDI_Q16_2 BDI_Q17_2 BDI_Q18_2  \\\n",
      "0    NaN  ...        1.0       1.0        1.0        2.0       1.0       1.0   \n",
      "1    1.0  ...        1.0       1.0        1.0        1.0       2.0       1.0   \n",
      "2    NaN  ...        1.0       1.0        1.0        1.0       1.0       1.0   \n",
      "3    NaN  ...        2.0       2.0        2.0        2.0       2.0       2.0   \n",
      "4    NaN  ...        2.0       1.0        2.0        2.0       2.0       1.0   \n",
      "..   ...  ...        ...       ...        ...        ...       ...       ...   \n",
      "383  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "384  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "385  1.0  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "386  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "387  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "\n",
      "     BDI_Q19_2  BDI_Q20_2  BDI_Q21_2  Diet_2  \n",
      "0          2.0        1.0        2.0     2.0  \n",
      "1          1.0        1.0        1.0     2.0  \n",
      "2          1.0        2.0        4.0     1.0  \n",
      "3          1.0        1.0        1.0     2.0  \n",
      "4          1.0        2.0        1.0     2.0  \n",
      "..         ...        ...        ...     ...  \n",
      "383        NaN        NaN        NaN     NaN  \n",
      "384        NaN        NaN        NaN     NaN  \n",
      "385        NaN        NaN        NaN     NaN  \n",
      "386        NaN        NaN        NaN     NaN  \n",
      "387        NaN        NaN        NaN     NaN  \n",
      "\n",
      "[388 rows x 3527 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('./최종 데이터 그래프그리기용.xlsx') \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df=df[['Sample_ID','GROUP','COHORT','AGE','SEX','BMI_1','PSQI_TOTAL_1','Insulin _1','CRP_1','WBC_1','Neutrophil_1','Lym_1','GLU0_1','Creatinine_1','AST_1','ALT_1','TG_1','LDL_1','Muscle_1','Fat_1_x','FatPercentage _1','WHR_1','SBP_1',\n",
    "            'DBP_1','HR_1','Waist_1','HDL_1','BUN_1','Chol_1',\n",
    "          'BMI_2','PSQI_TOTAL_2','Insulin _2','CRP_2','WBC_2','Neutrophil_2','Lym_2','GLU0_2',\n",
    "            'Creatinine_2','AST_2','ALT_2','TG_2','LDL_2','Muscle_2','Fat_2_x','FatPercentage_2','WHR_2','SBP_2',\n",
    "            'DBP_2','HR_2','Waist_2','HDL_2','BUN_2','Chol_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>COHORT</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Fat_2_x</th>\n",
       "      <th>FatPercentage_2</th>\n",
       "      <th>WHR_2</th>\n",
       "      <th>SBP_2</th>\n",
       "      <th>DBP_2</th>\n",
       "      <th>HR_2</th>\n",
       "      <th>Waist_2</th>\n",
       "      <th>HDL_2</th>\n",
       "      <th>BUN_2</th>\n",
       "      <th>Chol_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0001</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>M</td>\n",
       "      <td>21.110190</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.91</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>108.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0002</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>27.782064</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.51</td>\n",
       "      <td>...</td>\n",
       "      <td>19.9</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.99</td>\n",
       "      <td>138.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>90.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0003</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>24.944742</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.85</td>\n",
       "      <td>...</td>\n",
       "      <td>22.6</td>\n",
       "      <td>36.7</td>\n",
       "      <td>0.89</td>\n",
       "      <td>127.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>86.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>22.620489</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.14</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.9</td>\n",
       "      <td>0.82</td>\n",
       "      <td>119.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0005</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>20.524157</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.93</td>\n",
       "      <td>...</td>\n",
       "      <td>14.9</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.80</td>\n",
       "      <td>110.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>66.5</td>\n",
       "      <td>72.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>MetS_S0280</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>34.803410</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5.32</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>MetS_S0281</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>F</td>\n",
       "      <td>30.903615</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.82</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>MetS_S0282</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>F</td>\n",
       "      <td>28.676533</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>MetS_S0283</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>M</td>\n",
       "      <td>24.549738</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>6.67</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>MetS_S0284</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>F</td>\n",
       "      <td>24.605921</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.03</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample_ID GROUP  COHORT  AGE SEX      BMI_1  PSQI_TOTAL_1 Insulin _1  \\\n",
       "0         S0001   SMI       2   60   M  21.110190           8.0        7.7   \n",
       "1         S0002   SMI       2   61   M  27.782064           4.0        5.4   \n",
       "2         S0003   SMI       2   52   F  24.944742           3.0        5.1   \n",
       "3         S0004   SMI       2   41   F  22.620489           6.0        4.2   \n",
       "4         S0005   SMI       2   41   F  20.524157          10.0        3.2   \n",
       "..          ...   ...     ...  ...  ..        ...           ...        ...   \n",
       "383  MetS_S0280  MetS       1   24   F  34.803410           5.0       11.3   \n",
       "384  MetS_S0281  MetS       1   44   F  30.903615           3.0       10.6   \n",
       "385  MetS_S0282  MetS       1   37   F  28.676533           3.0       12.2   \n",
       "386  MetS_S0283  MetS       1   51   M  24.549738           5.0       10.4   \n",
       "387  MetS_S0284  MetS       1   42   F  24.605921           3.0       10.1   \n",
       "\n",
       "    CRP_1  WBC_1  ...  Fat_2_x  FatPercentage_2  WHR_2  SBP_2  DBP_2  HR_2  \\\n",
       "0     0.2   3.91  ...      9.7             15.9   0.89  108.0   78.0  87.0   \n",
       "1     0.2   5.51  ...     19.9             27.9   0.99  138.0   92.0  73.0   \n",
       "2     0.7   4.85  ...     22.6             36.7   0.89  127.0   80.0  66.0   \n",
       "3     0.6   6.14  ...     16.0             30.9   0.82  119.0   83.0  77.0   \n",
       "4     0.1   4.93  ...     14.9             26.8   0.80  110.0   68.0  67.0   \n",
       "..    ...    ...  ...      ...              ...    ...    ...    ...   ...   \n",
       "383   0.4   5.32  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "384   2.3   5.82  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "385     1   6.18  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "386   1.2   6.67  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "387   0.8   7.03  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "\n",
       "     Waist_2  HDL_2  BUN_2  Chol_2  \n",
       "0       83.0   77.0   13.1     NaN  \n",
       "1       90.5   59.0   19.2     NaN  \n",
       "2       86.5   40.0   17.1     NaN  \n",
       "3       77.0   54.0   12.2     NaN  \n",
       "4       66.5   72.0   16.5     NaN  \n",
       "..       ...    ...    ...     ...  \n",
       "383      NaN    NaN    NaN     NaN  \n",
       "384      NaN    NaN    NaN     NaN  \n",
       "385      NaN    NaN    NaN     NaN  \n",
       "386      NaN    NaN    NaN     NaN  \n",
       "387      NaN    NaN    NaN     NaN  \n",
       "\n",
       "[388 rows x 53 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>COHORT</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Fat_2_x</th>\n",
       "      <th>FatPercentage_2</th>\n",
       "      <th>WHR_2</th>\n",
       "      <th>SBP_2</th>\n",
       "      <th>DBP_2</th>\n",
       "      <th>HR_2</th>\n",
       "      <th>Waist_2</th>\n",
       "      <th>HDL_2</th>\n",
       "      <th>BUN_2</th>\n",
       "      <th>Chol_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0001</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>M</td>\n",
       "      <td>21.110190</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.91</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>108.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0002</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>27.782064</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.51</td>\n",
       "      <td>...</td>\n",
       "      <td>19.9</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.99</td>\n",
       "      <td>138.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>90.5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0003</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>24.944742</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.85</td>\n",
       "      <td>...</td>\n",
       "      <td>22.6</td>\n",
       "      <td>36.7</td>\n",
       "      <td>0.89</td>\n",
       "      <td>127.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>86.5</td>\n",
       "      <td>40.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>22.620489</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.14</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.9</td>\n",
       "      <td>0.82</td>\n",
       "      <td>119.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0005</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>20.524157</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.93</td>\n",
       "      <td>...</td>\n",
       "      <td>14.9</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.80</td>\n",
       "      <td>110.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>66.5</td>\n",
       "      <td>72.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>MetS_S0280</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>34.803410</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5.32</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>MetS_S0281</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>F</td>\n",
       "      <td>30.903615</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.82</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>MetS_S0282</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>F</td>\n",
       "      <td>28.676533</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>MetS_S0283</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>M</td>\n",
       "      <td>24.549738</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>6.67</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>MetS_S0284</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>F</td>\n",
       "      <td>24.605921</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.03</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample_ID GROUP  COHORT  AGE SEX      BMI_1  PSQI_TOTAL_1 Insulin _1  \\\n",
       "0         S0001   SMI       2   60   M  21.110190           8.0        7.7   \n",
       "1         S0002   SMI       2   61   M  27.782064           4.0        5.4   \n",
       "2         S0003   SMI       2   52   F  24.944742           3.0        5.1   \n",
       "3         S0004   SMI       2   41   F  22.620489           6.0        4.2   \n",
       "4         S0005   SMI       2   41   F  20.524157          10.0        3.2   \n",
       "..          ...   ...     ...  ...  ..        ...           ...        ...   \n",
       "383  MetS_S0280  MetS       1   24   F  34.803410           5.0       11.3   \n",
       "384  MetS_S0281  MetS       1   44   F  30.903615           3.0       10.6   \n",
       "385  MetS_S0282  MetS       1   37   F  28.676533           3.0       12.2   \n",
       "386  MetS_S0283  MetS       1   51   M  24.549738           5.0       10.4   \n",
       "387  MetS_S0284  MetS       1   42   F  24.605921           3.0       10.1   \n",
       "\n",
       "    CRP_1  WBC_1  ...  Fat_2_x  FatPercentage_2  WHR_2  SBP_2  DBP_2  HR_2  \\\n",
       "0     0.2   3.91  ...      9.7             15.9   0.89  108.0   78.0  87.0   \n",
       "1     0.2   5.51  ...     19.9             27.9   0.99  138.0   92.0  73.0   \n",
       "2     0.7   4.85  ...     22.6             36.7   0.89  127.0   80.0  66.0   \n",
       "3     0.6   6.14  ...     16.0             30.9   0.82  119.0   83.0  77.0   \n",
       "4     0.1   4.93  ...     14.9             26.8   0.80  110.0   68.0  67.0   \n",
       "..    ...    ...  ...      ...              ...    ...    ...    ...   ...   \n",
       "383   0.4   5.32  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "384   2.3   5.82  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "385     1   6.18  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "386   1.2   6.67  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "387   0.8   7.03  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "\n",
       "     Waist_2  HDL_2  BUN_2  Chol_2  \n",
       "0       83.0   77.0   13.1     NaN  \n",
       "1       90.5   59.0   19.2     NaN  \n",
       "2       86.5   40.0   17.1     NaN  \n",
       "3       77.0   54.0   12.2     NaN  \n",
       "4       66.5   72.0   16.5     NaN  \n",
       "..       ...    ...    ...     ...  \n",
       "383      NaN    NaN    NaN     NaN  \n",
       "384      NaN    NaN    NaN     NaN  \n",
       "385      NaN    NaN    NaN     NaN  \n",
       "386      NaN    NaN    NaN     NaN  \n",
       "387      NaN    NaN    NaN     NaN  \n",
       "\n",
       "[317 rows x 53 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#보조 호르몬 요법을 받고 있는 Cohort 3 제거 Filter 적용\n",
    "psqi_df = psqi_df[(psqi_df['COHORT'] != 3)]\n",
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df=psqi_df.dropna()\n",
    "psqi_df.reset_index(drop=True, inplace=True)\n",
    "psqi_df=psqi_df.drop([\"Sample_ID\", \"GROUP\", \"COHORT\"],axis=1)\n",
    "#1분, 매일다름, 정해진간이없음 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df[\"SEX\"] = psqi_df[\"SEX\"].apply(lambda x: 1. if x=='M' else 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df[\"Insulin _1\"] = psqi_df[\"Insulin _1\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)\n",
    "psqi_df[\"Insulin _2\"] = psqi_df[\"Insulin _2\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df[\"CRP_1\"] = psqi_df[\"CRP_1\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)\n",
    "psqi_df[\"CRP_2\"] = psqi_df[\"CRP_2\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>Neutrophil_1</th>\n",
       "      <th>Lym_1</th>\n",
       "      <th>GLU0_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Fat_2_x</th>\n",
       "      <th>FatPercentage_2</th>\n",
       "      <th>WHR_2</th>\n",
       "      <th>SBP_2</th>\n",
       "      <th>DBP_2</th>\n",
       "      <th>HR_2</th>\n",
       "      <th>Waist_2</th>\n",
       "      <th>HDL_2</th>\n",
       "      <th>BUN_2</th>\n",
       "      <th>Chol_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.00000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.366667</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>23.799644</td>\n",
       "      <td>5.105556</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.748889</td>\n",
       "      <td>5.844867</td>\n",
       "      <td>56.086111</td>\n",
       "      <td>34.113333</td>\n",
       "      <td>98.90000</td>\n",
       "      <td>...</td>\n",
       "      <td>19.053333</td>\n",
       "      <td>28.888333</td>\n",
       "      <td>0.862444</td>\n",
       "      <td>114.605556</td>\n",
       "      <td>72.477778</td>\n",
       "      <td>75.644444</td>\n",
       "      <td>81.328889</td>\n",
       "      <td>59.20000</td>\n",
       "      <td>12.984444</td>\n",
       "      <td>190.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.589776</td>\n",
       "      <td>0.461927</td>\n",
       "      <td>4.936177</td>\n",
       "      <td>2.893833</td>\n",
       "      <td>4.105985</td>\n",
       "      <td>1.344157</td>\n",
       "      <td>1.412280</td>\n",
       "      <td>8.502880</td>\n",
       "      <td>7.708889</td>\n",
       "      <td>14.43773</td>\n",
       "      <td>...</td>\n",
       "      <td>6.616151</td>\n",
       "      <td>7.098802</td>\n",
       "      <td>0.071696</td>\n",
       "      <td>13.213544</td>\n",
       "      <td>9.091991</td>\n",
       "      <td>10.306814</td>\n",
       "      <td>10.251265</td>\n",
       "      <td>14.01372</td>\n",
       "      <td>3.508550</td>\n",
       "      <td>32.017358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.231576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.820000</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>63.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>109.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.833309</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>4.857500</td>\n",
       "      <td>50.525000</td>\n",
       "      <td>28.975000</td>\n",
       "      <td>92.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>24.275000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>73.875000</td>\n",
       "      <td>49.00000</td>\n",
       "      <td>10.675000</td>\n",
       "      <td>167.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.422889</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>5.720000</td>\n",
       "      <td>55.950000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>95.50000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>28.450000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>57.00000</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>188.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.502662</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.505000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>6.580000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>102.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>22.125000</td>\n",
       "      <td>33.450000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>77.250000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>69.00000</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>211.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>10.550000</td>\n",
       "      <td>78.400000</td>\n",
       "      <td>55.400000</td>\n",
       "      <td>182.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>46.100000</td>\n",
       "      <td>48.300000</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>116.00000</td>\n",
       "      <td>36.400000</td>\n",
       "      <td>296.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              AGE         SEX       BMI_1  PSQI_TOTAL_1  Insulin _1  \\\n",
       "count  180.000000  180.000000  180.000000    180.000000  180.000000   \n",
       "mean    38.366667    0.305556   23.799644      5.105556    7.700000   \n",
       "std     11.589776    0.461927    4.936177      2.893833    4.105985   \n",
       "min     20.000000    0.000000   15.231576      0.000000    0.100000   \n",
       "25%     29.000000    0.000000   20.833309      3.000000    5.000000   \n",
       "50%     35.500000    0.000000   23.422889      5.000000    6.500000   \n",
       "75%     46.000000    1.000000   25.502662      7.000000    9.505000   \n",
       "max     63.000000    1.000000   67.500000     14.000000   24.700000   \n",
       "\n",
       "            CRP_1       WBC_1  Neutrophil_1       Lym_1     GLU0_1  ...  \\\n",
       "count  180.000000  180.000000    180.000000  180.000000  180.00000  ...   \n",
       "mean     0.748889    5.844867     56.086111   34.113333   98.90000  ...   \n",
       "std      1.344157    1.412280      8.502880    7.708889   14.43773  ...   \n",
       "min      0.000000    2.820000     34.500000   15.100000   63.00000  ...   \n",
       "25%      0.200000    4.857500     50.525000   28.975000   92.00000  ...   \n",
       "50%      0.300000    5.720000     55.950000   34.000000   95.50000  ...   \n",
       "75%      0.700000    6.580000     62.000000   39.000000  102.00000  ...   \n",
       "max     11.100000   10.550000     78.400000   55.400000  182.00000  ...   \n",
       "\n",
       "          Fat_2_x  FatPercentage_2       WHR_2       SBP_2       DBP_2  \\\n",
       "count  180.000000       180.000000  180.000000  180.000000  180.000000   \n",
       "mean    19.053333        28.888333    0.862444  114.605556   72.477778   \n",
       "std      6.616151         7.098802    0.071696   13.213544    9.091991   \n",
       "min      7.700000        11.500000    0.700000   91.000000   57.000000   \n",
       "25%     14.200000        24.275000    0.820000  104.000000   67.000000   \n",
       "50%     17.950000        28.450000    0.850000  114.000000   71.000000   \n",
       "75%     22.125000        33.450000    0.900000  123.000000   77.250000   \n",
       "max     46.100000        48.300000    1.070000  158.000000  107.000000   \n",
       "\n",
       "             HR_2     Waist_2      HDL_2       BUN_2      Chol_2  \n",
       "count  180.000000  180.000000  180.00000  180.000000  180.000000  \n",
       "mean    75.644444   81.328889   59.20000   12.984444  190.922222  \n",
       "std     10.306814   10.251265   14.01372    3.508550   32.017358  \n",
       "min     54.000000   61.000000   29.00000    6.000000  109.000000  \n",
       "25%     68.000000   73.875000   49.00000   10.675000  167.750000  \n",
       "50%     75.000000   80.500000   57.00000   12.700000  188.000000  \n",
       "75%     82.000000   89.000000   69.00000   14.600000  211.000000  \n",
       "max    112.000000  118.000000  116.00000   36.400000  296.000000  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    125\n",
       "1.0     55\n",
       "Name: SEX, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df[\"SEX\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>Neutrophil_1</th>\n",
       "      <th>Lym_1</th>\n",
       "      <th>GLU0_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Fat_2_x</th>\n",
       "      <th>FatPercentage_2</th>\n",
       "      <th>WHR_2</th>\n",
       "      <th>SBP_2</th>\n",
       "      <th>DBP_2</th>\n",
       "      <th>HR_2</th>\n",
       "      <th>Waist_2</th>\n",
       "      <th>HDL_2</th>\n",
       "      <th>BUN_2</th>\n",
       "      <th>Chol_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.097789</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.82</td>\n",
       "      <td>54.6</td>\n",
       "      <td>35.0</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>20.4</td>\n",
       "      <td>26.8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>131.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>88.5</td>\n",
       "      <td>53.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.472213</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.46</td>\n",
       "      <td>44.3</td>\n",
       "      <td>43.7</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>14.5</td>\n",
       "      <td>18.6</td>\n",
       "      <td>0.84</td>\n",
       "      <td>126.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.744827</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.99</td>\n",
       "      <td>51.0</td>\n",
       "      <td>37.8</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>17.8</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>131.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>196.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.616175</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.84</td>\n",
       "      <td>39.1</td>\n",
       "      <td>42.1</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.9</td>\n",
       "      <td>0.78</td>\n",
       "      <td>102.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.437500</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.22</td>\n",
       "      <td>49.3</td>\n",
       "      <td>39.3</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>12.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.80</td>\n",
       "      <td>106.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.259585</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.78</td>\n",
       "      <td>42.3</td>\n",
       "      <td>47.3</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>27.3</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.94</td>\n",
       "      <td>134.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.630719</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.60</td>\n",
       "      <td>51.7</td>\n",
       "      <td>34.6</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>22.1</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.95</td>\n",
       "      <td>113.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>97.5</td>\n",
       "      <td>51.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.641274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.4</td>\n",
       "      <td>6.34</td>\n",
       "      <td>55.9</td>\n",
       "      <td>34.9</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>17.5</td>\n",
       "      <td>29.9</td>\n",
       "      <td>0.84</td>\n",
       "      <td>107.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>80.5</td>\n",
       "      <td>49.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.421366</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.88</td>\n",
       "      <td>40.9</td>\n",
       "      <td>48.0</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>15.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.81</td>\n",
       "      <td>106.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.271653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.28</td>\n",
       "      <td>75.7</td>\n",
       "      <td>15.1</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>9.3</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>104.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AGE  SEX      BMI_1  PSQI_TOTAL_1  Insulin _1  CRP_1  WBC_1  \\\n",
       "0     35  1.0  24.097789           5.0        5.57    0.0   5.82   \n",
       "1     46  1.0  23.472213           5.0        7.35    0.7   5.46   \n",
       "2     32  1.0  23.744827           2.0        9.26    0.4   3.99   \n",
       "3     33  0.0  20.616175           4.0        3.52    0.0   5.84   \n",
       "4     28  0.0  18.437500           3.0        2.86    0.0   4.22   \n",
       "..   ...  ...        ...           ...         ...    ...    ...   \n",
       "175   63  0.0  26.259585           3.0        4.20    0.2   4.78   \n",
       "176   57  1.0  28.630719           4.0        8.80    3.0   4.60   \n",
       "177   35  0.0  21.641274           1.0        6.30    0.4   6.34   \n",
       "178   61  0.0  20.421366           8.0        4.80    0.2   4.88   \n",
       "179   56  1.0  22.271653           1.0        9.00    0.2   6.28   \n",
       "\n",
       "     Neutrophil_1  Lym_1  GLU0_1  ...  Fat_2_x  FatPercentage_2  WHR_2  SBP_2  \\\n",
       "0            54.6   35.0      89  ...     20.4             26.8   1.00  131.0   \n",
       "1            44.3   43.7      90  ...     14.5             18.6   0.84  126.0   \n",
       "2            51.0   37.8      96  ...     17.8             25.6   0.89  131.0   \n",
       "3            39.1   42.1      81  ...     12.8             21.9   0.78  102.0   \n",
       "4            49.3   39.3      63  ...     12.3             25.6   0.80  106.0   \n",
       "..            ...    ...     ...  ...      ...              ...    ...    ...   \n",
       "175          42.3   47.3      96  ...     27.3             39.3   0.94  134.0   \n",
       "176          51.7   34.6      94  ...     22.1             25.7   0.95  113.0   \n",
       "177          55.9   34.9      87  ...     17.5             29.9   0.84  107.0   \n",
       "178          40.9   48.0      93  ...     15.3             29.0   0.81  106.0   \n",
       "179          75.7   15.1     125  ...      9.3             13.1   0.85  104.0   \n",
       "\n",
       "     DBP_2   HR_2  Waist_2  HDL_2  BUN_2  Chol_2  \n",
       "0     74.0   66.0     88.5   53.0   17.5   180.0  \n",
       "1     87.0  108.0     85.0   64.0   14.4   203.0  \n",
       "2     77.0   87.0     81.0   49.0   14.1   196.0  \n",
       "3     62.0   70.0     69.0   98.0   10.5   224.0  \n",
       "4     72.0   69.0     61.0   71.0   11.3   168.0  \n",
       "..     ...    ...      ...    ...    ...     ...  \n",
       "175   89.0   81.0     98.0   66.0   17.1   141.0  \n",
       "176   76.0   66.0     97.5   51.0   14.6   134.0  \n",
       "177   72.0   64.0     80.5   49.0    9.7   147.0  \n",
       "178   76.0   92.0     79.0   60.0   10.2   134.0  \n",
       "179   73.0   79.0     91.0   31.0   36.4   148.0  \n",
       "\n",
       "[180 rows x 50 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x 배열 생성 (x=AGE, SEX, PSQI, BMI)\n",
    "X1=psqi_df[['AGE','SEX','BMI_1','Insulin _1','CRP_1','WBC_1','Neutrophil_1','Lym_1','GLU0_1',\n",
    "            'Creatinine_1','AST_1','ALT_1','TG_1','LDL_1','Muscle_1','Fat_1_x','FatPercentage _1','WHR_1','SBP_1',\n",
    "            'DBP_1','HR_1','Waist_1','BUN_1','Chol_1']]\n",
    "X2=psqi_df[['AGE','SEX','BMI_2','Insulin _2','CRP_2','WBC_2','Neutrophil_2','Lym_2','GLU0_2',\n",
    "            'Creatinine_2','AST_2','ALT_2','TG_2','LDL_2','Muscle_2','Fat_2_x','FatPercentage_2','WHR_2','SBP_2',\n",
    "            'DBP_2','HR_2','Waist_2','BUN_2','Chol_2']]\n",
    "X=np.concatenate((X1, X2), axis=0)\n",
    "\n",
    "\n",
    "#y 배열 생성 (y=HDL)\n",
    "Y1= psqi_df[['HDL_1']].values\n",
    "Y2= psqi_df[['HDL_2']].values\n",
    "Y=np.concatenate((Y1, Y2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 360)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((360, 24), (360, 1))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규화 (변수간의 스케일 차이)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.asarray(X).astype(np.float)\n",
    "Y=np.asarray(Y).astype(np.float)\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,train_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 72)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((360, 24), (360, 1))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X.shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "72/72 - 0s - loss: 3479.1494 - mse: 3479.1494\n",
      "Epoch 2/150\n",
      "72/72 - 0s - loss: 2809.2605 - mse: 2809.2605\n",
      "Epoch 3/150\n",
      "72/72 - 0s - loss: 1752.9126 - mse: 1752.9126\n",
      "Epoch 4/150\n",
      "72/72 - 0s - loss: 759.3361 - mse: 759.3361\n",
      "Epoch 5/150\n",
      "72/72 - 0s - loss: 335.3536 - mse: 335.3536\n",
      "Epoch 6/150\n",
      "72/72 - 0s - loss: 274.1880 - mse: 274.1880\n",
      "Epoch 7/150\n",
      "72/72 - 0s - loss: 237.4617 - mse: 237.4617\n",
      "Epoch 8/150\n",
      "72/72 - 0s - loss: 209.5995 - mse: 209.5995\n",
      "Epoch 9/150\n",
      "72/72 - 0s - loss: 187.7410 - mse: 187.7410\n",
      "Epoch 10/150\n",
      "72/72 - 0s - loss: 169.1682 - mse: 169.1682\n",
      "Epoch 11/150\n",
      "72/72 - 0s - loss: 154.1734 - mse: 154.1734\n",
      "Epoch 12/150\n",
      "72/72 - 0s - loss: 143.0615 - mse: 143.0615\n",
      "Epoch 13/150\n",
      "72/72 - 0s - loss: 131.5415 - mse: 131.5415\n",
      "Epoch 14/150\n",
      "72/72 - 0s - loss: 122.8230 - mse: 122.8230\n",
      "Epoch 15/150\n",
      "72/72 - 0s - loss: 113.0283 - mse: 113.0283\n",
      "Epoch 16/150\n",
      "72/72 - 0s - loss: 104.5149 - mse: 104.5149\n",
      "Epoch 17/150\n",
      "72/72 - 0s - loss: 98.0532 - mse: 98.0532\n",
      "Epoch 18/150\n",
      "72/72 - 0s - loss: 91.7282 - mse: 91.7282\n",
      "Epoch 19/150\n",
      "72/72 - 0s - loss: 85.3124 - mse: 85.3124\n",
      "Epoch 20/150\n",
      "72/72 - 0s - loss: 80.0672 - mse: 80.0672\n",
      "Epoch 21/150\n",
      "72/72 - 0s - loss: 73.9570 - mse: 73.9570\n",
      "Epoch 22/150\n",
      "72/72 - 0s - loss: 68.2403 - mse: 68.2403\n",
      "Epoch 23/150\n",
      "72/72 - 0s - loss: 65.7799 - mse: 65.7799\n",
      "Epoch 24/150\n",
      "72/72 - 0s - loss: 62.1540 - mse: 62.1540\n",
      "Epoch 25/150\n",
      "72/72 - 0s - loss: 58.3877 - mse: 58.3877\n",
      "Epoch 26/150\n",
      "72/72 - 0s - loss: 55.1762 - mse: 55.1762\n",
      "Epoch 27/150\n",
      "72/72 - 0s - loss: 52.2075 - mse: 52.2075\n",
      "Epoch 28/150\n",
      "72/72 - 0s - loss: 50.0004 - mse: 50.0004\n",
      "Epoch 29/150\n",
      "72/72 - 0s - loss: 47.2496 - mse: 47.2496\n",
      "Epoch 30/150\n",
      "72/72 - 0s - loss: 44.6305 - mse: 44.6305\n",
      "Epoch 31/150\n",
      "72/72 - 0s - loss: 42.3819 - mse: 42.3819\n",
      "Epoch 32/150\n",
      "72/72 - 0s - loss: 41.8090 - mse: 41.8090\n",
      "Epoch 33/150\n",
      "72/72 - 0s - loss: 39.1924 - mse: 39.1924\n",
      "Epoch 34/150\n",
      "72/72 - 0s - loss: 37.9175 - mse: 37.9175\n",
      "Epoch 35/150\n",
      "72/72 - 0s - loss: 36.4483 - mse: 36.4483\n",
      "Epoch 36/150\n",
      "72/72 - 0s - loss: 35.5519 - mse: 35.5519\n",
      "Epoch 37/150\n",
      "72/72 - 0s - loss: 34.4011 - mse: 34.4011\n",
      "Epoch 38/150\n",
      "72/72 - 0s - loss: 33.1816 - mse: 33.1816\n",
      "Epoch 39/150\n",
      "72/72 - 0s - loss: 32.5491 - mse: 32.5491\n",
      "Epoch 40/150\n",
      "72/72 - 0s - loss: 31.9778 - mse: 31.9778\n",
      "Epoch 41/150\n",
      "72/72 - 0s - loss: 31.0961 - mse: 31.0961\n",
      "Epoch 42/150\n",
      "72/72 - 0s - loss: 29.5989 - mse: 29.5989\n",
      "Epoch 43/150\n",
      "72/72 - 0s - loss: 29.9304 - mse: 29.9304\n",
      "Epoch 44/150\n",
      "72/72 - 0s - loss: 29.0828 - mse: 29.0828\n",
      "Epoch 45/150\n",
      "72/72 - 0s - loss: 27.3177 - mse: 27.3177\n",
      "Epoch 46/150\n",
      "72/72 - 0s - loss: 27.7042 - mse: 27.7042\n",
      "Epoch 47/150\n",
      "72/72 - 0s - loss: 27.4356 - mse: 27.4356\n",
      "Epoch 48/150\n",
      "72/72 - 0s - loss: 27.0178 - mse: 27.0178\n",
      "Epoch 49/150\n",
      "72/72 - 0s - loss: 26.3012 - mse: 26.3012\n",
      "Epoch 50/150\n",
      "72/72 - 0s - loss: 25.8559 - mse: 25.8559\n",
      "Epoch 51/150\n",
      "72/72 - 0s - loss: 25.3347 - mse: 25.3347\n",
      "Epoch 52/150\n",
      "72/72 - 0s - loss: 24.8525 - mse: 24.8525\n",
      "Epoch 53/150\n",
      "72/72 - 0s - loss: 24.2466 - mse: 24.2466\n",
      "Epoch 54/150\n",
      "72/72 - 0s - loss: 23.4441 - mse: 23.4441\n",
      "Epoch 55/150\n",
      "72/72 - 0s - loss: 23.8306 - mse: 23.8306\n",
      "Epoch 56/150\n",
      "72/72 - 0s - loss: 23.4502 - mse: 23.4502\n",
      "Epoch 57/150\n",
      "72/72 - 0s - loss: 23.0075 - mse: 23.0075\n",
      "Epoch 58/150\n",
      "72/72 - 0s - loss: 22.4496 - mse: 22.4496\n",
      "Epoch 59/150\n",
      "72/72 - 0s - loss: 22.0677 - mse: 22.0677\n",
      "Epoch 60/150\n",
      "72/72 - 0s - loss: 21.9987 - mse: 21.9987\n",
      "Epoch 61/150\n",
      "72/72 - 0s - loss: 22.0307 - mse: 22.0307\n",
      "Epoch 62/150\n",
      "72/72 - 0s - loss: 21.4365 - mse: 21.4365\n",
      "Epoch 63/150\n",
      "72/72 - 0s - loss: 20.9041 - mse: 20.9041\n",
      "Epoch 64/150\n",
      "72/72 - 0s - loss: 20.8285 - mse: 20.8285\n",
      "Epoch 65/150\n",
      "72/72 - 0s - loss: 20.4370 - mse: 20.4370\n",
      "Epoch 66/150\n",
      "72/72 - 0s - loss: 20.1425 - mse: 20.1425\n",
      "Epoch 67/150\n",
      "72/72 - 0s - loss: 19.2123 - mse: 19.2123\n",
      "Epoch 68/150\n",
      "72/72 - 0s - loss: 19.8677 - mse: 19.8677\n",
      "Epoch 69/150\n",
      "72/72 - 0s - loss: 18.9957 - mse: 18.9957\n",
      "Epoch 70/150\n",
      "72/72 - 0s - loss: 19.4869 - mse: 19.4869\n",
      "Epoch 71/150\n",
      "72/72 - 0s - loss: 19.1246 - mse: 19.1246\n",
      "Epoch 72/150\n",
      "72/72 - 0s - loss: 18.7538 - mse: 18.7538\n",
      "Epoch 73/150\n",
      "72/72 - 0s - loss: 19.0229 - mse: 19.0229\n",
      "Epoch 74/150\n",
      "72/72 - 0s - loss: 18.4587 - mse: 18.4587\n",
      "Epoch 75/150\n",
      "72/72 - 0s - loss: 17.9202 - mse: 17.9202\n",
      "Epoch 76/150\n",
      "72/72 - 0s - loss: 18.2449 - mse: 18.2449\n",
      "Epoch 77/150\n",
      "72/72 - 0s - loss: 17.8161 - mse: 17.8161\n",
      "Epoch 78/150\n",
      "72/72 - 0s - loss: 17.4257 - mse: 17.4257\n",
      "Epoch 79/150\n",
      "72/72 - 0s - loss: 17.4998 - mse: 17.4998\n",
      "Epoch 80/150\n",
      "72/72 - 0s - loss: 17.3297 - mse: 17.3297\n",
      "Epoch 81/150\n",
      "72/72 - 0s - loss: 16.6784 - mse: 16.6784\n",
      "Epoch 82/150\n",
      "72/72 - 0s - loss: 16.5822 - mse: 16.5822\n",
      "Epoch 83/150\n",
      "72/72 - 0s - loss: 16.6585 - mse: 16.6585\n",
      "Epoch 84/150\n",
      "72/72 - 0s - loss: 16.6093 - mse: 16.6093\n",
      "Epoch 85/150\n",
      "72/72 - 0s - loss: 16.2475 - mse: 16.2475\n",
      "Epoch 86/150\n",
      "72/72 - 0s - loss: 15.8244 - mse: 15.8244\n",
      "Epoch 87/150\n",
      "72/72 - 0s - loss: 15.5879 - mse: 15.5879\n",
      "Epoch 88/150\n",
      "72/72 - 0s - loss: 15.5759 - mse: 15.5759\n",
      "Epoch 89/150\n",
      "72/72 - 0s - loss: 15.4625 - mse: 15.4625\n",
      "Epoch 90/150\n",
      "72/72 - 0s - loss: 15.2446 - mse: 15.2446\n",
      "Epoch 91/150\n",
      "72/72 - 0s - loss: 15.0461 - mse: 15.0461\n",
      "Epoch 92/150\n",
      "72/72 - 0s - loss: 14.8521 - mse: 14.8521\n",
      "Epoch 93/150\n",
      "72/72 - 0s - loss: 14.2074 - mse: 14.2074\n",
      "Epoch 94/150\n",
      "72/72 - 0s - loss: 14.5661 - mse: 14.5661\n",
      "Epoch 95/150\n",
      "72/72 - 0s - loss: 14.2666 - mse: 14.2666\n",
      "Epoch 96/150\n",
      "72/72 - 0s - loss: 13.8535 - mse: 13.8535\n",
      "Epoch 97/150\n",
      "72/72 - 0s - loss: 14.0812 - mse: 14.0812\n",
      "Epoch 98/150\n",
      "72/72 - 0s - loss: 13.7123 - mse: 13.7123\n",
      "Epoch 99/150\n",
      "72/72 - 0s - loss: 13.5983 - mse: 13.5983\n",
      "Epoch 100/150\n",
      "72/72 - 0s - loss: 13.2198 - mse: 13.2198\n",
      "Epoch 101/150\n",
      "72/72 - 0s - loss: 13.2750 - mse: 13.2750\n",
      "Epoch 102/150\n",
      "72/72 - 0s - loss: 12.9436 - mse: 12.9436\n",
      "Epoch 103/150\n",
      "72/72 - 0s - loss: 12.6676 - mse: 12.6676\n",
      "Epoch 104/150\n",
      "72/72 - 0s - loss: 12.6228 - mse: 12.6228\n",
      "Epoch 105/150\n",
      "72/72 - 0s - loss: 12.6244 - mse: 12.6244\n",
      "Epoch 106/150\n",
      "72/72 - 0s - loss: 12.4456 - mse: 12.4456\n",
      "Epoch 107/150\n",
      "72/72 - 0s - loss: 12.3787 - mse: 12.3787\n",
      "Epoch 108/150\n",
      "72/72 - 0s - loss: 11.7652 - mse: 11.7652\n",
      "Epoch 109/150\n",
      "72/72 - 0s - loss: 11.7839 - mse: 11.7839\n",
      "Epoch 110/150\n",
      "72/72 - 0s - loss: 11.5385 - mse: 11.5385\n",
      "Epoch 111/150\n",
      "72/72 - 0s - loss: 11.6597 - mse: 11.6597\n",
      "Epoch 112/150\n",
      "72/72 - 0s - loss: 11.4675 - mse: 11.4675\n",
      "Epoch 113/150\n",
      "72/72 - 0s - loss: 11.5538 - mse: 11.5538\n",
      "Epoch 114/150\n",
      "72/72 - 0s - loss: 11.3818 - mse: 11.3818\n",
      "Epoch 115/150\n",
      "72/72 - 0s - loss: 10.8366 - mse: 10.8366\n",
      "Epoch 116/150\n",
      "72/72 - 0s - loss: 11.2461 - mse: 11.2461\n",
      "Epoch 117/150\n",
      "72/72 - 0s - loss: 10.7499 - mse: 10.7499\n",
      "Epoch 118/150\n",
      "72/72 - 0s - loss: 10.6872 - mse: 10.6872\n",
      "Epoch 119/150\n",
      "72/72 - 0s - loss: 10.4495 - mse: 10.4495\n",
      "Epoch 120/150\n",
      "72/72 - 0s - loss: 10.3095 - mse: 10.3095\n",
      "Epoch 121/150\n",
      "72/72 - 0s - loss: 10.3657 - mse: 10.3657\n",
      "Epoch 122/150\n",
      "72/72 - 0s - loss: 9.7434 - mse: 9.7434\n",
      "Epoch 123/150\n",
      "72/72 - 0s - loss: 10.0009 - mse: 10.0009\n",
      "Epoch 124/150\n",
      "72/72 - 0s - loss: 9.3568 - mse: 9.3568\n",
      "Epoch 125/150\n",
      "72/72 - 0s - loss: 9.8436 - mse: 9.8436\n",
      "Epoch 126/150\n",
      "72/72 - 0s - loss: 9.6201 - mse: 9.6201\n",
      "Epoch 127/150\n",
      "72/72 - 0s - loss: 9.5676 - mse: 9.5676\n",
      "Epoch 128/150\n",
      "72/72 - 0s - loss: 9.1095 - mse: 9.1095\n",
      "Epoch 129/150\n",
      "72/72 - 0s - loss: 8.9307 - mse: 8.9307\n",
      "Epoch 130/150\n",
      "72/72 - 0s - loss: 9.0276 - mse: 9.0276\n",
      "Epoch 131/150\n",
      "72/72 - 0s - loss: 8.4498 - mse: 8.4498\n",
      "Epoch 132/150\n",
      "72/72 - 0s - loss: 8.6536 - mse: 8.6536\n",
      "Epoch 133/150\n",
      "72/72 - 0s - loss: 8.9010 - mse: 8.9010\n",
      "Epoch 134/150\n",
      "72/72 - 0s - loss: 8.4721 - mse: 8.4721\n",
      "Epoch 135/150\n",
      "72/72 - 0s - loss: 8.4089 - mse: 8.4089\n",
      "Epoch 136/150\n",
      "72/72 - 0s - loss: 8.1574 - mse: 8.1574\n",
      "Epoch 137/150\n",
      "72/72 - 0s - loss: 8.0747 - mse: 8.0747\n",
      "Epoch 138/150\n",
      "72/72 - 0s - loss: 8.1137 - mse: 8.1137\n",
      "Epoch 139/150\n",
      "72/72 - 0s - loss: 7.6177 - mse: 7.6177\n",
      "Epoch 140/150\n",
      "72/72 - 0s - loss: 8.0914 - mse: 8.0914\n",
      "Epoch 141/150\n",
      "72/72 - 0s - loss: 7.3963 - mse: 7.3963\n",
      "Epoch 142/150\n",
      "72/72 - 0s - loss: 7.6175 - mse: 7.6175\n",
      "Epoch 143/150\n",
      "72/72 - 0s - loss: 7.6598 - mse: 7.6598\n",
      "Epoch 144/150\n",
      "72/72 - 0s - loss: 7.0649 - mse: 7.0649\n",
      "Epoch 145/150\n",
      "72/72 - 0s - loss: 7.1913 - mse: 7.1913\n",
      "Epoch 146/150\n",
      "72/72 - 0s - loss: 7.1232 - mse: 7.1232\n",
      "Epoch 147/150\n",
      "72/72 - 0s - loss: 7.3604 - mse: 7.3604\n",
      "Epoch 148/150\n",
      "72/72 - 0s - loss: 6.7476 - mse: 6.7476\n",
      "Epoch 149/150\n",
      "72/72 - 0s - loss: 6.5168 - mse: 6.5168\n",
      "Epoch 150/150\n",
      "72/72 - 0s - loss: 6.7167 - mse: 6.7167\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 122.3363 - mse: 122.3363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[122.33626556396484, 122.33626556396484]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define model\n",
    "model=Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=dim))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='rmsprop',metrics=['mse'])\n",
    "\n",
    "#fit model\n",
    "history=model.fit(x_train, y_train, epochs=150, batch_size=4, verbose=2)\n",
    "model.evaluate(x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                800       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,889\n",
      "Trainable params: 1,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_p = model.predict(x_train)\n",
    "y_test_p = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_train,y_train_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_test,y_test_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list=np.array(y_train).flatten().tolist() #y_train 리스트\n",
    "y_test_list=np.array(y_test).flatten().tolist() #y_test 리스트\n",
    "y_p_train_list=np.array(y_train_p).flatten().tolist() #y_train 예측 리스트\n",
    "y_p_test_list=np.array(y_test_p).flatten().tolist() #y_test 예측 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#오차 범위 3 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set prediction accuracy: 50.35 %\n",
      "test set prediction accuracy: 56.94 %\n"
     ]
    }
   ],
   "source": [
    "#평균 성능 테스트\n",
    "scores = 0\n",
    "mean=np.mean(Y, axis=0)\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= mean <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"train set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= mean <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"test set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### <오차범위 3>\n",
      "- train set prediction accuracy(+-3): 81.25 % <br>\n",
      "- test set prediction accuracy(+-3): 34.72 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 5>\n",
      "- train set prediction accuracy(+-5): 95.49 % <br>\n",
      "- test set prediction accuracy(+-5): 51.39 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 10>\n",
      "- train set prediction accuracy(+-10): 99.65 % <br>\n",
      "- test set prediction accuracy(+-10): 83.33 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 20>\n",
      "- train set prediction accuracy(+-20): 100.00 % <br>\n",
      "- test set prediction accuracy(+-20): 97.22 % <br>\n"
     ]
    }
   ],
   "source": [
    "######입력용#######\n",
    "\n",
    "#오차 범위 3 설정\n",
    "print('### <오차범위 3>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "print('### <오차범위 5>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "print('### <오차범위 10>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "print('### <오차범위 20>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x 배열 생성 (선별)\n",
    "X1=psqi_df[['SEX','AGE','BMI_1','WBC_1','GLU0_1','ALT_1','TG_1','LDL_1',\n",
    "            'Muscle_1','Fat_1_x','SBP_1','DBP_1','HR_1','Waist_1','FatPercentage _1']].values\n",
    "X2=psqi_df[['SEX','AGE','BMI_2','WBC_2','GLU0_2','ALT_2','TG_2','LDL_2',\n",
    "            'Muscle_2','Fat_2_x','SBP_2','DBP_2','HR_2','Waist_2','FatPercentage_2']].values\n",
    "X=np.concatenate((X1, X2), axis=0)\n",
    "\n",
    "#y 배열 생성 (y=HDL)\n",
    "Y1= psqi_df[['HDL_1']].values\n",
    "Y2= psqi_df[['HDL_2']].values\n",
    "Y=np.concatenate((Y1, Y2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규화 (변수간의 스케일 차이)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.asarray(X).astype(np.float)\n",
    "Y=np.asarray(Y).astype(np.float)\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,train_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 72)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((360, 15), (360, 1))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X.shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "72/72 - 0s - loss: 3522.6057 - mse: 3522.6057\n",
      "Epoch 2/150\n",
      "72/72 - 0s - loss: 2991.9578 - mse: 2991.9578\n",
      "Epoch 3/150\n",
      "72/72 - 0s - loss: 2078.3640 - mse: 2078.3640\n",
      "Epoch 4/150\n",
      "72/72 - 0s - loss: 1022.8870 - mse: 1022.8870\n",
      "Epoch 5/150\n",
      "72/72 - 0s - loss: 434.3903 - mse: 434.3903\n",
      "Epoch 6/150\n",
      "72/72 - 0s - loss: 322.1927 - mse: 322.1927\n",
      "Epoch 7/150\n",
      "72/72 - 0s - loss: 273.8774 - mse: 273.8774\n",
      "Epoch 8/150\n",
      "72/72 - 0s - loss: 239.2340 - mse: 239.2340\n",
      "Epoch 9/150\n",
      "72/72 - 0s - loss: 213.4657 - mse: 213.4657\n",
      "Epoch 10/150\n",
      "72/72 - 0s - loss: 196.8519 - mse: 196.8519\n",
      "Epoch 11/150\n",
      "72/72 - 0s - loss: 185.3632 - mse: 185.3632\n",
      "Epoch 12/150\n",
      "72/72 - 0s - loss: 176.4215 - mse: 176.4215\n",
      "Epoch 13/150\n",
      "72/72 - 0s - loss: 169.2244 - mse: 169.2244\n",
      "Epoch 14/150\n",
      "72/72 - 0s - loss: 163.8345 - mse: 163.8345\n",
      "Epoch 15/150\n",
      "72/72 - 0s - loss: 159.5625 - mse: 159.5625\n",
      "Epoch 16/150\n",
      "72/72 - 0s - loss: 155.8181 - mse: 155.8181\n",
      "Epoch 17/150\n",
      "72/72 - 0s - loss: 153.0180 - mse: 153.0180\n",
      "Epoch 18/150\n",
      "72/72 - 0s - loss: 150.2150 - mse: 150.2150\n",
      "Epoch 19/150\n",
      "72/72 - 0s - loss: 148.6845 - mse: 148.6845\n",
      "Epoch 20/150\n",
      "72/72 - 0s - loss: 146.3027 - mse: 146.3027\n",
      "Epoch 21/150\n",
      "72/72 - 0s - loss: 144.6540 - mse: 144.6540\n",
      "Epoch 22/150\n",
      "72/72 - 0s - loss: 143.1986 - mse: 143.1986\n",
      "Epoch 23/150\n",
      "72/72 - 0s - loss: 140.1604 - mse: 140.1604\n",
      "Epoch 24/150\n",
      "72/72 - 0s - loss: 139.1260 - mse: 139.1260\n",
      "Epoch 25/150\n",
      "72/72 - 0s - loss: 136.8076 - mse: 136.8076\n",
      "Epoch 26/150\n",
      "72/72 - 0s - loss: 135.8851 - mse: 135.8851\n",
      "Epoch 27/150\n",
      "72/72 - 0s - loss: 134.3276 - mse: 134.3276\n",
      "Epoch 28/150\n",
      "72/72 - 0s - loss: 133.5543 - mse: 133.5543\n",
      "Epoch 29/150\n",
      "72/72 - 0s - loss: 132.0479 - mse: 132.0479\n",
      "Epoch 30/150\n",
      "72/72 - 0s - loss: 129.9593 - mse: 129.9593\n",
      "Epoch 31/150\n",
      "72/72 - 0s - loss: 130.0143 - mse: 130.0143\n",
      "Epoch 32/150\n",
      "72/72 - 0s - loss: 129.5297 - mse: 129.5297\n",
      "Epoch 33/150\n",
      "72/72 - 0s - loss: 126.5929 - mse: 126.5929\n",
      "Epoch 34/150\n",
      "72/72 - 0s - loss: 126.3197 - mse: 126.3197\n",
      "Epoch 35/150\n",
      "72/72 - 0s - loss: 126.2419 - mse: 126.2419\n",
      "Epoch 36/150\n",
      "72/72 - 0s - loss: 124.6394 - mse: 124.6394\n",
      "Epoch 37/150\n",
      "72/72 - 0s - loss: 124.0481 - mse: 124.0481\n",
      "Epoch 38/150\n",
      "72/72 - 0s - loss: 123.4858 - mse: 123.4858\n",
      "Epoch 39/150\n",
      "72/72 - 0s - loss: 122.9803 - mse: 122.9803\n",
      "Epoch 40/150\n",
      "72/72 - 0s - loss: 123.1846 - mse: 123.1846\n",
      "Epoch 41/150\n",
      "72/72 - 0s - loss: 121.0621 - mse: 121.0621\n",
      "Epoch 42/150\n",
      "72/72 - 0s - loss: 121.4712 - mse: 121.4712\n",
      "Epoch 43/150\n",
      "72/72 - 0s - loss: 121.1462 - mse: 121.1462\n",
      "Epoch 44/150\n",
      "72/72 - 0s - loss: 119.5643 - mse: 119.5643\n",
      "Epoch 45/150\n",
      "72/72 - 0s - loss: 117.7588 - mse: 117.7588\n",
      "Epoch 46/150\n",
      "72/72 - 0s - loss: 118.8927 - mse: 118.8927\n",
      "Epoch 47/150\n",
      "72/72 - 0s - loss: 118.8711 - mse: 118.8711\n",
      "Epoch 48/150\n",
      "72/72 - 0s - loss: 117.4606 - mse: 117.4606\n",
      "Epoch 49/150\n",
      "72/72 - 0s - loss: 116.6213 - mse: 116.6213\n",
      "Epoch 50/150\n",
      "72/72 - 0s - loss: 116.7019 - mse: 116.7019\n",
      "Epoch 51/150\n",
      "72/72 - 0s - loss: 115.9649 - mse: 115.9649\n",
      "Epoch 52/150\n",
      "72/72 - 0s - loss: 115.6358 - mse: 115.6358\n",
      "Epoch 53/150\n",
      "72/72 - 0s - loss: 114.8702 - mse: 114.8702\n",
      "Epoch 54/150\n",
      "72/72 - 0s - loss: 114.3252 - mse: 114.3252\n",
      "Epoch 55/150\n",
      "72/72 - 0s - loss: 114.1107 - mse: 114.1107\n",
      "Epoch 56/150\n",
      "72/72 - 0s - loss: 110.3153 - mse: 110.3153\n",
      "Epoch 57/150\n",
      "72/72 - 0s - loss: 113.7580 - mse: 113.7580\n",
      "Epoch 58/150\n",
      "72/72 - 0s - loss: 113.3420 - mse: 113.3420\n",
      "Epoch 59/150\n",
      "72/72 - 0s - loss: 112.5495 - mse: 112.5495\n",
      "Epoch 60/150\n",
      "72/72 - 0s - loss: 111.3821 - mse: 111.3821\n",
      "Epoch 61/150\n",
      "72/72 - 0s - loss: 110.2858 - mse: 110.2858\n",
      "Epoch 62/150\n",
      "72/72 - 0s - loss: 110.6718 - mse: 110.6718\n",
      "Epoch 63/150\n",
      "72/72 - 0s - loss: 110.9586 - mse: 110.9586\n",
      "Epoch 64/150\n",
      "72/72 - 0s - loss: 110.0216 - mse: 110.0216\n",
      "Epoch 65/150\n",
      "72/72 - 0s - loss: 108.3631 - mse: 108.3631\n",
      "Epoch 66/150\n",
      "72/72 - 0s - loss: 107.3041 - mse: 107.3041\n",
      "Epoch 67/150\n",
      "72/72 - 0s - loss: 109.3475 - mse: 109.3475\n",
      "Epoch 68/150\n",
      "72/72 - 0s - loss: 109.1138 - mse: 109.1138\n",
      "Epoch 69/150\n",
      "72/72 - 0s - loss: 108.3567 - mse: 108.3567\n",
      "Epoch 70/150\n",
      "72/72 - 0s - loss: 107.2079 - mse: 107.2079\n",
      "Epoch 71/150\n",
      "72/72 - 0s - loss: 106.8845 - mse: 106.8845\n",
      "Epoch 72/150\n",
      "72/72 - 0s - loss: 107.0940 - mse: 107.0940\n",
      "Epoch 73/150\n",
      "72/72 - 0s - loss: 106.7168 - mse: 106.7168\n",
      "Epoch 74/150\n",
      "72/72 - 0s - loss: 105.8158 - mse: 105.8158\n",
      "Epoch 75/150\n",
      "72/72 - 0s - loss: 104.9943 - mse: 104.9943\n",
      "Epoch 76/150\n",
      "72/72 - 0s - loss: 105.4836 - mse: 105.4836\n",
      "Epoch 77/150\n",
      "72/72 - 0s - loss: 103.0436 - mse: 103.0436\n",
      "Epoch 78/150\n",
      "72/72 - 0s - loss: 104.5151 - mse: 104.5151\n",
      "Epoch 79/150\n",
      "72/72 - 0s - loss: 104.9985 - mse: 104.9985\n",
      "Epoch 80/150\n",
      "72/72 - 0s - loss: 104.1924 - mse: 104.1924\n",
      "Epoch 81/150\n",
      "72/72 - 0s - loss: 102.6972 - mse: 102.6972\n",
      "Epoch 82/150\n",
      "72/72 - 0s - loss: 102.9812 - mse: 102.9812\n",
      "Epoch 83/150\n",
      "72/72 - 0s - loss: 102.9872 - mse: 102.9872\n",
      "Epoch 84/150\n",
      "72/72 - 0s - loss: 100.8659 - mse: 100.8659\n",
      "Epoch 85/150\n",
      "72/72 - 0s - loss: 102.8055 - mse: 102.8055\n",
      "Epoch 86/150\n",
      "72/72 - 0s - loss: 100.5921 - mse: 100.5921\n",
      "Epoch 87/150\n",
      "72/72 - 0s - loss: 101.4283 - mse: 101.4283\n",
      "Epoch 88/150\n",
      "72/72 - 0s - loss: 100.5992 - mse: 100.5992\n",
      "Epoch 89/150\n",
      "72/72 - 0s - loss: 99.6682 - mse: 99.6682\n",
      "Epoch 90/150\n",
      "72/72 - 0s - loss: 100.0760 - mse: 100.0760\n",
      "Epoch 91/150\n",
      "72/72 - 0s - loss: 99.9399 - mse: 99.9399\n",
      "Epoch 92/150\n",
      "72/72 - 0s - loss: 98.8951 - mse: 98.8951\n",
      "Epoch 93/150\n",
      "72/72 - 0s - loss: 99.6588 - mse: 99.6588\n",
      "Epoch 94/150\n",
      "72/72 - 0s - loss: 98.8096 - mse: 98.8096\n",
      "Epoch 95/150\n",
      "72/72 - 0s - loss: 98.4956 - mse: 98.4956\n",
      "Epoch 96/150\n",
      "72/72 - 0s - loss: 98.4099 - mse: 98.4099\n",
      "Epoch 97/150\n",
      "72/72 - 0s - loss: 97.7135 - mse: 97.7135\n",
      "Epoch 98/150\n",
      "72/72 - 0s - loss: 96.0647 - mse: 96.0647\n",
      "Epoch 99/150\n",
      "72/72 - 0s - loss: 97.2176 - mse: 97.2176\n",
      "Epoch 100/150\n",
      "72/72 - 0s - loss: 96.5290 - mse: 96.5290\n",
      "Epoch 101/150\n",
      "72/72 - 0s - loss: 96.2977 - mse: 96.2977\n",
      "Epoch 102/150\n",
      "72/72 - 0s - loss: 95.3136 - mse: 95.3136\n",
      "Epoch 103/150\n",
      "72/72 - 0s - loss: 93.9305 - mse: 93.9305\n",
      "Epoch 104/150\n",
      "72/72 - 0s - loss: 95.8801 - mse: 95.8801\n",
      "Epoch 105/150\n",
      "72/72 - 0s - loss: 94.0010 - mse: 94.0010\n",
      "Epoch 106/150\n",
      "72/72 - 0s - loss: 94.0880 - mse: 94.0880\n",
      "Epoch 107/150\n",
      "72/72 - 0s - loss: 93.6574 - mse: 93.6574\n",
      "Epoch 108/150\n",
      "72/72 - 0s - loss: 94.0459 - mse: 94.0459\n",
      "Epoch 109/150\n",
      "72/72 - 0s - loss: 92.9709 - mse: 92.9709\n",
      "Epoch 110/150\n",
      "72/72 - 0s - loss: 92.7948 - mse: 92.7948\n",
      "Epoch 111/150\n",
      "72/72 - 0s - loss: 92.9436 - mse: 92.9436\n",
      "Epoch 112/150\n",
      "72/72 - 0s - loss: 91.8843 - mse: 91.8843\n",
      "Epoch 113/150\n",
      "72/72 - 0s - loss: 91.2832 - mse: 91.2832\n",
      "Epoch 114/150\n",
      "72/72 - 0s - loss: 90.0376 - mse: 90.0376\n",
      "Epoch 115/150\n",
      "72/72 - 0s - loss: 91.1745 - mse: 91.1745\n",
      "Epoch 116/150\n",
      "72/72 - 0s - loss: 90.6044 - mse: 90.6044\n",
      "Epoch 117/150\n",
      "72/72 - 0s - loss: 88.3678 - mse: 88.3678\n",
      "Epoch 118/150\n",
      "72/72 - 0s - loss: 90.7619 - mse: 90.7619\n",
      "Epoch 119/150\n",
      "72/72 - 0s - loss: 89.0590 - mse: 89.0590\n",
      "Epoch 120/150\n",
      "72/72 - 0s - loss: 89.0866 - mse: 89.0866\n",
      "Epoch 121/150\n",
      "72/72 - 0s - loss: 88.3597 - mse: 88.3597\n",
      "Epoch 122/150\n",
      "72/72 - 0s - loss: 88.0274 - mse: 88.0274\n",
      "Epoch 123/150\n",
      "72/72 - 0s - loss: 86.4337 - mse: 86.4337\n",
      "Epoch 124/150\n",
      "72/72 - 0s - loss: 86.7962 - mse: 86.7962\n",
      "Epoch 125/150\n",
      "72/72 - 0s - loss: 87.0117 - mse: 87.0117\n",
      "Epoch 126/150\n",
      "72/72 - 0s - loss: 86.5734 - mse: 86.5734\n",
      "Epoch 127/150\n",
      "72/72 - 0s - loss: 86.1233 - mse: 86.1233\n",
      "Epoch 128/150\n",
      "72/72 - 0s - loss: 86.1565 - mse: 86.1565\n",
      "Epoch 129/150\n",
      "72/72 - 0s - loss: 85.4192 - mse: 85.4192\n",
      "Epoch 130/150\n",
      "72/72 - 0s - loss: 84.0531 - mse: 84.0531\n",
      "Epoch 131/150\n",
      "72/72 - 0s - loss: 84.3191 - mse: 84.3191\n",
      "Epoch 132/150\n",
      "72/72 - 0s - loss: 83.9699 - mse: 83.9699\n",
      "Epoch 133/150\n",
      "72/72 - 0s - loss: 83.2275 - mse: 83.2275\n",
      "Epoch 134/150\n",
      "72/72 - 0s - loss: 83.4605 - mse: 83.4605\n",
      "Epoch 135/150\n",
      "72/72 - 0s - loss: 83.2841 - mse: 83.2841\n",
      "Epoch 136/150\n",
      "72/72 - 0s - loss: 82.0218 - mse: 82.0218\n",
      "Epoch 137/150\n",
      "72/72 - 0s - loss: 82.0926 - mse: 82.0926\n",
      "Epoch 138/150\n",
      "72/72 - 0s - loss: 81.4089 - mse: 81.4089\n",
      "Epoch 139/150\n",
      "72/72 - 0s - loss: 81.5499 - mse: 81.5499\n",
      "Epoch 140/150\n",
      "72/72 - 0s - loss: 81.0124 - mse: 81.0124\n",
      "Epoch 141/150\n",
      "72/72 - 0s - loss: 79.0505 - mse: 79.0505\n",
      "Epoch 142/150\n",
      "72/72 - 0s - loss: 79.4598 - mse: 79.4598\n",
      "Epoch 143/150\n",
      "72/72 - 0s - loss: 78.5007 - mse: 78.5007\n",
      "Epoch 144/150\n",
      "72/72 - 0s - loss: 79.4513 - mse: 79.4513\n",
      "Epoch 145/150\n",
      "72/72 - 0s - loss: 77.9484 - mse: 77.9484\n",
      "Epoch 146/150\n",
      "72/72 - 0s - loss: 78.6436 - mse: 78.6436\n",
      "Epoch 147/150\n",
      "72/72 - 0s - loss: 77.8831 - mse: 77.8831\n",
      "Epoch 148/150\n",
      "72/72 - 0s - loss: 78.3287 - mse: 78.3287\n",
      "Epoch 149/150\n",
      "72/72 - 0s - loss: 76.4139 - mse: 76.4139\n",
      "Epoch 150/150\n",
      "72/72 - 0s - loss: 75.9968 - mse: 75.9968\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 130.6039 - mse: 130.6039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[130.60394287109375, 130.60394287109375]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define model\n",
    "model=Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=dim))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='rmsprop',metrics=['mse'])\n",
    "\n",
    "#fit model\n",
    "history=model.fit(x_train, y_train, epochs=150, batch_size=4, verbose=2)\n",
    "model.evaluate(x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 32)                512       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,601\n",
      "Trainable params: 1,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_p = model.predict(x_train)\n",
    "y_test_p = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_train,y_train_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_test,y_test_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list=np.array(y_train).flatten().tolist() #y_train 리스트\n",
    "y_test_list=np.array(y_test).flatten().tolist() #y_test 리스트\n",
    "y_p_train_list=np.array(y_train_p).flatten().tolist() #y_train 예측 리스트\n",
    "y_p_test_list=np.array(y_test_p).flatten().tolist() #y_test 예측 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#오차 범위 3 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set prediction accuracy: 50.35 %\n",
      "test set prediction accuracy: 56.94 %\n"
     ]
    }
   ],
   "source": [
    "#평균 성능 테스트\n",
    "scores = 0\n",
    "mean=np.mean(Y, axis=0)\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= mean <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"train set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= mean <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"test set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### <오차범위 3>\n",
      "- train set prediction accuracy(+-3): 35.07 % <br>\n",
      "- test set prediction accuracy(+-3): 19.44 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 5>\n",
      "- train set prediction accuracy(+-5): 48.96 % <br>\n",
      "- test set prediction accuracy(+-5): 34.72 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 10>\n",
      "- train set prediction accuracy(+-10): 79.17 % <br>\n",
      "- test set prediction accuracy(+-10): 62.50 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 20>\n",
      "- train set prediction accuracy(+-20): 96.88 % <br>\n",
      "- test set prediction accuracy(+-20): 93.06 % <br>\n"
     ]
    }
   ],
   "source": [
    "######입력용#######\n",
    "\n",
    "#오차 범위 3 설정\n",
    "print('### <오차범위 3>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "print('### <오차범위 5>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "print('### <오차범위 10>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "print('### <오차범위 20>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x 배열 생성 (선별 중에 피검사 안하는 거)\n",
    "X1=psqi_df[['SEX','AGE','BMI_1','Muscle_1','Fat_1_x','FatPercentage _1','SBP_1','DBP_1','HR_1','Waist_1']].values\n",
    "X2=psqi_df[['SEX','AGE','BMI_2','Muscle_2','Fat_2_x','FatPercentage_2','SBP_2','DBP_2','HR_2','Waist_2']].values\n",
    "X=np.concatenate((X1, X2), axis=0)\n",
    "\n",
    "#y 배열 생성 (y=HDL)\n",
    "Y1= psqi_df[['HDL_1']].values\n",
    "Y2= psqi_df[['HDL_2']].values\n",
    "Y=np.concatenate((Y1, Y2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규화 (변수간의 스케일 차이)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.asarray(X).astype(np.float)\n",
    "Y=np.asarray(Y).astype(np.float)\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,train_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 72)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((360, 10), (360, 1))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X.shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "72/72 - 0s - loss: 3567.7122 - mse: 3567.7122\n",
      "Epoch 2/150\n",
      "72/72 - 0s - loss: 3141.9500 - mse: 3141.9500\n",
      "Epoch 3/150\n",
      "72/72 - 0s - loss: 2377.5476 - mse: 2377.5476\n",
      "Epoch 4/150\n",
      "72/72 - 0s - loss: 1404.1970 - mse: 1404.1970\n",
      "Epoch 5/150\n",
      "72/72 - 0s - loss: 620.1484 - mse: 620.1484\n",
      "Epoch 6/150\n",
      "72/72 - 0s - loss: 360.4008 - mse: 360.4008\n",
      "Epoch 7/150\n",
      "72/72 - 0s - loss: 301.3846 - mse: 301.3846\n",
      "Epoch 8/150\n",
      "72/72 - 0s - loss: 269.0450 - mse: 269.0450\n",
      "Epoch 9/150\n",
      "72/72 - 0s - loss: 244.2105 - mse: 244.2105\n",
      "Epoch 10/150\n",
      "72/72 - 0s - loss: 221.7361 - mse: 221.7361\n",
      "Epoch 11/150\n",
      "72/72 - 0s - loss: 206.7144 - mse: 206.7144\n",
      "Epoch 12/150\n",
      "72/72 - 0s - loss: 195.6013 - mse: 195.6013\n",
      "Epoch 13/150\n",
      "72/72 - 0s - loss: 187.3375 - mse: 187.3375\n",
      "Epoch 14/150\n",
      "72/72 - 0s - loss: 179.7497 - mse: 179.7497\n",
      "Epoch 15/150\n",
      "72/72 - 0s - loss: 173.0469 - mse: 173.0469\n",
      "Epoch 16/150\n",
      "72/72 - 0s - loss: 170.1583 - mse: 170.1583\n",
      "Epoch 17/150\n",
      "72/72 - 0s - loss: 165.5497 - mse: 165.5497\n",
      "Epoch 18/150\n",
      "72/72 - 0s - loss: 161.7321 - mse: 161.7321\n",
      "Epoch 19/150\n",
      "72/72 - 0s - loss: 158.1244 - mse: 158.1244\n",
      "Epoch 20/150\n",
      "72/72 - 0s - loss: 155.2284 - mse: 155.2284\n",
      "Epoch 21/150\n",
      "72/72 - 0s - loss: 154.0261 - mse: 154.0261\n",
      "Epoch 22/150\n",
      "72/72 - 0s - loss: 151.5072 - mse: 151.5072\n",
      "Epoch 23/150\n",
      "72/72 - 0s - loss: 149.3923 - mse: 149.3923\n",
      "Epoch 24/150\n",
      "72/72 - 0s - loss: 147.3431 - mse: 147.3431\n",
      "Epoch 25/150\n",
      "72/72 - 0s - loss: 145.2740 - mse: 145.2740\n",
      "Epoch 26/150\n",
      "72/72 - 0s - loss: 144.9012 - mse: 144.9012\n",
      "Epoch 27/150\n",
      "72/72 - 0s - loss: 142.7991 - mse: 142.7991\n",
      "Epoch 28/150\n",
      "72/72 - 0s - loss: 141.5534 - mse: 141.5534\n",
      "Epoch 29/150\n",
      "72/72 - 0s - loss: 140.5641 - mse: 140.5641\n",
      "Epoch 30/150\n",
      "72/72 - 0s - loss: 138.4547 - mse: 138.4547\n",
      "Epoch 31/150\n",
      "72/72 - 0s - loss: 137.2583 - mse: 137.2583\n",
      "Epoch 32/150\n",
      "72/72 - 0s - loss: 136.7459 - mse: 136.7459\n",
      "Epoch 33/150\n",
      "72/72 - 0s - loss: 135.6692 - mse: 135.6692\n",
      "Epoch 34/150\n",
      "72/72 - 0s - loss: 134.9863 - mse: 134.9863\n",
      "Epoch 35/150\n",
      "72/72 - 0s - loss: 133.9584 - mse: 133.9584\n",
      "Epoch 36/150\n",
      "72/72 - 0s - loss: 133.3381 - mse: 133.3381\n",
      "Epoch 37/150\n",
      "72/72 - 0s - loss: 132.3483 - mse: 132.3483\n",
      "Epoch 38/150\n",
      "72/72 - 0s - loss: 131.7971 - mse: 131.7971\n",
      "Epoch 39/150\n",
      "72/72 - 0s - loss: 131.6924 - mse: 131.6924\n",
      "Epoch 40/150\n",
      "72/72 - 0s - loss: 130.3144 - mse: 130.3144\n",
      "Epoch 41/150\n",
      "72/72 - 0s - loss: 130.3430 - mse: 130.3430\n",
      "Epoch 42/150\n",
      "72/72 - 0s - loss: 130.0390 - mse: 130.0390\n",
      "Epoch 43/150\n",
      "72/72 - 0s - loss: 129.3154 - mse: 129.3154\n",
      "Epoch 44/150\n",
      "72/72 - 0s - loss: 127.7121 - mse: 127.7121\n",
      "Epoch 45/150\n",
      "72/72 - 0s - loss: 128.2205 - mse: 128.2205\n",
      "Epoch 46/150\n",
      "72/72 - 0s - loss: 128.4126 - mse: 128.4126\n",
      "Epoch 47/150\n",
      "72/72 - 0s - loss: 127.3278 - mse: 127.3278\n",
      "Epoch 48/150\n",
      "72/72 - 0s - loss: 126.0086 - mse: 126.0086\n",
      "Epoch 49/150\n",
      "72/72 - 0s - loss: 126.6885 - mse: 126.6885\n",
      "Epoch 50/150\n",
      "72/72 - 0s - loss: 126.2104 - mse: 126.2104\n",
      "Epoch 51/150\n",
      "72/72 - 0s - loss: 124.2022 - mse: 124.2022\n",
      "Epoch 52/150\n",
      "72/72 - 0s - loss: 126.3797 - mse: 126.3797\n",
      "Epoch 53/150\n",
      "72/72 - 0s - loss: 125.1562 - mse: 125.1562\n",
      "Epoch 54/150\n",
      "72/72 - 0s - loss: 124.4502 - mse: 124.4502\n",
      "Epoch 55/150\n",
      "72/72 - 0s - loss: 123.9989 - mse: 123.9989\n",
      "Epoch 56/150\n",
      "72/72 - 0s - loss: 124.3913 - mse: 124.3913\n",
      "Epoch 57/150\n",
      "72/72 - 0s - loss: 121.7158 - mse: 121.7158\n",
      "Epoch 58/150\n",
      "72/72 - 0s - loss: 124.8532 - mse: 124.8532\n",
      "Epoch 59/150\n",
      "72/72 - 0s - loss: 123.3872 - mse: 123.3872\n",
      "Epoch 60/150\n",
      "72/72 - 0s - loss: 123.6682 - mse: 123.6682\n",
      "Epoch 61/150\n",
      "72/72 - 0s - loss: 122.7756 - mse: 122.7756\n",
      "Epoch 62/150\n",
      "72/72 - 0s - loss: 121.9535 - mse: 121.9535\n",
      "Epoch 63/150\n",
      "72/72 - 0s - loss: 121.0805 - mse: 121.0805\n",
      "Epoch 64/150\n",
      "72/72 - 0s - loss: 123.0049 - mse: 123.0049\n",
      "Epoch 65/150\n",
      "72/72 - 0s - loss: 121.0845 - mse: 121.0845\n",
      "Epoch 66/150\n",
      "72/72 - 0s - loss: 121.8649 - mse: 121.8649\n",
      "Epoch 67/150\n",
      "72/72 - 0s - loss: 121.1403 - mse: 121.1403\n",
      "Epoch 68/150\n",
      "72/72 - 0s - loss: 120.5481 - mse: 120.5481\n",
      "Epoch 69/150\n",
      "72/72 - 0s - loss: 120.5447 - mse: 120.5447\n",
      "Epoch 70/150\n",
      "72/72 - 0s - loss: 120.2958 - mse: 120.2958\n",
      "Epoch 71/150\n",
      "72/72 - 0s - loss: 120.7682 - mse: 120.7682\n",
      "Epoch 72/150\n",
      "72/72 - 0s - loss: 119.5708 - mse: 119.5708\n",
      "Epoch 73/150\n",
      "72/72 - 0s - loss: 120.2468 - mse: 120.2468\n",
      "Epoch 74/150\n",
      "72/72 - 0s - loss: 118.9198 - mse: 118.9198\n",
      "Epoch 75/150\n",
      "72/72 - 0s - loss: 119.5331 - mse: 119.5331\n",
      "Epoch 76/150\n",
      "72/72 - 0s - loss: 117.9819 - mse: 117.9819\n",
      "Epoch 77/150\n",
      "72/72 - 0s - loss: 118.7220 - mse: 118.7220\n",
      "Epoch 78/150\n",
      "72/72 - 0s - loss: 118.5481 - mse: 118.5481\n",
      "Epoch 79/150\n",
      "72/72 - 0s - loss: 117.8319 - mse: 117.8319\n",
      "Epoch 80/150\n",
      "72/72 - 0s - loss: 118.7532 - mse: 118.7532\n",
      "Epoch 81/150\n",
      "72/72 - 0s - loss: 118.8220 - mse: 118.8220\n",
      "Epoch 82/150\n",
      "72/72 - 0s - loss: 117.2509 - mse: 117.2509\n",
      "Epoch 83/150\n",
      "72/72 - 0s - loss: 119.1448 - mse: 119.1448\n",
      "Epoch 84/150\n",
      "72/72 - 0s - loss: 117.1611 - mse: 117.1611\n",
      "Epoch 85/150\n",
      "72/72 - 0s - loss: 117.4549 - mse: 117.4549\n",
      "Epoch 86/150\n",
      "72/72 - 0s - loss: 115.9260 - mse: 115.9260\n",
      "Epoch 87/150\n",
      "72/72 - 0s - loss: 116.4336 - mse: 116.4336\n",
      "Epoch 88/150\n",
      "72/72 - 0s - loss: 115.8548 - mse: 115.8548\n",
      "Epoch 89/150\n",
      "72/72 - 0s - loss: 114.7260 - mse: 114.7260\n",
      "Epoch 90/150\n",
      "72/72 - 0s - loss: 115.8591 - mse: 115.8591\n",
      "Epoch 91/150\n",
      "72/72 - 0s - loss: 115.1677 - mse: 115.1677\n",
      "Epoch 92/150\n",
      "72/72 - 0s - loss: 114.8949 - mse: 114.8949\n",
      "Epoch 93/150\n",
      "72/72 - 0s - loss: 115.8459 - mse: 115.8459\n",
      "Epoch 94/150\n",
      "72/72 - 0s - loss: 115.1512 - mse: 115.1512\n",
      "Epoch 95/150\n",
      "72/72 - 0s - loss: 114.1133 - mse: 114.1133\n",
      "Epoch 96/150\n",
      "72/72 - 0s - loss: 114.8569 - mse: 114.8569\n",
      "Epoch 97/150\n",
      "72/72 - 0s - loss: 114.6741 - mse: 114.6741\n",
      "Epoch 98/150\n",
      "72/72 - 0s - loss: 114.0650 - mse: 114.0650\n",
      "Epoch 99/150\n",
      "72/72 - 0s - loss: 113.9548 - mse: 113.9548\n",
      "Epoch 100/150\n",
      "72/72 - 0s - loss: 113.8977 - mse: 113.8977\n",
      "Epoch 101/150\n",
      "72/72 - 0s - loss: 114.3784 - mse: 114.3784\n",
      "Epoch 102/150\n",
      "72/72 - 0s - loss: 112.7772 - mse: 112.7772\n",
      "Epoch 103/150\n",
      "72/72 - 0s - loss: 112.8695 - mse: 112.8695\n",
      "Epoch 104/150\n",
      "72/72 - 0s - loss: 112.7040 - mse: 112.7040\n",
      "Epoch 105/150\n",
      "72/72 - 0s - loss: 112.3168 - mse: 112.3168\n",
      "Epoch 106/150\n",
      "72/72 - 0s - loss: 111.2469 - mse: 111.2469\n",
      "Epoch 107/150\n",
      "72/72 - 0s - loss: 113.0550 - mse: 113.0550\n",
      "Epoch 108/150\n",
      "72/72 - 0s - loss: 112.8878 - mse: 112.8878\n",
      "Epoch 109/150\n",
      "72/72 - 0s - loss: 112.1013 - mse: 112.1013\n",
      "Epoch 110/150\n",
      "72/72 - 0s - loss: 112.4076 - mse: 112.4076\n",
      "Epoch 111/150\n",
      "72/72 - 0s - loss: 111.7885 - mse: 111.7885\n",
      "Epoch 112/150\n",
      "72/72 - 0s - loss: 111.7512 - mse: 111.7512\n",
      "Epoch 113/150\n",
      "72/72 - 0s - loss: 110.1413 - mse: 110.1413\n",
      "Epoch 114/150\n",
      "72/72 - 0s - loss: 111.8760 - mse: 111.8760\n",
      "Epoch 115/150\n",
      "72/72 - 0s - loss: 110.8282 - mse: 110.8282\n",
      "Epoch 116/150\n",
      "72/72 - 0s - loss: 110.9629 - mse: 110.9629\n",
      "Epoch 117/150\n",
      "72/72 - 0s - loss: 109.4219 - mse: 109.4219\n",
      "Epoch 118/150\n",
      "72/72 - 0s - loss: 110.4279 - mse: 110.4279\n",
      "Epoch 119/150\n",
      "72/72 - 0s - loss: 109.5256 - mse: 109.5256\n",
      "Epoch 120/150\n",
      "72/72 - 0s - loss: 109.9807 - mse: 109.9807\n",
      "Epoch 121/150\n",
      "72/72 - 0s - loss: 109.4097 - mse: 109.4097\n",
      "Epoch 122/150\n",
      "72/72 - 0s - loss: 110.2628 - mse: 110.2628\n",
      "Epoch 123/150\n",
      "72/72 - 0s - loss: 108.5735 - mse: 108.5735\n",
      "Epoch 124/150\n",
      "72/72 - 0s - loss: 109.2755 - mse: 109.2755\n",
      "Epoch 125/150\n",
      "72/72 - 0s - loss: 107.8540 - mse: 107.8540\n",
      "Epoch 126/150\n",
      "72/72 - 0s - loss: 108.9318 - mse: 108.9318\n",
      "Epoch 127/150\n",
      "72/72 - 0s - loss: 108.6608 - mse: 108.6608\n",
      "Epoch 128/150\n",
      "72/72 - 0s - loss: 107.3509 - mse: 107.3509\n",
      "Epoch 129/150\n",
      "72/72 - 0s - loss: 108.1099 - mse: 108.1099\n",
      "Epoch 130/150\n",
      "72/72 - 0s - loss: 106.0288 - mse: 106.0288\n",
      "Epoch 131/150\n",
      "72/72 - 0s - loss: 107.4670 - mse: 107.4670\n",
      "Epoch 132/150\n",
      "72/72 - 0s - loss: 107.1753 - mse: 107.1753\n",
      "Epoch 133/150\n",
      "72/72 - 0s - loss: 105.4670 - mse: 105.4670\n",
      "Epoch 134/150\n",
      "72/72 - 0s - loss: 106.8559 - mse: 106.8559\n",
      "Epoch 135/150\n",
      "72/72 - 0s - loss: 107.0825 - mse: 107.0825\n",
      "Epoch 136/150\n",
      "72/72 - 0s - loss: 105.7063 - mse: 105.7063\n",
      "Epoch 137/150\n",
      "72/72 - 0s - loss: 105.2807 - mse: 105.2807\n",
      "Epoch 138/150\n",
      "72/72 - 0s - loss: 105.0650 - mse: 105.0650\n",
      "Epoch 139/150\n",
      "72/72 - 0s - loss: 104.6119 - mse: 104.6119\n",
      "Epoch 140/150\n",
      "72/72 - 0s - loss: 105.6043 - mse: 105.6043\n",
      "Epoch 141/150\n",
      "72/72 - 0s - loss: 105.3703 - mse: 105.3703\n",
      "Epoch 142/150\n",
      "72/72 - 0s - loss: 104.3661 - mse: 104.3661\n",
      "Epoch 143/150\n",
      "72/72 - 0s - loss: 103.3497 - mse: 103.3497\n",
      "Epoch 144/150\n",
      "72/72 - 0s - loss: 105.1057 - mse: 105.1057\n",
      "Epoch 145/150\n",
      "72/72 - 0s - loss: 103.7798 - mse: 103.7798\n",
      "Epoch 146/150\n",
      "72/72 - 0s - loss: 104.4895 - mse: 104.4895\n",
      "Epoch 147/150\n",
      "72/72 - 0s - loss: 104.2595 - mse: 104.2595\n",
      "Epoch 148/150\n",
      "72/72 - 0s - loss: 103.7140 - mse: 103.7140\n",
      "Epoch 149/150\n",
      "72/72 - 0s - loss: 101.3517 - mse: 101.3517\n",
      "Epoch 150/150\n",
      "72/72 - 0s - loss: 103.6681 - mse: 103.6681\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7f8152be5b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 133.7242 - mse: 133.7242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[133.7241973876953, 133.7241973876953]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define model\n",
    "model=Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=dim))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='rmsprop',metrics=['mse'])\n",
    "\n",
    "#fit model\n",
    "history=model.fit(x_train, y_train, epochs=150, batch_size=4, verbose=2)\n",
    "model.evaluate(x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 32)                352       \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,441\n",
      "Trainable params: 1,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_p = model.predict(x_train)\n",
    "y_test_p = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_train,y_train_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_test,y_test_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list=np.array(y_train).flatten().tolist() #y_train 리스트\n",
    "y_test_list=np.array(y_test).flatten().tolist() #y_test 리스트\n",
    "y_p_train_list=np.array(y_train_p).flatten().tolist() #y_train 예측 리스트\n",
    "y_p_test_list=np.array(y_test_p).flatten().tolist() #y_test 예측 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#오차 범위 3 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set prediction accuracy: 50.35 %\n",
      "test set prediction accuracy: 56.94 %\n"
     ]
    }
   ],
   "source": [
    "#평균 성능 테스트\n",
    "scores = 0\n",
    "mean=np.mean(Y, axis=0)\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= mean <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"train set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= mean <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"test set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### <오차범위 3>\n",
      "- train set prediction accuracy(+-3): 27.43 % <br>\n",
      "- test set prediction accuracy(+-3): 18.06 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 5>\n",
      "- train set prediction accuracy(+-5): 44.79 % <br>\n",
      "- test set prediction accuracy(+-5): 29.17 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 10>\n",
      "- train set prediction accuracy(+-10): 73.26 % <br>\n",
      "- test set prediction accuracy(+-10): 65.28 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 20>\n",
      "- train set prediction accuracy(+-20): 95.14 % <br>\n",
      "- test set prediction accuracy(+-20): 91.67 % <br>\n"
     ]
    }
   ],
   "source": [
    "######입력용#######\n",
    "\n",
    "#오차 범위 3 설정\n",
    "print('### <오차범위 3>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "print('### <오차범위 5>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "print('### <오차범위 10>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "print('### <오차범위 20>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다쓴거\n",
    "### <오차범위 3>\n",
    "- train_all set prediction accuracy(+-3): 88.89 % <br>\n",
    "- test_all set prediction accuracy(+-3): 31.94 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 5>\n",
    "- train_all set prediction accuracy(+-5): 96.53 % <br>\n",
    "- test_all set prediction accuracy(+-5): 55.56 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 10>\n",
    "- train_all set prediction accuracy(+-10): 100.00 % <br>\n",
    "- test_all set prediction accuracy(+-10): 83.33 % <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다안쓴거\n",
    "### <오차범위 3>\n",
    "- train_some set prediction accuracy(+-3): 32.99 % <br>\n",
    "- test_some set prediction accuracy(+-3): 27.78 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 5>\n",
    "- train_some set prediction accuracy(+-5): 54.86 % <br>\n",
    "- test_some set prediction accuracy(+-5): 40.28 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 10>\n",
    "- train_some set prediction accuracy(+-10): 82.29 % <br>\n",
    "- test_some set prediction accuracy(+-10): 59.72 % <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
