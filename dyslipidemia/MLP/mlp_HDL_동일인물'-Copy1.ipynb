{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FONT 깨질때 폰트깨질때\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname = \"C:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font',family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sample_ID GROUP  COHORT SEX  AGE     Trait FitbitOX      Fitbit_ID  HTN  \\\n",
      "0         S0001   SMI       2   M   60  fitbit_O        O  sevrance00001  NaN   \n",
      "1         S0002   SMI       2   M   61  fitbit_O        O  sevrance00002  NaN   \n",
      "2         S0003   SMI       2   F   52  fitbit_O        O  sevrance00003  NaN   \n",
      "3         S0004   SMI       2   F   41  fitbit_O        O  sevrance00004  NaN   \n",
      "4         S0005   SMI       2   F   41  fitbit_O        O  sevrance00005  NaN   \n",
      "..          ...   ...     ...  ..  ...       ...      ...            ...  ...   \n",
      "383  MetS_S0280  MetS       1   F   24  fitbit_O        O   gnfmmets+139  NaN   \n",
      "384  MetS_S0281  MetS       1   F   44  fitbit_O        O   gnfmmets+140  NaN   \n",
      "385  MetS_S0282  MetS       1   F   37  fitbit_O        O   gnfmmets+141  1.0   \n",
      "386  MetS_S0283  MetS       1   M   51  fitbit_X        X              X  NaN   \n",
      "387  MetS_S0284  MetS       1   F   42  fitbit_X        X              X  NaN   \n",
      "\n",
      "      DM  ...  BDI_Q13_2 BDI_Q14_2  BDI_Q15_2  BDI_Q16_2 BDI_Q17_2 BDI_Q18_2  \\\n",
      "0    NaN  ...        1.0       1.0        1.0        2.0       1.0       1.0   \n",
      "1    1.0  ...        1.0       1.0        1.0        1.0       2.0       1.0   \n",
      "2    NaN  ...        1.0       1.0        1.0        1.0       1.0       1.0   \n",
      "3    NaN  ...        2.0       2.0        2.0        2.0       2.0       2.0   \n",
      "4    NaN  ...        2.0       1.0        2.0        2.0       2.0       1.0   \n",
      "..   ...  ...        ...       ...        ...        ...       ...       ...   \n",
      "383  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "384  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "385  1.0  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "386  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "387  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "\n",
      "     BDI_Q19_2  BDI_Q20_2  BDI_Q21_2  Diet_2  \n",
      "0          2.0        1.0        2.0     2.0  \n",
      "1          1.0        1.0        1.0     2.0  \n",
      "2          1.0        2.0        4.0     1.0  \n",
      "3          1.0        1.0        1.0     2.0  \n",
      "4          1.0        2.0        1.0     2.0  \n",
      "..         ...        ...        ...     ...  \n",
      "383        NaN        NaN        NaN     NaN  \n",
      "384        NaN        NaN        NaN     NaN  \n",
      "385        NaN        NaN        NaN     NaN  \n",
      "386        NaN        NaN        NaN     NaN  \n",
      "387        NaN        NaN        NaN     NaN  \n",
      "\n",
      "[388 rows x 3527 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('./최종 데이터 그래프그리기용.xlsx') \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df=df[['Sample_ID','GROUP','COHORT','AGE','SEX','BMI_1','PSQI_TOTAL_1','Insulin _1','CRP_1','WBC_1','Neutrophil_1','Lym_1','GLU0_1','Creatinine_1','AST_1','ALT_1','TG_1','LDL_1','Muscle_1','Fat_1_x','FatPercentage _1','WHR_1','SBP_1',\n",
    "            'DBP_1','HR_1','Waist_1','Chol_1','BUN_1','HDL_1',\n",
    "          'BMI_2','PSQI_TOTAL_2','Insulin _2','CRP_2','WBC_2','Neutrophil_2','Lym_2','GLU0_2',\n",
    "            'Creatinine_2','AST_2','ALT_2','TG_2','LDL_2','Muscle_2','Fat_2_x','FatPercentage_2','WHR_2','SBP_2',\n",
    "            'DBP_2','HR_2','Waist_2','Chol_2','BUN_2','HDL_2',\n",
    "           'PSQI_C1_2','PSQI_C2_2','PSQI_C3_2','PSQI_C4_2','PSQI_C5_2','PSQI_C6_2','PSQI_C7_2','PSQI_Q1_2','PSQI_Q2_2','PSQI_Q3_2','PSQI_Q4_2',\n",
    "            'PSQI_Q5a_2','PSQI_Q5b_2','PSQI_Q5c_2','PSQI_Q5d_2','PSQI_Q5e_2','PSQI_Q5f_2','PSQI_Q5g_2','PSQI_Q5h_2','PSQI_Q5i_2','PSQI_Q5j_2','PSQI_Q6_2','PSQI_Q7_2','PSQI_Q8_2','PSQI_Q9_2',\n",
    "           'PSQI_C1_1','PSQI_C2_1','PSQI_C3_1','PSQI_C4_1','PSQI_C5_1','PSQI_C6_1','PSQI_C7_1','PSQI_Q1_1','PSQI_Q2_1','PSQI_Q3_1','PSQI_Q4_1',\n",
    "            'PSQI_Q5a_1','PSQI_Q5b_1','PSQI_Q5c_1','PSQI_Q5d_1','PSQI_Q5e_1','PSQI_Q5f_1','PSQI_Q5g_1','PSQI_Q5h_1','PSQI_Q5i_1','PSQI_Q5j_1','PSQI_Q6_1','PSQI_Q7_1','PSQI_Q8_1','PSQI_Q9_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>COHORT</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>...</th>\n",
       "      <th>PSQI_Q5e_1</th>\n",
       "      <th>PSQI_Q5f_1</th>\n",
       "      <th>PSQI_Q5g_1</th>\n",
       "      <th>PSQI_Q5h_1</th>\n",
       "      <th>PSQI_Q5i_1</th>\n",
       "      <th>PSQI_Q5j_1</th>\n",
       "      <th>PSQI_Q6_1</th>\n",
       "      <th>PSQI_Q7_1</th>\n",
       "      <th>PSQI_Q8_1</th>\n",
       "      <th>PSQI_Q9_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0001</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>M</td>\n",
       "      <td>21.110190</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0002</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>27.782064</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.51</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0003</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>24.944742</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.85</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>22.620489</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.14</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0005</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>20.524157</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.93</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>MetS_S0280</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>34.803410</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>MetS_S0281</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>F</td>\n",
       "      <td>30.903615</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.82</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>MetS_S0282</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>F</td>\n",
       "      <td>28.676533</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>MetS_S0283</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>M</td>\n",
       "      <td>24.549738</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>6.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>MetS_S0284</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>F</td>\n",
       "      <td>24.605921</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample_ID GROUP  COHORT  AGE SEX      BMI_1  PSQI_TOTAL_1 Insulin _1  \\\n",
       "0         S0001   SMI       2   60   M  21.110190           8.0        7.7   \n",
       "1         S0002   SMI       2   61   M  27.782064           4.0        5.4   \n",
       "2         S0003   SMI       2   52   F  24.944742           3.0        5.1   \n",
       "3         S0004   SMI       2   41   F  22.620489           6.0        4.2   \n",
       "4         S0005   SMI       2   41   F  20.524157          10.0        3.2   \n",
       "..          ...   ...     ...  ...  ..        ...           ...        ...   \n",
       "383  MetS_S0280  MetS       1   24   F  34.803410           5.0       11.3   \n",
       "384  MetS_S0281  MetS       1   44   F  30.903615           3.0       10.6   \n",
       "385  MetS_S0282  MetS       1   37   F  28.676533           3.0       12.2   \n",
       "386  MetS_S0283  MetS       1   51   M  24.549738           5.0       10.4   \n",
       "387  MetS_S0284  MetS       1   42   F  24.605921           3.0       10.1   \n",
       "\n",
       "    CRP_1  WBC_1  ...  PSQI_Q5e_1  PSQI_Q5f_1  PSQI_Q5g_1  PSQI_Q5h_1  \\\n",
       "0     0.2   3.91  ...         0.0         0.0         0.0         1.0   \n",
       "1     0.2   5.51  ...         3.0         1.0         1.0         1.0   \n",
       "2     0.7   4.85  ...         3.0         1.0         1.0         0.0   \n",
       "3     0.6   6.14  ...         1.0         0.0         0.0         0.0   \n",
       "4     0.1   4.93  ...         0.0         0.0         0.0         1.0   \n",
       "..    ...    ...  ...         ...         ...         ...         ...   \n",
       "383   0.4   5.32  ...         0.0         0.0         0.0         0.0   \n",
       "384   2.3   5.82  ...         2.0         0.0         1.0         0.0   \n",
       "385     1   6.18  ...         0.0         0.0         0.0         0.0   \n",
       "386   1.2   6.67  ...         0.0         0.0         0.0         0.0   \n",
       "387   0.8   7.03  ...         0.0         0.0         0.0         0.0   \n",
       "\n",
       "     PSQI_Q5i_1  PSQI_Q5j_1  PSQI_Q6_1  PSQI_Q7_1  PSQI_Q8_1  PSQI_Q9_1  \n",
       "0           1.0         0.0        NaN        NaN        2.0        1.0  \n",
       "1           0.0         0.0        NaN        NaN        1.0        1.0  \n",
       "2           0.0         0.0        NaN        NaN        2.0        0.0  \n",
       "3           0.0         0.0        NaN        NaN        2.0        1.0  \n",
       "4           0.0         0.0        NaN        NaN        2.0        1.0  \n",
       "..          ...         ...        ...        ...        ...        ...  \n",
       "383         0.0         1.0        1.0        0.0        0.0        0.0  \n",
       "384         1.0         0.0        1.0        0.0        0.0        0.0  \n",
       "385         0.0         0.0        1.0        0.0        0.0        0.0  \n",
       "386         0.0         0.0        2.0        0.0        0.0        0.0  \n",
       "387         0.0         0.0        1.0        0.0        0.0        0.0  \n",
       "\n",
       "[388 rows x 103 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>COHORT</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>...</th>\n",
       "      <th>PSQI_Q5e_1</th>\n",
       "      <th>PSQI_Q5f_1</th>\n",
       "      <th>PSQI_Q5g_1</th>\n",
       "      <th>PSQI_Q5h_1</th>\n",
       "      <th>PSQI_Q5i_1</th>\n",
       "      <th>PSQI_Q5j_1</th>\n",
       "      <th>PSQI_Q6_1</th>\n",
       "      <th>PSQI_Q7_1</th>\n",
       "      <th>PSQI_Q8_1</th>\n",
       "      <th>PSQI_Q9_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0001</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>M</td>\n",
       "      <td>21.110190</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.91</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0002</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>27.782064</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.51</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0003</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>24.944742</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.85</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>22.620489</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.14</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0005</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>20.524157</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.93</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>MetS_S0280</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>34.803410</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5.32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>MetS_S0281</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>F</td>\n",
       "      <td>30.903615</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.82</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>MetS_S0282</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>F</td>\n",
       "      <td>28.676533</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>MetS_S0283</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>M</td>\n",
       "      <td>24.549738</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>6.67</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>MetS_S0284</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>F</td>\n",
       "      <td>24.605921</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.03</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample_ID GROUP  COHORT  AGE SEX      BMI_1  PSQI_TOTAL_1 Insulin _1  \\\n",
       "0         S0001   SMI       2   60   M  21.110190           8.0        7.7   \n",
       "1         S0002   SMI       2   61   M  27.782064           4.0        5.4   \n",
       "2         S0003   SMI       2   52   F  24.944742           3.0        5.1   \n",
       "3         S0004   SMI       2   41   F  22.620489           6.0        4.2   \n",
       "4         S0005   SMI       2   41   F  20.524157          10.0        3.2   \n",
       "..          ...   ...     ...  ...  ..        ...           ...        ...   \n",
       "383  MetS_S0280  MetS       1   24   F  34.803410           5.0       11.3   \n",
       "384  MetS_S0281  MetS       1   44   F  30.903615           3.0       10.6   \n",
       "385  MetS_S0282  MetS       1   37   F  28.676533           3.0       12.2   \n",
       "386  MetS_S0283  MetS       1   51   M  24.549738           5.0       10.4   \n",
       "387  MetS_S0284  MetS       1   42   F  24.605921           3.0       10.1   \n",
       "\n",
       "    CRP_1  WBC_1  ...  PSQI_Q5e_1  PSQI_Q5f_1  PSQI_Q5g_1  PSQI_Q5h_1  \\\n",
       "0     0.2   3.91  ...         0.0         0.0         0.0         1.0   \n",
       "1     0.2   5.51  ...         3.0         1.0         1.0         1.0   \n",
       "2     0.7   4.85  ...         3.0         1.0         1.0         0.0   \n",
       "3     0.6   6.14  ...         1.0         0.0         0.0         0.0   \n",
       "4     0.1   4.93  ...         0.0         0.0         0.0         1.0   \n",
       "..    ...    ...  ...         ...         ...         ...         ...   \n",
       "383   0.4   5.32  ...         0.0         0.0         0.0         0.0   \n",
       "384   2.3   5.82  ...         2.0         0.0         1.0         0.0   \n",
       "385     1   6.18  ...         0.0         0.0         0.0         0.0   \n",
       "386   1.2   6.67  ...         0.0         0.0         0.0         0.0   \n",
       "387   0.8   7.03  ...         0.0         0.0         0.0         0.0   \n",
       "\n",
       "     PSQI_Q5i_1  PSQI_Q5j_1  PSQI_Q6_1  PSQI_Q7_1  PSQI_Q8_1  PSQI_Q9_1  \n",
       "0           1.0         0.0        NaN        NaN        2.0        1.0  \n",
       "1           0.0         0.0        NaN        NaN        1.0        1.0  \n",
       "2           0.0         0.0        NaN        NaN        2.0        0.0  \n",
       "3           0.0         0.0        NaN        NaN        2.0        1.0  \n",
       "4           0.0         0.0        NaN        NaN        2.0        1.0  \n",
       "..          ...         ...        ...        ...        ...        ...  \n",
       "383         0.0         1.0        1.0        0.0        0.0        0.0  \n",
       "384         1.0         0.0        1.0        0.0        0.0        0.0  \n",
       "385         0.0         0.0        1.0        0.0        0.0        0.0  \n",
       "386         0.0         0.0        2.0        0.0        0.0        0.0  \n",
       "387         0.0         0.0        1.0        0.0        0.0        0.0  \n",
       "\n",
       "[317 rows x 103 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#보조 호르몬 요법을 받고 있는 Cohort 3 제거 Filter 적용\n",
    "psqi_df = psqi_df[(psqi_df['COHORT'] != 3)]\n",
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df=psqi_df.dropna()\n",
    "psqi_df.reset_index(drop=True, inplace=True)\n",
    "psqi_df=psqi_df.drop([\"Sample_ID\", \"GROUP\", \"COHORT\"],axis=1)\n",
    "#1분, 매일다름, 정해진간이없음 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df[\"SEX\"] = psqi_df[\"SEX\"].apply(lambda x: 1. if x=='M' else 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df[\"Insulin _1\"] = psqi_df[\"Insulin _1\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)\n",
    "psqi_df[\"Insulin _2\"] = psqi_df[\"Insulin _2\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df[\"CRP_1\"] = psqi_df[\"CRP_1\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)\n",
    "psqi_df[\"CRP_2\"] = psqi_df[\"CRP_2\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>Neutrophil_1</th>\n",
       "      <th>Lym_1</th>\n",
       "      <th>GLU0_1</th>\n",
       "      <th>...</th>\n",
       "      <th>PSQI_Q5e_1</th>\n",
       "      <th>PSQI_Q5f_1</th>\n",
       "      <th>PSQI_Q5g_1</th>\n",
       "      <th>PSQI_Q5h_1</th>\n",
       "      <th>PSQI_Q5i_1</th>\n",
       "      <th>PSQI_Q5j_1</th>\n",
       "      <th>PSQI_Q6_1</th>\n",
       "      <th>PSQI_Q7_1</th>\n",
       "      <th>PSQI_Q8_1</th>\n",
       "      <th>PSQI_Q9_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.107955</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>23.787859</td>\n",
       "      <td>5.062500</td>\n",
       "      <td>7.715909</td>\n",
       "      <td>0.757955</td>\n",
       "      <td>5.856227</td>\n",
       "      <td>56.110795</td>\n",
       "      <td>34.115909</td>\n",
       "      <td>98.857955</td>\n",
       "      <td>...</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.267045</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.397727</td>\n",
       "      <td>0.448864</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.142045</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.460227</td>\n",
       "      <td>0.556818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.451001</td>\n",
       "      <td>0.462489</td>\n",
       "      <td>4.980203</td>\n",
       "      <td>2.844858</td>\n",
       "      <td>4.133429</td>\n",
       "      <td>1.357495</td>\n",
       "      <td>1.420172</td>\n",
       "      <td>8.566716</td>\n",
       "      <td>7.746644</td>\n",
       "      <td>14.580897</td>\n",
       "      <td>...</td>\n",
       "      <td>1.027416</td>\n",
       "      <td>0.652244</td>\n",
       "      <td>0.618335</td>\n",
       "      <td>0.725688</td>\n",
       "      <td>0.930560</td>\n",
       "      <td>0.852727</td>\n",
       "      <td>0.690544</td>\n",
       "      <td>0.559220</td>\n",
       "      <td>0.840821</td>\n",
       "      <td>0.746159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.231576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.820000</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.833309</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.975000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>4.857500</td>\n",
       "      <td>50.525000</td>\n",
       "      <td>28.975000</td>\n",
       "      <td>91.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.351473</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>5.720000</td>\n",
       "      <td>55.950000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.502662</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.505000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>6.612500</td>\n",
       "      <td>62.025000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>10.550000</td>\n",
       "      <td>78.400000</td>\n",
       "      <td>55.400000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              AGE         SEX       BMI_1  PSQI_TOTAL_1  Insulin _1  \\\n",
       "count  176.000000  176.000000  176.000000    176.000000  176.000000   \n",
       "mean    38.107955    0.306818   23.787859      5.062500    7.715909   \n",
       "std     11.451001    0.462489    4.980203      2.844858    4.133429   \n",
       "min     20.000000    0.000000   15.231576      0.000000    0.100000   \n",
       "25%     29.000000    0.000000   20.833309      3.000000    4.975000   \n",
       "50%     35.000000    0.000000   23.351473      5.000000    6.600000   \n",
       "75%     46.000000    1.000000   25.502662      7.000000    9.505000   \n",
       "max     63.000000    1.000000   67.500000     14.000000   24.700000   \n",
       "\n",
       "            CRP_1       WBC_1  Neutrophil_1       Lym_1      GLU0_1  ...  \\\n",
       "count  176.000000  176.000000    176.000000  176.000000  176.000000  ...   \n",
       "mean     0.757955    5.856227     56.110795   34.115909   98.857955  ...   \n",
       "std      1.357495    1.420172      8.566716    7.746644   14.580897  ...   \n",
       "min      0.000000    2.820000     34.500000   15.100000   63.000000  ...   \n",
       "25%      0.200000    4.857500     50.525000   28.975000   91.750000  ...   \n",
       "50%      0.300000    5.720000     55.950000   34.000000   95.000000  ...   \n",
       "75%      0.700000    6.612500     62.025000   39.000000  102.000000  ...   \n",
       "max     11.100000   10.550000     78.400000   55.400000  182.000000  ...   \n",
       "\n",
       "       PSQI_Q5e_1  PSQI_Q5f_1  PSQI_Q5g_1  PSQI_Q5h_1  PSQI_Q5i_1  PSQI_Q5j_1  \\\n",
       "count  176.000000  176.000000  176.000000  176.000000  176.000000  176.000000   \n",
       "mean     0.636364    0.267045    0.227273    0.397727    0.448864    0.375000   \n",
       "std      1.027416    0.652244    0.618335    0.725688    0.930560    0.852727   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      1.000000    0.000000    0.000000    1.000000    0.000000    0.000000   \n",
       "max      3.000000    3.000000    3.000000    3.000000    3.000000    3.000000   \n",
       "\n",
       "        PSQI_Q6_1   PSQI_Q7_1   PSQI_Q8_1   PSQI_Q9_1  \n",
       "count  176.000000  176.000000  176.000000  176.000000  \n",
       "mean     1.142045    0.136364    0.460227    0.556818  \n",
       "std      0.690544    0.559220    0.840821    0.746159  \n",
       "min      0.000000    0.000000    0.000000    0.000000  \n",
       "25%      1.000000    0.000000    0.000000    0.000000  \n",
       "50%      1.000000    0.000000    0.000000    0.000000  \n",
       "75%      2.000000    0.000000    1.000000    1.000000  \n",
       "max      3.000000    3.000000    3.000000    3.000000  \n",
       "\n",
       "[8 rows x 99 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    122\n",
       "1.0     54\n",
       "Name: SEX, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df[\"SEX\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>Neutrophil_1</th>\n",
       "      <th>Lym_1</th>\n",
       "      <th>GLU0_1</th>\n",
       "      <th>...</th>\n",
       "      <th>PSQI_Q5e_1</th>\n",
       "      <th>PSQI_Q5f_1</th>\n",
       "      <th>PSQI_Q5g_1</th>\n",
       "      <th>PSQI_Q5h_1</th>\n",
       "      <th>PSQI_Q5i_1</th>\n",
       "      <th>PSQI_Q5j_1</th>\n",
       "      <th>PSQI_Q6_1</th>\n",
       "      <th>PSQI_Q7_1</th>\n",
       "      <th>PSQI_Q8_1</th>\n",
       "      <th>PSQI_Q9_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.097789</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.82</td>\n",
       "      <td>54.6</td>\n",
       "      <td>35.0</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.472213</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.46</td>\n",
       "      <td>44.3</td>\n",
       "      <td>43.7</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.744827</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.99</td>\n",
       "      <td>51.0</td>\n",
       "      <td>37.8</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.616175</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.84</td>\n",
       "      <td>39.1</td>\n",
       "      <td>42.1</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.437500</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.22</td>\n",
       "      <td>49.3</td>\n",
       "      <td>39.3</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.259585</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.78</td>\n",
       "      <td>42.3</td>\n",
       "      <td>47.3</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.630719</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.60</td>\n",
       "      <td>51.7</td>\n",
       "      <td>34.6</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.641274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.4</td>\n",
       "      <td>6.34</td>\n",
       "      <td>55.9</td>\n",
       "      <td>34.9</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.421366</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.88</td>\n",
       "      <td>40.9</td>\n",
       "      <td>48.0</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.271653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.28</td>\n",
       "      <td>75.7</td>\n",
       "      <td>15.1</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AGE  SEX      BMI_1  PSQI_TOTAL_1  Insulin _1  CRP_1  WBC_1  \\\n",
       "0     35  1.0  24.097789           5.0        5.57    0.0   5.82   \n",
       "1     46  1.0  23.472213           5.0        7.35    0.7   5.46   \n",
       "2     32  1.0  23.744827           2.0        9.26    0.4   3.99   \n",
       "3     33  0.0  20.616175           4.0        3.52    0.0   5.84   \n",
       "4     28  0.0  18.437500           3.0        2.86    0.0   4.22   \n",
       "..   ...  ...        ...           ...         ...    ...    ...   \n",
       "171   63  0.0  26.259585           3.0        4.20    0.2   4.78   \n",
       "172   57  1.0  28.630719           4.0        8.80    3.0   4.60   \n",
       "173   35  0.0  21.641274           1.0        6.30    0.4   6.34   \n",
       "174   61  0.0  20.421366           8.0        4.80    0.2   4.88   \n",
       "175   56  1.0  22.271653           1.0        9.00    0.2   6.28   \n",
       "\n",
       "     Neutrophil_1  Lym_1  GLU0_1  ...  PSQI_Q5e_1  PSQI_Q5f_1  PSQI_Q5g_1  \\\n",
       "0            54.6   35.0      89  ...         0.0         0.0         0.0   \n",
       "1            44.3   43.7      90  ...         3.0         3.0         0.0   \n",
       "2            51.0   37.8      96  ...         0.0         0.0         0.0   \n",
       "3            39.1   42.1      81  ...         0.0         0.0         0.0   \n",
       "4            49.3   39.3      63  ...         0.0         0.0         0.0   \n",
       "..            ...    ...     ...  ...         ...         ...         ...   \n",
       "171          42.3   47.3      96  ...         3.0         0.0         0.0   \n",
       "172          51.7   34.6      94  ...         0.0         0.0         1.0   \n",
       "173          55.9   34.9      87  ...         0.0         0.0         0.0   \n",
       "174          40.9   48.0      93  ...         1.0         0.0         0.0   \n",
       "175          75.7   15.1     125  ...         0.0         0.0         0.0   \n",
       "\n",
       "     PSQI_Q5h_1  PSQI_Q5i_1  PSQI_Q5j_1  PSQI_Q6_1  PSQI_Q7_1  PSQI_Q8_1  \\\n",
       "0           0.0         0.0         0.0        1.0        0.0        1.0   \n",
       "1           0.0         0.0         0.0        2.0        0.0        0.0   \n",
       "2           0.0         0.0         0.0        1.0        0.0        0.0   \n",
       "3           0.0         1.0         1.0        1.0        0.0        1.0   \n",
       "4           0.0         0.0         1.0        1.0        0.0        0.0   \n",
       "..          ...         ...         ...        ...        ...        ...   \n",
       "171         0.0         0.0         0.0        0.0        0.0        0.0   \n",
       "172         0.0         3.0         0.0        1.0        0.0        1.0   \n",
       "173         0.0         0.0         0.0        0.0        0.0        0.0   \n",
       "174         0.0         0.0         0.0        2.0        0.0        0.0   \n",
       "175         0.0         2.0         0.0        0.0        0.0        0.0   \n",
       "\n",
       "     PSQI_Q9_1  \n",
       "0          1.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  \n",
       "..         ...  \n",
       "171        0.0  \n",
       "172        3.0  \n",
       "173        0.0  \n",
       "174        1.0  \n",
       "175        0.0  \n",
       "\n",
       "[176 rows x 100 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x 배열 생성 (x=AGE, SEX, PSQI, BMI)\n",
    "X1=psqi_df[['AGE','SEX','BMI_1','PSQI_TOTAL_1','Insulin _1','CRP_1','WBC_1','Neutrophil_1','Lym_1','GLU0_1',\n",
    "            'Creatinine_1','AST_1','ALT_1','TG_1','LDL_1','Muscle_1','Fat_1_x','FatPercentage _1','WHR_1','SBP_1',\n",
    "            'DBP_1','HR_1','Waist_1','Chol_1','BUN_1']]\n",
    "\n",
    "X2=psqi_df[['AGE','SEX','BMI_2','PSQI_TOTAL_2','Insulin _2','CRP_2','WBC_2','Neutrophil_2','Lym_2','GLU0_2',\n",
    "            'Creatinine_2','AST_2','ALT_2','TG_2','LDL_2','Muscle_2','Fat_2_x','FatPercentage_2','WHR_2','SBP_2',\n",
    "            'DBP_2','HR_2','Waist_2','Chol_2','BUN_2']]\n",
    "X=np.concatenate((X1, X2), axis=0)\n",
    "\n",
    "\n",
    "#y 배열 생성 (y=HDL)\n",
    "Y1= psqi_df[['HDL_1']].values\n",
    "Y2= psqi_df[['HDL_2']].values\n",
    "Y=np.concatenate((Y1, Y2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['PSQI_Q2_1', 'PSQI_Q6_1', 'PSQI_Q5f_1', 'PSQI_C5_1', 'PSQI_C4_1', 'PSQI_Q5a_1', 'PSQI_Q5b_1', 'PSQI_Q5e_1', 'PSQI_Q5c_1', 'PSQI_C3_1', 'PSQI_Q5i_1', 'PSQI_Q4_1', 'PSQI_Q7_1', 'PSQI_C7_1', 'PSQI_Q5j_1', 'PSQI_C2_1', 'PSQI_C6_1', 'PSQI_Q8_1', 'PSQI_Q5g_1', 'PSQI_Q9_1', 'PSQI_Q5d_1', 'PSQI_C1_1', 'PSQI_Q5h_1', 'PSQI_Q1_1', 'PSQI_Q3_1'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-a23f92400197>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m X1=psqi_df[['AGE','SEX','BMI_1','PSQI_TOTAL_1','PSQI_C1_1','PSQI_C2_1','PSQI_C3_1','PSQI_C4_1','PSQI_C5_1','PSQI_C6_1','PSQI_C7_1','PSQI_Q1_1','PSQI_Q2_1','PSQI_Q3_1','PSQI_Q4_1',\n\u001b[0m\u001b[0;32m      2\u001b[0m             'PSQI_Q5a_1','PSQI_Q5b_1','PSQI_Q5c_1','PSQI_Q5d_1','PSQI_Q5e_1','PSQI_Q5f_1','PSQI_Q5g_1','PSQI_Q5h_1','PSQI_Q5i_1','PSQI_Q5j_1','PSQI_Q6_1','PSQI_Q7_1','PSQI_Q8_1','PSQI_Q9_1']].values\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m X2=psqi_df[['AGE','SEX','BMI_2','PSQI_TOTAL_2','PSQI_C1_2','PSQI_C2_2','PSQI_C3_2','PSQI_C4_2','PSQI_C5_2','PSQI_C6_2','PSQI_C7_2','PSQI_Q1_2','PSQI_Q2_2','PSQI_Q3_2','PSQI_Q4_2',\n\u001b[0;32m      5\u001b[0m             'PSQI_Q5a_2','PSQI_Q5b_2','PSQI_Q5c_2','PSQI_Q5d_2','PSQI_Q5e_2','PSQI_Q5f_2','PSQI_Q5g_2','PSQI_Q5h_2','PSQI_Q5i_2','PSQI_Q5j_2','PSQI_Q6_2','PSQI_Q7_2','PSQI_Q8_2','PSQI_Q9_2']].values\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2906\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2907\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2908\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2910\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1254\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1255\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1302\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1305\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[1;31m# we skip the warning on Categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['PSQI_Q2_1', 'PSQI_Q6_1', 'PSQI_Q5f_1', 'PSQI_C5_1', 'PSQI_C4_1', 'PSQI_Q5a_1', 'PSQI_Q5b_1', 'PSQI_Q5e_1', 'PSQI_Q5c_1', 'PSQI_C3_1', 'PSQI_Q5i_1', 'PSQI_Q4_1', 'PSQI_Q7_1', 'PSQI_C7_1', 'PSQI_Q5j_1', 'PSQI_C2_1', 'PSQI_C6_1', 'PSQI_Q8_1', 'PSQI_Q5g_1', 'PSQI_Q9_1', 'PSQI_Q5d_1', 'PSQI_C1_1', 'PSQI_Q5h_1', 'PSQI_Q1_1', 'PSQI_Q3_1'] not in index\""
     ]
    }
   ],
   "source": [
    "X1=psqi_df[['AGE','SEX','BMI_1','PSQI_TOTAL_1','PSQI_C1_1','PSQI_C2_1','PSQI_C3_1','PSQI_C4_1','PSQI_C5_1','PSQI_C6_1','PSQI_C7_1','PSQI_Q1_1','PSQI_Q2_1','PSQI_Q3_1','PSQI_Q4_1',\n",
    "            'PSQI_Q5a_1','PSQI_Q5b_1','PSQI_Q5c_1','PSQI_Q5d_1','PSQI_Q5e_1','PSQI_Q5f_1','PSQI_Q5g_1','PSQI_Q5h_1','PSQI_Q5i_1','PSQI_Q5j_1','PSQI_Q6_1','PSQI_Q7_1','PSQI_Q8_1','PSQI_Q9_1']].values\n",
    "            \n",
    "X2=psqi_df[['AGE','SEX','BMI_2','PSQI_TOTAL_2','PSQI_C1_2','PSQI_C2_2','PSQI_C3_2','PSQI_C4_2','PSQI_C5_2','PSQI_C6_2','PSQI_C7_2','PSQI_Q1_2','PSQI_Q2_2','PSQI_Q3_2','PSQI_Q4_2',\n",
    "            'PSQI_Q5a_2','PSQI_Q5b_2','PSQI_Q5c_2','PSQI_Q5d_2','PSQI_Q5e_2','PSQI_Q5f_2','PSQI_Q5g_2','PSQI_Q5h_2','PSQI_Q5i_2','PSQI_Q5j_2','PSQI_Q6_2','PSQI_Q7_2','PSQI_Q8_2','PSQI_Q9_2']].values\n",
    "            \n",
    "X=np.concatenate((X1, X2), axis=0)\n",
    "            \n",
    "#y 배열 생성 (y=HDL)\n",
    "Y1= psqi_df[['HDL_1']].values\n",
    "Y2= psqi_df[['HDL_2']].values\n",
    "Y=np.concatenate((Y1, Y2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규화 (변수간의 스케일 차이)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.asarray(X).astype(np.float)\n",
    "Y=np.asarray(Y).astype(np.float)\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,train_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=X.shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define model\n",
    "model=Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=dim))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='rmsprop',metrics=['mse'])\n",
    "\n",
    "#fit model\n",
    "history=model.fit(x_train, y_train, epochs=150, batch_size=4, verbose=2)\n",
    "model.evaluate(x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_p = model.predict(x_train)\n",
    "y_test_p = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_train,y_train_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_test,y_test_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list=np.array(y_train).flatten().tolist() #y_train 리스트\n",
    "y_test_list=np.array(y_test).flatten().tolist() #y_test 리스트\n",
    "y_p_train_list=np.array(y_train_p).flatten().tolist() #y_train 예측 리스트\n",
    "y_p_test_list=np.array(y_test_p).flatten().tolist() #y_test 예측 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#오차 범위 3 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#평균 성능 테스트\n",
    "scores = 0\n",
    "mean=np.mean(Y, axis=0)\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= mean <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"train set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= mean <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"test set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######입력용#######\n",
    "\n",
    "#오차 범위 3 설정\n",
    "print('### <오차범위 3>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "print('### <오차범위 5>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "print('### <오차범위 10>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "print('### <오차범위 20>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x 배열 생성 (선별)\n",
    "X1=psqi_df[['SEX','AGE','PSQI_TOTAL_1','BMI_1','WBC_1','GLU0_1','ALT_1','TG_1','LDL_1',\n",
    "            'Muscle_1','Fat_1_x','SBP_1','DBP_1','HR_1','Waist_1']].values\n",
    "X2=psqi_df[['SEX','AGE','PSQI_TOTAL_2','BMI_2','WBC_2','GLU0_2','ALT_2','TG_2','LDL_2',\n",
    "            'Muscle_2','Fat_2_x','SBP_2','DBP_2','HR_2','Waist_2']].values\n",
    "X=np.concatenate((X1, X2), axis=0)\n",
    "\n",
    "#y 배열 생성 (y=HDL)\n",
    "Y1= psqi_df[['HDL_1']].values\n",
    "Y2= psqi_df[['HDL_2']].values\n",
    "Y=np.concatenate((Y1, Y2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규화 (변수간의 스케일 차이)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.asarray(X).astype(np.float)\n",
    "Y=np.asarray(Y).astype(np.float)\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,train_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 72)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((360, 15), (360, 1))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X.shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "72/72 - 0s - loss: 3424.0293 - mse: 3424.0293\n",
      "Epoch 2/150\n",
      "72/72 - 0s - loss: 2701.5935 - mse: 2701.5935\n",
      "Epoch 3/150\n",
      "72/72 - 0s - loss: 1638.4359 - mse: 1638.4359\n",
      "Epoch 4/150\n",
      "72/72 - 0s - loss: 691.1234 - mse: 691.1234\n",
      "Epoch 5/150\n",
      "72/72 - 0s - loss: 340.3846 - mse: 340.3846\n",
      "Epoch 6/150\n",
      "72/72 - 0s - loss: 285.9919 - mse: 285.9919\n",
      "Epoch 7/150\n",
      "72/72 - 0s - loss: 256.6147 - mse: 256.6147\n",
      "Epoch 8/150\n",
      "72/72 - 0s - loss: 234.9383 - mse: 234.9383\n",
      "Epoch 9/150\n",
      "72/72 - 0s - loss: 215.5215 - mse: 215.5215\n",
      "Epoch 10/150\n",
      "72/72 - 0s - loss: 203.5555 - mse: 203.5555\n",
      "Epoch 11/150\n",
      "72/72 - 0s - loss: 190.5338 - mse: 190.5338\n",
      "Epoch 12/150\n",
      "72/72 - 0s - loss: 182.8746 - mse: 182.8746\n",
      "Epoch 13/150\n",
      "72/72 - 0s - loss: 173.3569 - mse: 173.3569\n",
      "Epoch 14/150\n",
      "72/72 - 0s - loss: 166.9334 - mse: 166.9334\n",
      "Epoch 15/150\n",
      "72/72 - 0s - loss: 160.1434 - mse: 160.1434\n",
      "Epoch 16/150\n",
      "72/72 - 0s - loss: 155.4052 - mse: 155.4052\n",
      "Epoch 17/150\n",
      "72/72 - 0s - loss: 150.5592 - mse: 150.5592\n",
      "Epoch 18/150\n",
      "72/72 - 0s - loss: 145.8365 - mse: 145.8365\n",
      "Epoch 19/150\n",
      "72/72 - 0s - loss: 142.7325 - mse: 142.7325\n",
      "Epoch 20/150\n",
      "72/72 - 0s - loss: 140.5303 - mse: 140.5303\n",
      "Epoch 21/150\n",
      "72/72 - 0s - loss: 136.7305 - mse: 136.7305\n",
      "Epoch 22/150\n",
      "72/72 - 0s - loss: 134.3646 - mse: 134.3646\n",
      "Epoch 23/150\n",
      "72/72 - 0s - loss: 131.3466 - mse: 131.3466\n",
      "Epoch 24/150\n",
      "72/72 - 0s - loss: 129.3200 - mse: 129.3200\n",
      "Epoch 25/150\n",
      "72/72 - 0s - loss: 127.1032 - mse: 127.1032\n",
      "Epoch 26/150\n",
      "72/72 - 0s - loss: 123.6745 - mse: 123.6745\n",
      "Epoch 27/150\n",
      "72/72 - 0s - loss: 125.2975 - mse: 125.2975\n",
      "Epoch 28/150\n",
      "72/72 - 0s - loss: 122.5820 - mse: 122.5820\n",
      "Epoch 29/150\n",
      "72/72 - 0s - loss: 119.8092 - mse: 119.8092\n",
      "Epoch 30/150\n",
      "72/72 - 0s - loss: 119.9166 - mse: 119.9166\n",
      "Epoch 31/150\n",
      "72/72 - 0s - loss: 118.7407 - mse: 118.7407\n",
      "Epoch 32/150\n",
      "72/72 - 0s - loss: 118.2030 - mse: 118.2030\n",
      "Epoch 33/150\n",
      "72/72 - 0s - loss: 116.3837 - mse: 116.3837\n",
      "Epoch 34/150\n",
      "72/72 - 0s - loss: 113.7858 - mse: 113.7858\n",
      "Epoch 35/150\n",
      "72/72 - 0s - loss: 115.1621 - mse: 115.1621\n",
      "Epoch 36/150\n",
      "72/72 - 0s - loss: 113.6824 - mse: 113.6824\n",
      "Epoch 37/150\n",
      "72/72 - 0s - loss: 112.8146 - mse: 112.8146\n",
      "Epoch 38/150\n",
      "72/72 - 0s - loss: 112.1863 - mse: 112.1863\n",
      "Epoch 39/150\n",
      "72/72 - 0s - loss: 108.9134 - mse: 108.9134\n",
      "Epoch 40/150\n",
      "72/72 - 0s - loss: 109.8284 - mse: 109.8284\n",
      "Epoch 41/150\n",
      "72/72 - 0s - loss: 108.2295 - mse: 108.2295\n",
      "Epoch 42/150\n",
      "72/72 - 0s - loss: 108.7587 - mse: 108.7587\n",
      "Epoch 43/150\n",
      "72/72 - 0s - loss: 108.0963 - mse: 108.0963\n",
      "Epoch 44/150\n",
      "72/72 - 0s - loss: 106.0334 - mse: 106.0334\n",
      "Epoch 45/150\n",
      "72/72 - 0s - loss: 105.7368 - mse: 105.7368\n",
      "Epoch 46/150\n",
      "72/72 - 0s - loss: 104.0905 - mse: 104.0905\n",
      "Epoch 47/150\n",
      "72/72 - 0s - loss: 104.3768 - mse: 104.3768\n",
      "Epoch 48/150\n",
      "72/72 - 0s - loss: 103.1347 - mse: 103.1347\n",
      "Epoch 49/150\n",
      "72/72 - 0s - loss: 101.4499 - mse: 101.4499\n",
      "Epoch 50/150\n",
      "72/72 - 0s - loss: 102.3874 - mse: 102.3874\n",
      "Epoch 51/150\n",
      "72/72 - 0s - loss: 100.1616 - mse: 100.1616\n",
      "Epoch 52/150\n",
      "72/72 - 0s - loss: 100.7637 - mse: 100.7637\n",
      "Epoch 53/150\n",
      "72/72 - 0s - loss: 98.3104 - mse: 98.3104\n",
      "Epoch 54/150\n",
      "72/72 - 0s - loss: 98.5124 - mse: 98.5124\n",
      "Epoch 55/150\n",
      "72/72 - 0s - loss: 98.5154 - mse: 98.5154\n",
      "Epoch 56/150\n",
      "72/72 - 0s - loss: 98.4332 - mse: 98.4332\n",
      "Epoch 57/150\n",
      "72/72 - 0s - loss: 96.9220 - mse: 96.9220\n",
      "Epoch 58/150\n",
      "72/72 - 0s - loss: 94.6238 - mse: 94.6238\n",
      "Epoch 59/150\n",
      "72/72 - 0s - loss: 95.9211 - mse: 95.9211\n",
      "Epoch 60/150\n",
      "72/72 - 0s - loss: 95.8519 - mse: 95.8519\n",
      "Epoch 61/150\n",
      "72/72 - 0s - loss: 95.1265 - mse: 95.1265\n",
      "Epoch 62/150\n",
      "72/72 - 0s - loss: 94.1977 - mse: 94.1977\n",
      "Epoch 63/150\n",
      "72/72 - 0s - loss: 94.4620 - mse: 94.4620\n",
      "Epoch 64/150\n",
      "72/72 - 0s - loss: 93.5232 - mse: 93.5232\n",
      "Epoch 65/150\n",
      "72/72 - 0s - loss: 92.8122 - mse: 92.8122\n",
      "Epoch 66/150\n",
      "72/72 - 0s - loss: 91.7421 - mse: 91.7421\n",
      "Epoch 67/150\n",
      "72/72 - 0s - loss: 91.3957 - mse: 91.3957\n",
      "Epoch 68/150\n",
      "72/72 - 0s - loss: 91.2150 - mse: 91.2150\n",
      "Epoch 69/150\n",
      "72/72 - 0s - loss: 90.8873 - mse: 90.8873\n",
      "Epoch 70/150\n",
      "72/72 - 0s - loss: 90.8399 - mse: 90.8399\n",
      "Epoch 71/150\n",
      "72/72 - 0s - loss: 90.2666 - mse: 90.2666\n",
      "Epoch 72/150\n",
      "72/72 - 0s - loss: 90.4594 - mse: 90.4594\n",
      "Epoch 73/150\n",
      "72/72 - 0s - loss: 89.5333 - mse: 89.5333\n",
      "Epoch 74/150\n",
      "72/72 - 0s - loss: 87.8298 - mse: 87.8298\n",
      "Epoch 75/150\n",
      "72/72 - 0s - loss: 88.6685 - mse: 88.6685\n",
      "Epoch 76/150\n",
      "72/72 - 0s - loss: 87.6541 - mse: 87.6541\n",
      "Epoch 77/150\n",
      "72/72 - 0s - loss: 87.0916 - mse: 87.0916\n",
      "Epoch 78/150\n",
      "72/72 - 0s - loss: 86.3646 - mse: 86.3646\n",
      "Epoch 79/150\n",
      "72/72 - 0s - loss: 86.3349 - mse: 86.3349\n",
      "Epoch 80/150\n",
      "72/72 - 0s - loss: 86.1031 - mse: 86.1031\n",
      "Epoch 81/150\n",
      "72/72 - 0s - loss: 86.3734 - mse: 86.3734\n",
      "Epoch 82/150\n",
      "72/72 - 0s - loss: 84.9846 - mse: 84.9846\n",
      "Epoch 83/150\n",
      "72/72 - 0s - loss: 85.0497 - mse: 85.0497\n",
      "Epoch 84/150\n",
      "72/72 - 0s - loss: 83.2583 - mse: 83.2583\n",
      "Epoch 85/150\n",
      "72/72 - 0s - loss: 84.6294 - mse: 84.6294\n",
      "Epoch 86/150\n",
      "72/72 - 0s - loss: 82.5279 - mse: 82.5279\n",
      "Epoch 87/150\n",
      "72/72 - 0s - loss: 83.0111 - mse: 83.0111\n",
      "Epoch 88/150\n",
      "72/72 - 0s - loss: 81.8442 - mse: 81.8442\n",
      "Epoch 89/150\n",
      "72/72 - 0s - loss: 81.1297 - mse: 81.1297\n",
      "Epoch 90/150\n",
      "72/72 - 0s - loss: 81.4299 - mse: 81.4299\n",
      "Epoch 91/150\n",
      "72/72 - 0s - loss: 81.6742 - mse: 81.6742\n",
      "Epoch 92/150\n",
      "72/72 - 0s - loss: 80.2734 - mse: 80.2734\n",
      "Epoch 93/150\n",
      "72/72 - 0s - loss: 79.9230 - mse: 79.9230\n",
      "Epoch 94/150\n",
      "72/72 - 0s - loss: 80.6684 - mse: 80.6684\n",
      "Epoch 95/150\n",
      "72/72 - 0s - loss: 79.6231 - mse: 79.6231\n",
      "Epoch 96/150\n",
      "72/72 - 0s - loss: 78.1936 - mse: 78.1936\n",
      "Epoch 97/150\n",
      "72/72 - 0s - loss: 78.9784 - mse: 78.9784\n",
      "Epoch 98/150\n",
      "72/72 - 0s - loss: 78.5311 - mse: 78.5311\n",
      "Epoch 99/150\n",
      "72/72 - 0s - loss: 77.7374 - mse: 77.7374\n",
      "Epoch 100/150\n",
      "72/72 - 0s - loss: 77.9992 - mse: 77.9992\n",
      "Epoch 101/150\n",
      "72/72 - 0s - loss: 76.9510 - mse: 76.9510\n",
      "Epoch 102/150\n",
      "72/72 - 0s - loss: 76.1520 - mse: 76.1520\n",
      "Epoch 103/150\n",
      "72/72 - 0s - loss: 76.3490 - mse: 76.3490\n",
      "Epoch 104/150\n",
      "72/72 - 0s - loss: 76.5494 - mse: 76.5494\n",
      "Epoch 105/150\n",
      "72/72 - 0s - loss: 74.0312 - mse: 74.0312\n",
      "Epoch 106/150\n",
      "72/72 - 0s - loss: 74.0945 - mse: 74.0945\n",
      "Epoch 107/150\n",
      "72/72 - 0s - loss: 74.1068 - mse: 74.1068\n",
      "Epoch 108/150\n",
      "72/72 - 0s - loss: 73.3123 - mse: 73.3123\n",
      "Epoch 109/150\n",
      "72/72 - 0s - loss: 72.9909 - mse: 72.9909\n",
      "Epoch 110/150\n",
      "72/72 - 0s - loss: 73.0948 - mse: 73.0948\n",
      "Epoch 111/150\n",
      "72/72 - 0s - loss: 72.9609 - mse: 72.9609\n",
      "Epoch 112/150\n",
      "72/72 - 0s - loss: 72.3702 - mse: 72.3702\n",
      "Epoch 113/150\n",
      "72/72 - 0s - loss: 72.3406 - mse: 72.3406\n",
      "Epoch 114/150\n",
      "72/72 - 0s - loss: 71.8017 - mse: 71.8017\n",
      "Epoch 115/150\n",
      "72/72 - 0s - loss: 71.2321 - mse: 71.2321\n",
      "Epoch 116/150\n",
      "72/72 - 0s - loss: 70.5423 - mse: 70.5423\n",
      "Epoch 117/150\n",
      "72/72 - 0s - loss: 69.7052 - mse: 69.7052\n",
      "Epoch 118/150\n",
      "72/72 - 0s - loss: 69.3292 - mse: 69.3292\n",
      "Epoch 119/150\n",
      "72/72 - 0s - loss: 68.9819 - mse: 68.9819\n",
      "Epoch 120/150\n",
      "72/72 - 0s - loss: 68.6350 - mse: 68.6350\n",
      "Epoch 121/150\n",
      "72/72 - 0s - loss: 68.4291 - mse: 68.4291\n",
      "Epoch 122/150\n",
      "72/72 - 0s - loss: 68.4759 - mse: 68.4759\n",
      "Epoch 123/150\n",
      "72/72 - 0s - loss: 67.2248 - mse: 67.2248\n",
      "Epoch 124/150\n",
      "72/72 - 0s - loss: 67.3141 - mse: 67.3141\n",
      "Epoch 125/150\n",
      "72/72 - 0s - loss: 66.3027 - mse: 66.3027\n",
      "Epoch 126/150\n",
      "72/72 - 0s - loss: 67.0721 - mse: 67.0721\n",
      "Epoch 127/150\n",
      "72/72 - 0s - loss: 65.1359 - mse: 65.1359\n",
      "Epoch 128/150\n",
      "72/72 - 0s - loss: 65.8530 - mse: 65.8530\n",
      "Epoch 129/150\n",
      "72/72 - 0s - loss: 65.2932 - mse: 65.2932\n",
      "Epoch 130/150\n",
      "72/72 - 0s - loss: 64.4596 - mse: 64.4596\n",
      "Epoch 131/150\n",
      "72/72 - 0s - loss: 63.8255 - mse: 63.8255\n",
      "Epoch 132/150\n",
      "72/72 - 0s - loss: 63.8169 - mse: 63.8169\n",
      "Epoch 133/150\n",
      "72/72 - 0s - loss: 62.7757 - mse: 62.7757\n",
      "Epoch 134/150\n",
      "72/72 - 0s - loss: 62.8339 - mse: 62.8339\n",
      "Epoch 135/150\n",
      "72/72 - 0s - loss: 62.3938 - mse: 62.3938\n",
      "Epoch 136/150\n",
      "72/72 - 0s - loss: 61.4490 - mse: 61.4490\n",
      "Epoch 137/150\n",
      "72/72 - 0s - loss: 61.9239 - mse: 61.9239\n",
      "Epoch 138/150\n",
      "72/72 - 0s - loss: 60.3843 - mse: 60.3843\n",
      "Epoch 139/150\n",
      "72/72 - 0s - loss: 61.5165 - mse: 61.5165\n",
      "Epoch 140/150\n",
      "72/72 - 0s - loss: 60.0452 - mse: 60.0452\n",
      "Epoch 141/150\n",
      "72/72 - 0s - loss: 59.5671 - mse: 59.5671\n",
      "Epoch 142/150\n",
      "72/72 - 0s - loss: 59.7812 - mse: 59.7812\n",
      "Epoch 143/150\n",
      "72/72 - 0s - loss: 58.0510 - mse: 58.0510\n",
      "Epoch 144/150\n",
      "72/72 - 0s - loss: 58.7250 - mse: 58.7250\n",
      "Epoch 145/150\n",
      "72/72 - 0s - loss: 58.9087 - mse: 58.9087\n",
      "Epoch 146/150\n",
      "72/72 - 0s - loss: 57.1502 - mse: 57.1502\n",
      "Epoch 147/150\n",
      "72/72 - 0s - loss: 57.8000 - mse: 57.8000\n",
      "Epoch 148/150\n",
      "72/72 - 0s - loss: 56.7864 - mse: 56.7864\n",
      "Epoch 149/150\n",
      "72/72 - 0s - loss: 56.6995 - mse: 56.6995\n",
      "Epoch 150/150\n",
      "72/72 - 0s - loss: 56.5299 - mse: 56.5299\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff226f91e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 147.5013 - mse: 147.5013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[147.50125122070312, 147.50125122070312]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define model\n",
    "model=Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=dim))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='rmsprop',metrics=['mse'])\n",
    "\n",
    "#fit model\n",
    "history=model.fit(x_train, y_train, epochs=150, batch_size=4, verbose=2)\n",
    "model.evaluate(x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 32)                512       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,601\n",
      "Trainable params: 1,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_p = model.predict(x_train)\n",
    "y_test_p = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_train,y_train_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_test,y_test_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list=np.array(y_train).flatten().tolist() #y_train 리스트\n",
    "y_test_list=np.array(y_test).flatten().tolist() #y_test 리스트\n",
    "y_p_train_list=np.array(y_train_p).flatten().tolist() #y_train 예측 리스트\n",
    "y_p_test_list=np.array(y_test_p).flatten().tolist() #y_test 예측 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#오차 범위 3 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set prediction accuracy: 50.35 %\n",
      "test set prediction accuracy: 56.94 %\n"
     ]
    }
   ],
   "source": [
    "#평균 성능 테스트\n",
    "scores = 0\n",
    "mean=np.mean(Y, axis=0)\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= mean <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"train set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= mean <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"test set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### <오차범위 3>\n",
      "- train set prediction accuracy(+-3): 40.62 % <br>\n",
      "- test set prediction accuracy(+-3): 19.44 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 5>\n",
      "- train set prediction accuracy(+-5): 58.68 % <br>\n",
      "- test set prediction accuracy(+-5): 37.50 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 10>\n",
      "- train set prediction accuracy(+-10): 86.81 % <br>\n",
      "- test set prediction accuracy(+-10): 65.28 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 20>\n",
      "- train set prediction accuracy(+-20): 97.92 % <br>\n",
      "- test set prediction accuracy(+-20): 90.28 % <br>\n"
     ]
    }
   ],
   "source": [
    "######입력용#######\n",
    "\n",
    "#오차 범위 3 설정\n",
    "print('### <오차범위 3>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "print('### <오차범위 5>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "print('### <오차범위 10>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "print('### <오차범위 20>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x 배열 생성 (선별)\n",
    "X1=psqi_df[['SEX','AGE','PSQI_TOTAL_1','BMI_1']].values\n",
    "X2=psqi_df[['SEX','AGE','PSQI_TOTAL_2','BMI_2']].values\n",
    "X=np.concatenate((X1, X2), axis=0)\n",
    "\n",
    "#y 배열 생성 (y=HDL)\n",
    "Y1= psqi_df[['HDL_1']].values\n",
    "Y2= psqi_df[['HDL_2']].values\n",
    "Y=np.concatenate((Y1, Y2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규화 (변수간의 스케일 차이)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.asarray(X).astype(np.float)\n",
    "Y=np.asarray(Y).astype(np.float)\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,train_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 72)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((360, 4), (360, 1))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X.shape[1]\n",
    "dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "72/72 - 0s - loss: 3531.8220 - mse: 3531.8220\n",
      "Epoch 2/150\n",
      "72/72 - 0s - loss: 3054.7214 - mse: 3054.7214\n",
      "Epoch 3/150\n",
      "72/72 - 0s - loss: 2238.6733 - mse: 2238.6733\n",
      "Epoch 4/150\n",
      "72/72 - 0s - loss: 1252.5398 - mse: 1252.5398\n",
      "Epoch 5/150\n",
      "72/72 - 0s - loss: 460.3199 - mse: 460.3199\n",
      "Epoch 6/150\n",
      "72/72 - 0s - loss: 205.6983 - mse: 205.6983\n",
      "Epoch 7/150\n",
      "72/72 - 0s - loss: 186.0065 - mse: 186.0065\n",
      "Epoch 8/150\n",
      "72/72 - 0s - loss: 180.1106 - mse: 180.1106\n",
      "Epoch 9/150\n",
      "72/72 - 0s - loss: 174.6033 - mse: 174.6033\n",
      "Epoch 10/150\n",
      "72/72 - 0s - loss: 169.9005 - mse: 169.9005\n",
      "Epoch 11/150\n",
      "72/72 - 0s - loss: 165.8472 - mse: 165.8472\n",
      "Epoch 12/150\n",
      "72/72 - 0s - loss: 162.3279 - mse: 162.3279\n",
      "Epoch 13/150\n",
      "72/72 - 0s - loss: 159.8842 - mse: 159.8842\n",
      "Epoch 14/150\n",
      "72/72 - 0s - loss: 155.0700 - mse: 155.0700\n",
      "Epoch 15/150\n",
      "72/72 - 0s - loss: 155.6598 - mse: 155.6598\n",
      "Epoch 16/150\n",
      "72/72 - 0s - loss: 152.3801 - mse: 152.3801\n",
      "Epoch 17/150\n",
      "72/72 - 0s - loss: 149.9518 - mse: 149.9518\n",
      "Epoch 18/150\n",
      "72/72 - 0s - loss: 149.4490 - mse: 149.4490\n",
      "Epoch 19/150\n",
      "72/72 - 0s - loss: 148.4580 - mse: 148.4580\n",
      "Epoch 20/150\n",
      "72/72 - 0s - loss: 146.8631 - mse: 146.8631\n",
      "Epoch 21/150\n",
      "72/72 - 0s - loss: 144.2969 - mse: 144.2969\n",
      "Epoch 22/150\n",
      "72/72 - 0s - loss: 142.7939 - mse: 142.7939\n",
      "Epoch 23/150\n",
      "72/72 - 0s - loss: 142.3629 - mse: 142.3629\n",
      "Epoch 24/150\n",
      "72/72 - 0s - loss: 143.2600 - mse: 143.2600\n",
      "Epoch 25/150\n",
      "72/72 - 0s - loss: 142.1041 - mse: 142.1041\n",
      "Epoch 26/150\n",
      "72/72 - 0s - loss: 141.5390 - mse: 141.5390\n",
      "Epoch 27/150\n",
      "72/72 - 0s - loss: 140.0781 - mse: 140.0781\n",
      "Epoch 28/150\n",
      "72/72 - 0s - loss: 140.3271 - mse: 140.3271\n",
      "Epoch 29/150\n",
      "72/72 - 0s - loss: 140.0654 - mse: 140.0654\n",
      "Epoch 30/150\n",
      "72/72 - 0s - loss: 139.7799 - mse: 139.7799\n",
      "Epoch 31/150\n",
      "72/72 - 0s - loss: 138.6152 - mse: 138.6152\n",
      "Epoch 32/150\n",
      "72/72 - 0s - loss: 139.5097 - mse: 139.5097\n",
      "Epoch 33/150\n",
      "72/72 - 0s - loss: 138.7264 - mse: 138.7264\n",
      "Epoch 34/150\n",
      "72/72 - 0s - loss: 137.8074 - mse: 137.8074\n",
      "Epoch 35/150\n",
      "72/72 - 0s - loss: 136.6092 - mse: 136.6092\n",
      "Epoch 36/150\n",
      "72/72 - 0s - loss: 137.0057 - mse: 137.0057\n",
      "Epoch 37/150\n",
      "72/72 - 0s - loss: 136.8052 - mse: 136.8052\n",
      "Epoch 38/150\n",
      "72/72 - 0s - loss: 135.8032 - mse: 135.8032\n",
      "Epoch 39/150\n",
      "72/72 - 0s - loss: 136.1573 - mse: 136.1573\n",
      "Epoch 40/150\n",
      "72/72 - 0s - loss: 136.7310 - mse: 136.7310\n",
      "Epoch 41/150\n",
      "72/72 - 0s - loss: 135.3709 - mse: 135.3709\n",
      "Epoch 42/150\n",
      "72/72 - 0s - loss: 136.4032 - mse: 136.4032\n",
      "Epoch 43/150\n",
      "72/72 - 0s - loss: 136.0844 - mse: 136.0844\n",
      "Epoch 44/150\n",
      "72/72 - 0s - loss: 135.6478 - mse: 135.6478\n",
      "Epoch 45/150\n",
      "72/72 - 0s - loss: 134.6451 - mse: 134.6451\n",
      "Epoch 46/150\n",
      "72/72 - 0s - loss: 135.8973 - mse: 135.8973\n",
      "Epoch 47/150\n",
      "72/72 - 0s - loss: 134.6506 - mse: 134.6506\n",
      "Epoch 48/150\n",
      "72/72 - 0s - loss: 134.6549 - mse: 134.6549\n",
      "Epoch 49/150\n",
      "72/72 - 0s - loss: 134.5326 - mse: 134.5326\n",
      "Epoch 50/150\n",
      "72/72 - 0s - loss: 133.7459 - mse: 133.7459\n",
      "Epoch 51/150\n",
      "72/72 - 0s - loss: 133.6761 - mse: 133.6761\n",
      "Epoch 52/150\n",
      "72/72 - 0s - loss: 133.8542 - mse: 133.8542\n",
      "Epoch 53/150\n",
      "72/72 - 0s - loss: 134.4184 - mse: 134.4184\n",
      "Epoch 54/150\n",
      "72/72 - 0s - loss: 133.8707 - mse: 133.8707\n",
      "Epoch 55/150\n",
      "72/72 - 0s - loss: 133.2470 - mse: 133.2470\n",
      "Epoch 56/150\n",
      "72/72 - 0s - loss: 133.1400 - mse: 133.1400\n",
      "Epoch 57/150\n",
      "72/72 - 0s - loss: 133.7696 - mse: 133.7696\n",
      "Epoch 58/150\n",
      "72/72 - 0s - loss: 133.4263 - mse: 133.4263\n",
      "Epoch 59/150\n",
      "72/72 - 0s - loss: 133.0328 - mse: 133.0328\n",
      "Epoch 60/150\n",
      "72/72 - 0s - loss: 132.9021 - mse: 132.9021\n",
      "Epoch 61/150\n",
      "72/72 - 0s - loss: 133.3478 - mse: 133.3478\n",
      "Epoch 62/150\n",
      "72/72 - 0s - loss: 133.1862 - mse: 133.1862\n",
      "Epoch 63/150\n",
      "72/72 - 0s - loss: 132.2140 - mse: 132.2140\n",
      "Epoch 64/150\n",
      "72/72 - 0s - loss: 132.1117 - mse: 132.1117\n",
      "Epoch 65/150\n",
      "72/72 - 0s - loss: 132.7717 - mse: 132.7717\n",
      "Epoch 66/150\n",
      "72/72 - 0s - loss: 132.6816 - mse: 132.6816\n",
      "Epoch 67/150\n",
      "72/72 - 0s - loss: 131.5593 - mse: 131.5593\n",
      "Epoch 68/150\n",
      "72/72 - 0s - loss: 132.3103 - mse: 132.3103\n",
      "Epoch 69/150\n",
      "72/72 - 0s - loss: 131.7446 - mse: 131.7446\n",
      "Epoch 70/150\n",
      "72/72 - 0s - loss: 130.8345 - mse: 130.8345\n",
      "Epoch 71/150\n",
      "72/72 - 0s - loss: 131.1528 - mse: 131.1528\n",
      "Epoch 72/150\n",
      "72/72 - 0s - loss: 130.4762 - mse: 130.4762\n",
      "Epoch 73/150\n",
      "72/72 - 0s - loss: 130.7784 - mse: 130.7784\n",
      "Epoch 74/150\n",
      "72/72 - 0s - loss: 131.5271 - mse: 131.5271\n",
      "Epoch 75/150\n",
      "72/72 - 0s - loss: 130.9761 - mse: 130.9761\n",
      "Epoch 76/150\n",
      "72/72 - 0s - loss: 131.5030 - mse: 131.5030\n",
      "Epoch 77/150\n",
      "72/72 - 0s - loss: 130.5981 - mse: 130.5981\n",
      "Epoch 78/150\n",
      "72/72 - 0s - loss: 131.7767 - mse: 131.7767\n",
      "Epoch 79/150\n",
      "72/72 - 0s - loss: 131.5707 - mse: 131.5707\n",
      "Epoch 80/150\n",
      "72/72 - 0s - loss: 131.0641 - mse: 131.0641\n",
      "Epoch 81/150\n",
      "72/72 - 0s - loss: 130.9543 - mse: 130.9543\n",
      "Epoch 82/150\n",
      "72/72 - 0s - loss: 130.0337 - mse: 130.0337\n",
      "Epoch 83/150\n",
      "72/72 - 0s - loss: 130.8813 - mse: 130.8813\n",
      "Epoch 84/150\n",
      "72/72 - 0s - loss: 131.2061 - mse: 131.2061\n",
      "Epoch 85/150\n",
      "72/72 - 0s - loss: 130.0447 - mse: 130.0447\n",
      "Epoch 86/150\n",
      "72/72 - 0s - loss: 130.8481 - mse: 130.8481\n",
      "Epoch 87/150\n",
      "72/72 - 0s - loss: 130.9672 - mse: 130.9672\n",
      "Epoch 88/150\n",
      "72/72 - 0s - loss: 129.8244 - mse: 129.8244\n",
      "Epoch 89/150\n",
      "72/72 - 0s - loss: 130.1069 - mse: 130.1069\n",
      "Epoch 90/150\n",
      "72/72 - 0s - loss: 130.3339 - mse: 130.3339\n",
      "Epoch 91/150\n",
      "72/72 - 0s - loss: 130.6467 - mse: 130.6467\n",
      "Epoch 92/150\n",
      "72/72 - 0s - loss: 129.5990 - mse: 129.5990\n",
      "Epoch 93/150\n",
      "72/72 - 0s - loss: 129.2189 - mse: 129.2189\n",
      "Epoch 94/150\n",
      "72/72 - 0s - loss: 129.4692 - mse: 129.4692\n",
      "Epoch 95/150\n",
      "72/72 - 0s - loss: 129.7962 - mse: 129.7962\n",
      "Epoch 96/150\n",
      "72/72 - 0s - loss: 129.6705 - mse: 129.6705\n",
      "Epoch 97/150\n",
      "72/72 - 0s - loss: 130.1646 - mse: 130.1646\n",
      "Epoch 98/150\n",
      "72/72 - 0s - loss: 129.1737 - mse: 129.1737\n",
      "Epoch 99/150\n",
      "72/72 - 0s - loss: 129.5122 - mse: 129.5122\n",
      "Epoch 100/150\n",
      "72/72 - 0s - loss: 129.2218 - mse: 129.2218\n",
      "Epoch 101/150\n",
      "72/72 - 0s - loss: 129.0569 - mse: 129.0569\n",
      "Epoch 102/150\n",
      "72/72 - 0s - loss: 129.1859 - mse: 129.1859\n",
      "Epoch 103/150\n",
      "72/72 - 0s - loss: 129.4247 - mse: 129.4247\n",
      "Epoch 104/150\n",
      "72/72 - 0s - loss: 129.8381 - mse: 129.8381\n",
      "Epoch 105/150\n",
      "72/72 - 0s - loss: 129.0988 - mse: 129.0988\n",
      "Epoch 106/150\n",
      "72/72 - 0s - loss: 128.8302 - mse: 128.8302\n",
      "Epoch 107/150\n",
      "72/72 - 0s - loss: 128.9120 - mse: 128.9120\n",
      "Epoch 108/150\n",
      "72/72 - 0s - loss: 129.6753 - mse: 129.6753\n",
      "Epoch 109/150\n",
      "72/72 - 0s - loss: 129.1057 - mse: 129.1057\n",
      "Epoch 110/150\n",
      "72/72 - 0s - loss: 128.8530 - mse: 128.8530\n",
      "Epoch 111/150\n",
      "72/72 - 0s - loss: 129.1424 - mse: 129.1424\n",
      "Epoch 112/150\n",
      "72/72 - 0s - loss: 126.8664 - mse: 126.8664\n",
      "Epoch 113/150\n",
      "72/72 - 0s - loss: 127.9852 - mse: 127.9852\n",
      "Epoch 114/150\n",
      "72/72 - 0s - loss: 129.3256 - mse: 129.3256\n",
      "Epoch 115/150\n",
      "72/72 - 0s - loss: 128.7607 - mse: 128.7607\n",
      "Epoch 116/150\n",
      "72/72 - 0s - loss: 128.9748 - mse: 128.9748\n",
      "Epoch 117/150\n",
      "72/72 - 0s - loss: 128.4440 - mse: 128.4440\n",
      "Epoch 118/150\n",
      "72/72 - 0s - loss: 127.5799 - mse: 127.5799\n",
      "Epoch 119/150\n",
      "72/72 - 0s - loss: 128.6908 - mse: 128.6908\n",
      "Epoch 120/150\n",
      "72/72 - 0s - loss: 128.7481 - mse: 128.7481\n",
      "Epoch 121/150\n",
      "72/72 - 0s - loss: 129.0218 - mse: 129.0218\n",
      "Epoch 122/150\n",
      "72/72 - 0s - loss: 128.1237 - mse: 128.1237\n",
      "Epoch 123/150\n",
      "72/72 - 0s - loss: 127.9551 - mse: 127.9551\n",
      "Epoch 124/150\n",
      "72/72 - 0s - loss: 127.9050 - mse: 127.9050\n",
      "Epoch 125/150\n",
      "72/72 - 0s - loss: 127.2003 - mse: 127.2003\n",
      "Epoch 126/150\n",
      "72/72 - 0s - loss: 128.8501 - mse: 128.8501\n",
      "Epoch 127/150\n",
      "72/72 - 0s - loss: 127.4912 - mse: 127.4912\n",
      "Epoch 128/150\n",
      "72/72 - 0s - loss: 128.6902 - mse: 128.6902\n",
      "Epoch 129/150\n",
      "72/72 - 0s - loss: 128.2757 - mse: 128.2757\n",
      "Epoch 130/150\n",
      "72/72 - 0s - loss: 128.1399 - mse: 128.1399\n",
      "Epoch 131/150\n",
      "72/72 - 0s - loss: 128.3289 - mse: 128.3289\n",
      "Epoch 132/150\n",
      "72/72 - 0s - loss: 128.1356 - mse: 128.1356\n",
      "Epoch 133/150\n",
      "72/72 - 0s - loss: 127.7214 - mse: 127.7214\n",
      "Epoch 134/150\n",
      "72/72 - 0s - loss: 128.4813 - mse: 128.4813\n",
      "Epoch 135/150\n",
      "72/72 - 0s - loss: 127.5943 - mse: 127.5943\n",
      "Epoch 136/150\n",
      "72/72 - 0s - loss: 128.4583 - mse: 128.4583\n",
      "Epoch 137/150\n",
      "72/72 - 0s - loss: 128.2209 - mse: 128.2209\n",
      "Epoch 138/150\n",
      "72/72 - 0s - loss: 126.8081 - mse: 126.8081\n",
      "Epoch 139/150\n",
      "72/72 - 0s - loss: 127.7336 - mse: 127.7336\n",
      "Epoch 140/150\n",
      "72/72 - 0s - loss: 127.8819 - mse: 127.8819\n",
      "Epoch 141/150\n",
      "72/72 - 0s - loss: 127.3049 - mse: 127.3049\n",
      "Epoch 142/150\n",
      "72/72 - 0s - loss: 127.5343 - mse: 127.5343\n",
      "Epoch 143/150\n",
      "72/72 - 0s - loss: 127.7412 - mse: 127.7412\n",
      "Epoch 144/150\n",
      "72/72 - 0s - loss: 127.6930 - mse: 127.6930\n",
      "Epoch 145/150\n",
      "72/72 - 0s - loss: 126.2257 - mse: 126.2257\n",
      "Epoch 146/150\n",
      "72/72 - 0s - loss: 127.0789 - mse: 127.0789\n",
      "Epoch 147/150\n",
      "72/72 - 0s - loss: 125.3375 - mse: 125.3375\n",
      "Epoch 148/150\n",
      "72/72 - 0s - loss: 128.0535 - mse: 128.0535\n",
      "Epoch 149/150\n",
      "72/72 - 0s - loss: 126.7628 - mse: 126.7628\n",
      "Epoch 150/150\n",
      "72/72 - 0s - loss: 125.7188 - mse: 125.7188\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7ff241163af0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 915us/step - loss: 125.1341 - mse: 125.1341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[125.13408660888672, 125.13408660888672]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define model\n",
    "model=Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=dim))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "model.compile(loss='mse',optimizer='rmsprop',metrics=['mse'])\n",
    "\n",
    "#fit model\n",
    "history=model.fit(x_train, y_train, epochs=150, batch_size=4, verbose=2)\n",
    "model.evaluate(x_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,249\n",
      "Trainable params: 1,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_p = model.predict(x_train)\n",
    "y_test_p = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_train,y_train_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_test,y_test_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list=np.array(y_train).flatten().tolist() #y_train 리스트\n",
    "y_test_list=np.array(y_test).flatten().tolist() #y_test 리스트\n",
    "y_p_train_list=np.array(y_train_p).flatten().tolist() #y_train 예측 리스트\n",
    "y_p_test_list=np.array(y_test_p).flatten().tolist() #y_test 예측 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#오차 범위 3 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set prediction accuracy: 50.35 %\n",
      "test set prediction accuracy: 56.94 %\n"
     ]
    }
   ],
   "source": [
    "#평균 성능 테스트\n",
    "scores = 0\n",
    "mean=np.mean(Y, axis=0)\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= mean <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"train set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= mean <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"test set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### <오차범위 3>\n",
      "- train set prediction accuracy(+-3): 21.53 % <br>\n",
      "- test set prediction accuracy(+-3): 23.61 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 5>\n",
      "- train set prediction accuracy(+-5): 39.24 % <br>\n",
      "- test set prediction accuracy(+-5): 33.33 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 10>\n",
      "- train set prediction accuracy(+-10): 67.36 % <br>\n",
      "- test set prediction accuracy(+-10): 68.06 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 20>\n",
      "- train set prediction accuracy(+-20): 92.36 % <br>\n",
      "- test set prediction accuracy(+-20): 93.06 % <br>\n"
     ]
    }
   ],
   "source": [
    "######입력용#######\n",
    "\n",
    "#오차 범위 3 설정\n",
    "print('### <오차범위 3>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "print('### <오차범위 5>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "print('### <오차범위 10>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "print('### <오차범위 20>')\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다쓴거\n",
    "### <오차범위 3>\n",
    "- train_all set prediction accuracy(+-3): 88.89 % <br>\n",
    "- test_all set prediction accuracy(+-3): 31.94 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 5>\n",
    "- train_all set prediction accuracy(+-5): 96.53 % <br>\n",
    "- test_all set prediction accuracy(+-5): 55.56 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 10>\n",
    "- train_all set prediction accuracy(+-10): 100.00 % <br>\n",
    "- test_all set prediction accuracy(+-10): 83.33 % <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다안쓴거\n",
    "### <오차범위 3>\n",
    "- train_some set prediction accuracy(+-3): 32.99 % <br>\n",
    "- test_some set prediction accuracy(+-3): 27.78 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 5>\n",
    "- train_some set prediction accuracy(+-5): 54.86 % <br>\n",
    "- test_some set prediction accuracy(+-5): 40.28 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 10>\n",
    "- train_some set prediction accuracy(+-10): 82.29 % <br>\n",
    "- test_some set prediction accuracy(+-10): 59.72 % <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
