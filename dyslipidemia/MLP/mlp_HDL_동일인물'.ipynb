{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FONT 깨질때 폰트깨질때\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname = \"C:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font',family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sample_ID GROUP  COHORT SEX  AGE     Trait FitbitOX      Fitbit_ID  HTN  \\\n",
      "0         S0001   SMI       2   M   60  fitbit_O        O  sevrance00001  NaN   \n",
      "1         S0002   SMI       2   M   61  fitbit_O        O  sevrance00002  NaN   \n",
      "2         S0003   SMI       2   F   52  fitbit_O        O  sevrance00003  NaN   \n",
      "3         S0004   SMI       2   F   41  fitbit_O        O  sevrance00004  NaN   \n",
      "4         S0005   SMI       2   F   41  fitbit_O        O  sevrance00005  NaN   \n",
      "..          ...   ...     ...  ..  ...       ...      ...            ...  ...   \n",
      "383  MetS_S0280  MetS       1   F   24  fitbit_O        O   gnfmmets+139  NaN   \n",
      "384  MetS_S0281  MetS       1   F   44  fitbit_O        O   gnfmmets+140  NaN   \n",
      "385  MetS_S0282  MetS       1   F   37  fitbit_O        O   gnfmmets+141  1.0   \n",
      "386  MetS_S0283  MetS       1   M   51  fitbit_X        X              X  NaN   \n",
      "387  MetS_S0284  MetS       1   F   42  fitbit_X        X              X  NaN   \n",
      "\n",
      "      DM  ...  BDI_Q13_2 BDI_Q14_2  BDI_Q15_2  BDI_Q16_2 BDI_Q17_2 BDI_Q18_2  \\\n",
      "0    NaN  ...        1.0       1.0        1.0        2.0       1.0       1.0   \n",
      "1    1.0  ...        1.0       1.0        1.0        1.0       2.0       1.0   \n",
      "2    NaN  ...        1.0       1.0        1.0        1.0       1.0       1.0   \n",
      "3    NaN  ...        2.0       2.0        2.0        2.0       2.0       2.0   \n",
      "4    NaN  ...        2.0       1.0        2.0        2.0       2.0       1.0   \n",
      "..   ...  ...        ...       ...        ...        ...       ...       ...   \n",
      "383  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "384  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "385  1.0  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "386  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "387  NaN  ...        NaN       NaN        NaN        NaN       NaN       NaN   \n",
      "\n",
      "     BDI_Q19_2  BDI_Q20_2  BDI_Q21_2  Diet_2  \n",
      "0          2.0        1.0        2.0     2.0  \n",
      "1          1.0        1.0        1.0     2.0  \n",
      "2          1.0        2.0        4.0     1.0  \n",
      "3          1.0        1.0        1.0     2.0  \n",
      "4          1.0        2.0        1.0     2.0  \n",
      "..         ...        ...        ...     ...  \n",
      "383        NaN        NaN        NaN     NaN  \n",
      "384        NaN        NaN        NaN     NaN  \n",
      "385        NaN        NaN        NaN     NaN  \n",
      "386        NaN        NaN        NaN     NaN  \n",
      "387        NaN        NaN        NaN     NaN  \n",
      "\n",
      "[388 rows x 3527 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('./최종 데이터 그래프그리기용.xlsx') \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df=df[['Sample_ID','GROUP','COHORT','AGE','SEX','BMI_1','PSQI_TOTAL_1','Insulin _1','CRP_1','WBC_1','Neutrophil_1','Lym_1','GLU0_1','Creatinine_1','AST_1','ALT_1','TG_1','LDL_1','Muscle_1','Fat_1_x','FatPercentage _1','WHR_1','SBP_1',\n",
    "            'DBP_1','HR_1','Waist_1','Chol_1','BUN_1','HDL_1',\n",
    "          'BMI_2','PSQI_TOTAL_2','Insulin _2','CRP_2','WBC_2','Neutrophil_2','Lym_2','GLU0_2',\n",
    "            'Creatinine_2','AST_2','ALT_2','TG_2','LDL_2','Muscle_2','Fat_2_x','FatPercentage_2','WHR_2','SBP_2',\n",
    "            'DBP_2','HR_2','Waist_2','Chol_2','BUN_2','HDL_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>COHORT</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Fat_2_x</th>\n",
       "      <th>FatPercentage_2</th>\n",
       "      <th>WHR_2</th>\n",
       "      <th>SBP_2</th>\n",
       "      <th>DBP_2</th>\n",
       "      <th>HR_2</th>\n",
       "      <th>Waist_2</th>\n",
       "      <th>Chol_2</th>\n",
       "      <th>BUN_2</th>\n",
       "      <th>HDL_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0001</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>M</td>\n",
       "      <td>21.110190</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.91</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>108.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.1</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0002</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>27.782064</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.51</td>\n",
       "      <td>...</td>\n",
       "      <td>19.9</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.99</td>\n",
       "      <td>138.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>90.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.2</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0003</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>24.944742</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.85</td>\n",
       "      <td>...</td>\n",
       "      <td>22.6</td>\n",
       "      <td>36.7</td>\n",
       "      <td>0.89</td>\n",
       "      <td>127.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>86.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.1</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>22.620489</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.14</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.9</td>\n",
       "      <td>0.82</td>\n",
       "      <td>119.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.2</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0005</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>20.524157</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.93</td>\n",
       "      <td>...</td>\n",
       "      <td>14.9</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.80</td>\n",
       "      <td>110.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>66.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.5</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>MetS_S0280</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>34.803410</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5.32</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>MetS_S0281</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>F</td>\n",
       "      <td>30.903615</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.82</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>MetS_S0282</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>F</td>\n",
       "      <td>28.676533</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>MetS_S0283</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>M</td>\n",
       "      <td>24.549738</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>6.67</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>MetS_S0284</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>F</td>\n",
       "      <td>24.605921</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.03</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample_ID GROUP  COHORT  AGE SEX      BMI_1  PSQI_TOTAL_1 Insulin _1  \\\n",
       "0         S0001   SMI       2   60   M  21.110190           8.0        7.7   \n",
       "1         S0002   SMI       2   61   M  27.782064           4.0        5.4   \n",
       "2         S0003   SMI       2   52   F  24.944742           3.0        5.1   \n",
       "3         S0004   SMI       2   41   F  22.620489           6.0        4.2   \n",
       "4         S0005   SMI       2   41   F  20.524157          10.0        3.2   \n",
       "..          ...   ...     ...  ...  ..        ...           ...        ...   \n",
       "383  MetS_S0280  MetS       1   24   F  34.803410           5.0       11.3   \n",
       "384  MetS_S0281  MetS       1   44   F  30.903615           3.0       10.6   \n",
       "385  MetS_S0282  MetS       1   37   F  28.676533           3.0       12.2   \n",
       "386  MetS_S0283  MetS       1   51   M  24.549738           5.0       10.4   \n",
       "387  MetS_S0284  MetS       1   42   F  24.605921           3.0       10.1   \n",
       "\n",
       "    CRP_1  WBC_1  ...  Fat_2_x  FatPercentage_2  WHR_2  SBP_2  DBP_2  HR_2  \\\n",
       "0     0.2   3.91  ...      9.7             15.9   0.89  108.0   78.0  87.0   \n",
       "1     0.2   5.51  ...     19.9             27.9   0.99  138.0   92.0  73.0   \n",
       "2     0.7   4.85  ...     22.6             36.7   0.89  127.0   80.0  66.0   \n",
       "3     0.6   6.14  ...     16.0             30.9   0.82  119.0   83.0  77.0   \n",
       "4     0.1   4.93  ...     14.9             26.8   0.80  110.0   68.0  67.0   \n",
       "..    ...    ...  ...      ...              ...    ...    ...    ...   ...   \n",
       "383   0.4   5.32  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "384   2.3   5.82  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "385     1   6.18  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "386   1.2   6.67  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "387   0.8   7.03  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "\n",
       "     Waist_2  Chol_2  BUN_2  HDL_2  \n",
       "0       83.0     NaN   13.1   77.0  \n",
       "1       90.5     NaN   19.2   59.0  \n",
       "2       86.5     NaN   17.1   40.0  \n",
       "3       77.0     NaN   12.2   54.0  \n",
       "4       66.5     NaN   16.5   72.0  \n",
       "..       ...     ...    ...    ...  \n",
       "383      NaN     NaN    NaN    NaN  \n",
       "384      NaN     NaN    NaN    NaN  \n",
       "385      NaN     NaN    NaN    NaN  \n",
       "386      NaN     NaN    NaN    NaN  \n",
       "387      NaN     NaN    NaN    NaN  \n",
       "\n",
       "[388 rows x 53 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>COHORT</th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Fat_2_x</th>\n",
       "      <th>FatPercentage_2</th>\n",
       "      <th>WHR_2</th>\n",
       "      <th>SBP_2</th>\n",
       "      <th>DBP_2</th>\n",
       "      <th>HR_2</th>\n",
       "      <th>Waist_2</th>\n",
       "      <th>Chol_2</th>\n",
       "      <th>BUN_2</th>\n",
       "      <th>HDL_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0001</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>M</td>\n",
       "      <td>21.110190</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.91</td>\n",
       "      <td>...</td>\n",
       "      <td>9.7</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.89</td>\n",
       "      <td>108.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.1</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0002</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>27.782064</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.51</td>\n",
       "      <td>...</td>\n",
       "      <td>19.9</td>\n",
       "      <td>27.9</td>\n",
       "      <td>0.99</td>\n",
       "      <td>138.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>90.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.2</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0003</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>24.944742</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>4.85</td>\n",
       "      <td>...</td>\n",
       "      <td>22.6</td>\n",
       "      <td>36.7</td>\n",
       "      <td>0.89</td>\n",
       "      <td>127.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>86.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.1</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>22.620489</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.14</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>30.9</td>\n",
       "      <td>0.82</td>\n",
       "      <td>119.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.2</td>\n",
       "      <td>54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0005</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>F</td>\n",
       "      <td>20.524157</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4.93</td>\n",
       "      <td>...</td>\n",
       "      <td>14.9</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.80</td>\n",
       "      <td>110.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>66.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.5</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>MetS_S0280</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>F</td>\n",
       "      <td>34.803410</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5.32</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>MetS_S0281</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>F</td>\n",
       "      <td>30.903615</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>5.82</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>MetS_S0282</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>F</td>\n",
       "      <td>28.676533</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>1</td>\n",
       "      <td>6.18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>MetS_S0283</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>M</td>\n",
       "      <td>24.549738</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>6.67</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>MetS_S0284</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>F</td>\n",
       "      <td>24.605921</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.03</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample_ID GROUP  COHORT  AGE SEX      BMI_1  PSQI_TOTAL_1 Insulin _1  \\\n",
       "0         S0001   SMI       2   60   M  21.110190           8.0        7.7   \n",
       "1         S0002   SMI       2   61   M  27.782064           4.0        5.4   \n",
       "2         S0003   SMI       2   52   F  24.944742           3.0        5.1   \n",
       "3         S0004   SMI       2   41   F  22.620489           6.0        4.2   \n",
       "4         S0005   SMI       2   41   F  20.524157          10.0        3.2   \n",
       "..          ...   ...     ...  ...  ..        ...           ...        ...   \n",
       "383  MetS_S0280  MetS       1   24   F  34.803410           5.0       11.3   \n",
       "384  MetS_S0281  MetS       1   44   F  30.903615           3.0       10.6   \n",
       "385  MetS_S0282  MetS       1   37   F  28.676533           3.0       12.2   \n",
       "386  MetS_S0283  MetS       1   51   M  24.549738           5.0       10.4   \n",
       "387  MetS_S0284  MetS       1   42   F  24.605921           3.0       10.1   \n",
       "\n",
       "    CRP_1  WBC_1  ...  Fat_2_x  FatPercentage_2  WHR_2  SBP_2  DBP_2  HR_2  \\\n",
       "0     0.2   3.91  ...      9.7             15.9   0.89  108.0   78.0  87.0   \n",
       "1     0.2   5.51  ...     19.9             27.9   0.99  138.0   92.0  73.0   \n",
       "2     0.7   4.85  ...     22.6             36.7   0.89  127.0   80.0  66.0   \n",
       "3     0.6   6.14  ...     16.0             30.9   0.82  119.0   83.0  77.0   \n",
       "4     0.1   4.93  ...     14.9             26.8   0.80  110.0   68.0  67.0   \n",
       "..    ...    ...  ...      ...              ...    ...    ...    ...   ...   \n",
       "383   0.4   5.32  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "384   2.3   5.82  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "385     1   6.18  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "386   1.2   6.67  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "387   0.8   7.03  ...      NaN              NaN    NaN    NaN    NaN   NaN   \n",
       "\n",
       "     Waist_2  Chol_2  BUN_2  HDL_2  \n",
       "0       83.0     NaN   13.1   77.0  \n",
       "1       90.5     NaN   19.2   59.0  \n",
       "2       86.5     NaN   17.1   40.0  \n",
       "3       77.0     NaN   12.2   54.0  \n",
       "4       66.5     NaN   16.5   72.0  \n",
       "..       ...     ...    ...    ...  \n",
       "383      NaN     NaN    NaN    NaN  \n",
       "384      NaN     NaN    NaN    NaN  \n",
       "385      NaN     NaN    NaN    NaN  \n",
       "386      NaN     NaN    NaN    NaN  \n",
       "387      NaN     NaN    NaN    NaN  \n",
       "\n",
       "[317 rows x 53 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#보조 호르몬 요법을 받고 있는 Cohort 3 제거 Filter 적용\n",
    "psqi_df = psqi_df[(psqi_df['COHORT'] != 3)]\n",
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df=psqi_df.dropna()\n",
    "psqi_df.reset_index(drop=True, inplace=True)\n",
    "psqi_df=psqi_df.drop([\"Sample_ID\", \"GROUP\", \"COHORT\"],axis=1)\n",
    "#1분, 매일다름, 정해진간이없음 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df[\"SEX\"] = psqi_df[\"SEX\"].apply(lambda x: 1. if x=='M' else 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df[\"Insulin _1\"] = psqi_df[\"Insulin _1\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)\n",
    "psqi_df[\"Insulin _2\"] = psqi_df[\"Insulin _2\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df[\"CRP_1\"] = psqi_df[\"CRP_1\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)\n",
    "psqi_df[\"CRP_2\"] = psqi_df[\"CRP_2\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>Neutrophil_1</th>\n",
       "      <th>Lym_1</th>\n",
       "      <th>GLU0_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Fat_2_x</th>\n",
       "      <th>FatPercentage_2</th>\n",
       "      <th>WHR_2</th>\n",
       "      <th>SBP_2</th>\n",
       "      <th>DBP_2</th>\n",
       "      <th>HR_2</th>\n",
       "      <th>Waist_2</th>\n",
       "      <th>Chol_2</th>\n",
       "      <th>BUN_2</th>\n",
       "      <th>HDL_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>180.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>38.366667</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>23.799644</td>\n",
       "      <td>5.105556</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>0.748889</td>\n",
       "      <td>5.844867</td>\n",
       "      <td>56.086111</td>\n",
       "      <td>34.113333</td>\n",
       "      <td>98.90000</td>\n",
       "      <td>...</td>\n",
       "      <td>19.053333</td>\n",
       "      <td>28.888333</td>\n",
       "      <td>0.862444</td>\n",
       "      <td>114.605556</td>\n",
       "      <td>72.477778</td>\n",
       "      <td>75.644444</td>\n",
       "      <td>81.328889</td>\n",
       "      <td>190.922222</td>\n",
       "      <td>12.984444</td>\n",
       "      <td>59.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.589776</td>\n",
       "      <td>0.461927</td>\n",
       "      <td>4.936177</td>\n",
       "      <td>2.893833</td>\n",
       "      <td>4.105985</td>\n",
       "      <td>1.344157</td>\n",
       "      <td>1.412280</td>\n",
       "      <td>8.502880</td>\n",
       "      <td>7.708889</td>\n",
       "      <td>14.43773</td>\n",
       "      <td>...</td>\n",
       "      <td>6.616151</td>\n",
       "      <td>7.098802</td>\n",
       "      <td>0.071696</td>\n",
       "      <td>13.213544</td>\n",
       "      <td>9.091991</td>\n",
       "      <td>10.306814</td>\n",
       "      <td>10.251265</td>\n",
       "      <td>32.017358</td>\n",
       "      <td>3.508550</td>\n",
       "      <td>14.01372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.231576</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.820000</td>\n",
       "      <td>34.500000</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>63.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.700000</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>29.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.833309</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>4.857500</td>\n",
       "      <td>50.525000</td>\n",
       "      <td>28.975000</td>\n",
       "      <td>92.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>24.275000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>73.875000</td>\n",
       "      <td>167.750000</td>\n",
       "      <td>10.675000</td>\n",
       "      <td>49.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.422889</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>5.720000</td>\n",
       "      <td>55.950000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>95.50000</td>\n",
       "      <td>...</td>\n",
       "      <td>17.950000</td>\n",
       "      <td>28.450000</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>80.500000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>12.700000</td>\n",
       "      <td>57.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>46.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25.502662</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>9.505000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>6.580000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>102.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>22.125000</td>\n",
       "      <td>33.450000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>77.250000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>14.600000</td>\n",
       "      <td>69.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>24.700000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>10.550000</td>\n",
       "      <td>78.400000</td>\n",
       "      <td>55.400000</td>\n",
       "      <td>182.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>46.100000</td>\n",
       "      <td>48.300000</td>\n",
       "      <td>1.070000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>296.000000</td>\n",
       "      <td>36.400000</td>\n",
       "      <td>116.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              AGE         SEX       BMI_1  PSQI_TOTAL_1  Insulin _1  \\\n",
       "count  180.000000  180.000000  180.000000    180.000000  180.000000   \n",
       "mean    38.366667    0.305556   23.799644      5.105556    7.700000   \n",
       "std     11.589776    0.461927    4.936177      2.893833    4.105985   \n",
       "min     20.000000    0.000000   15.231576      0.000000    0.100000   \n",
       "25%     29.000000    0.000000   20.833309      3.000000    5.000000   \n",
       "50%     35.500000    0.000000   23.422889      5.000000    6.500000   \n",
       "75%     46.000000    1.000000   25.502662      7.000000    9.505000   \n",
       "max     63.000000    1.000000   67.500000     14.000000   24.700000   \n",
       "\n",
       "            CRP_1       WBC_1  Neutrophil_1       Lym_1     GLU0_1  ...  \\\n",
       "count  180.000000  180.000000    180.000000  180.000000  180.00000  ...   \n",
       "mean     0.748889    5.844867     56.086111   34.113333   98.90000  ...   \n",
       "std      1.344157    1.412280      8.502880    7.708889   14.43773  ...   \n",
       "min      0.000000    2.820000     34.500000   15.100000   63.00000  ...   \n",
       "25%      0.200000    4.857500     50.525000   28.975000   92.00000  ...   \n",
       "50%      0.300000    5.720000     55.950000   34.000000   95.50000  ...   \n",
       "75%      0.700000    6.580000     62.000000   39.000000  102.00000  ...   \n",
       "max     11.100000   10.550000     78.400000   55.400000  182.00000  ...   \n",
       "\n",
       "          Fat_2_x  FatPercentage_2       WHR_2       SBP_2       DBP_2  \\\n",
       "count  180.000000       180.000000  180.000000  180.000000  180.000000   \n",
       "mean    19.053333        28.888333    0.862444  114.605556   72.477778   \n",
       "std      6.616151         7.098802    0.071696   13.213544    9.091991   \n",
       "min      7.700000        11.500000    0.700000   91.000000   57.000000   \n",
       "25%     14.200000        24.275000    0.820000  104.000000   67.000000   \n",
       "50%     17.950000        28.450000    0.850000  114.000000   71.000000   \n",
       "75%     22.125000        33.450000    0.900000  123.000000   77.250000   \n",
       "max     46.100000        48.300000    1.070000  158.000000  107.000000   \n",
       "\n",
       "             HR_2     Waist_2      Chol_2       BUN_2      HDL_2  \n",
       "count  180.000000  180.000000  180.000000  180.000000  180.00000  \n",
       "mean    75.644444   81.328889  190.922222   12.984444   59.20000  \n",
       "std     10.306814   10.251265   32.017358    3.508550   14.01372  \n",
       "min     54.000000   61.000000  109.000000    6.000000   29.00000  \n",
       "25%     68.000000   73.875000  167.750000   10.675000   49.00000  \n",
       "50%     75.000000   80.500000  188.000000   12.700000   57.00000  \n",
       "75%     82.000000   89.000000  211.000000   14.600000   69.00000  \n",
       "max    112.000000  118.000000  296.000000   36.400000  116.00000  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    125\n",
       "1.0     55\n",
       "Name: SEX, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df[\"SEX\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>PSQI_TOTAL_1</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>CRP_1</th>\n",
       "      <th>WBC_1</th>\n",
       "      <th>Neutrophil_1</th>\n",
       "      <th>Lym_1</th>\n",
       "      <th>GLU0_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Fat_2_x</th>\n",
       "      <th>FatPercentage_2</th>\n",
       "      <th>WHR_2</th>\n",
       "      <th>SBP_2</th>\n",
       "      <th>DBP_2</th>\n",
       "      <th>HR_2</th>\n",
       "      <th>Waist_2</th>\n",
       "      <th>Chol_2</th>\n",
       "      <th>BUN_2</th>\n",
       "      <th>HDL_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.097789</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.82</td>\n",
       "      <td>54.6</td>\n",
       "      <td>35.0</td>\n",
       "      <td>89</td>\n",
       "      <td>...</td>\n",
       "      <td>20.4</td>\n",
       "      <td>26.8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>131.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>88.5</td>\n",
       "      <td>180.0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.472213</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.35</td>\n",
       "      <td>0.7</td>\n",
       "      <td>5.46</td>\n",
       "      <td>44.3</td>\n",
       "      <td>43.7</td>\n",
       "      <td>90</td>\n",
       "      <td>...</td>\n",
       "      <td>14.5</td>\n",
       "      <td>18.6</td>\n",
       "      <td>0.84</td>\n",
       "      <td>126.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>14.4</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.744827</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.26</td>\n",
       "      <td>0.4</td>\n",
       "      <td>3.99</td>\n",
       "      <td>51.0</td>\n",
       "      <td>37.8</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>17.8</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.89</td>\n",
       "      <td>131.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.616175</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.84</td>\n",
       "      <td>39.1</td>\n",
       "      <td>42.1</td>\n",
       "      <td>81</td>\n",
       "      <td>...</td>\n",
       "      <td>12.8</td>\n",
       "      <td>21.9</td>\n",
       "      <td>0.78</td>\n",
       "      <td>102.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.437500</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.22</td>\n",
       "      <td>49.3</td>\n",
       "      <td>39.3</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>12.3</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.80</td>\n",
       "      <td>106.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>11.3</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.259585</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.20</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.78</td>\n",
       "      <td>42.3</td>\n",
       "      <td>47.3</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>27.3</td>\n",
       "      <td>39.3</td>\n",
       "      <td>0.94</td>\n",
       "      <td>134.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.630719</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.80</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.60</td>\n",
       "      <td>51.7</td>\n",
       "      <td>34.6</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>22.1</td>\n",
       "      <td>25.7</td>\n",
       "      <td>0.95</td>\n",
       "      <td>113.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>97.5</td>\n",
       "      <td>134.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>35</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.641274</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>0.4</td>\n",
       "      <td>6.34</td>\n",
       "      <td>55.9</td>\n",
       "      <td>34.9</td>\n",
       "      <td>87</td>\n",
       "      <td>...</td>\n",
       "      <td>17.5</td>\n",
       "      <td>29.9</td>\n",
       "      <td>0.84</td>\n",
       "      <td>107.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>80.5</td>\n",
       "      <td>147.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>61</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.421366</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.80</td>\n",
       "      <td>0.2</td>\n",
       "      <td>4.88</td>\n",
       "      <td>40.9</td>\n",
       "      <td>48.0</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>15.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.81</td>\n",
       "      <td>106.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>10.2</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>56</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.271653</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.28</td>\n",
       "      <td>75.7</td>\n",
       "      <td>15.1</td>\n",
       "      <td>125</td>\n",
       "      <td>...</td>\n",
       "      <td>9.3</td>\n",
       "      <td>13.1</td>\n",
       "      <td>0.85</td>\n",
       "      <td>104.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>36.4</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AGE  SEX      BMI_1  PSQI_TOTAL_1  Insulin _1  CRP_1  WBC_1  \\\n",
       "0     35  1.0  24.097789           5.0        5.57    0.0   5.82   \n",
       "1     46  1.0  23.472213           5.0        7.35    0.7   5.46   \n",
       "2     32  1.0  23.744827           2.0        9.26    0.4   3.99   \n",
       "3     33  0.0  20.616175           4.0        3.52    0.0   5.84   \n",
       "4     28  0.0  18.437500           3.0        2.86    0.0   4.22   \n",
       "..   ...  ...        ...           ...         ...    ...    ...   \n",
       "175   63  0.0  26.259585           3.0        4.20    0.2   4.78   \n",
       "176   57  1.0  28.630719           4.0        8.80    3.0   4.60   \n",
       "177   35  0.0  21.641274           1.0        6.30    0.4   6.34   \n",
       "178   61  0.0  20.421366           8.0        4.80    0.2   4.88   \n",
       "179   56  1.0  22.271653           1.0        9.00    0.2   6.28   \n",
       "\n",
       "     Neutrophil_1  Lym_1  GLU0_1  ...  Fat_2_x  FatPercentage_2  WHR_2  SBP_2  \\\n",
       "0            54.6   35.0      89  ...     20.4             26.8   1.00  131.0   \n",
       "1            44.3   43.7      90  ...     14.5             18.6   0.84  126.0   \n",
       "2            51.0   37.8      96  ...     17.8             25.6   0.89  131.0   \n",
       "3            39.1   42.1      81  ...     12.8             21.9   0.78  102.0   \n",
       "4            49.3   39.3      63  ...     12.3             25.6   0.80  106.0   \n",
       "..            ...    ...     ...  ...      ...              ...    ...    ...   \n",
       "175          42.3   47.3      96  ...     27.3             39.3   0.94  134.0   \n",
       "176          51.7   34.6      94  ...     22.1             25.7   0.95  113.0   \n",
       "177          55.9   34.9      87  ...     17.5             29.9   0.84  107.0   \n",
       "178          40.9   48.0      93  ...     15.3             29.0   0.81  106.0   \n",
       "179          75.7   15.1     125  ...      9.3             13.1   0.85  104.0   \n",
       "\n",
       "     DBP_2   HR_2  Waist_2  Chol_2  BUN_2  HDL_2  \n",
       "0     74.0   66.0     88.5   180.0   17.5   53.0  \n",
       "1     87.0  108.0     85.0   203.0   14.4   64.0  \n",
       "2     77.0   87.0     81.0   196.0   14.1   49.0  \n",
       "3     62.0   70.0     69.0   224.0   10.5   98.0  \n",
       "4     72.0   69.0     61.0   168.0   11.3   71.0  \n",
       "..     ...    ...      ...     ...    ...    ...  \n",
       "175   89.0   81.0     98.0   141.0   17.1   66.0  \n",
       "176   76.0   66.0     97.5   134.0   14.6   51.0  \n",
       "177   72.0   64.0     80.5   147.0    9.7   49.0  \n",
       "178   76.0   92.0     79.0   134.0   10.2   60.0  \n",
       "179   73.0   79.0     91.0   148.0   36.4   31.0  \n",
       "\n",
       "[180 rows x 50 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x 배열 생성 (x=AGE, SEX, PSQI, BMI)\n",
    "X1=psqi_df[['AGE','SEX','BMI_1','PSQI_TOTAL_1','Insulin _1','CRP_1','WBC_1','Neutrophil_1','Lym_1','GLU0_1',\n",
    "            'Creatinine_1','AST_1','ALT_1','TG_1','LDL_1','Muscle_1','Fat_1_x','FatPercentage _1','WHR_1','SBP_1',\n",
    "            'DBP_1','HR_1','Waist_1','Chol_1','BUN_1']]\n",
    "\n",
    "X2=psqi_df[['AGE','SEX','BMI_2','PSQI_TOTAL_2','Insulin _2','CRP_2','WBC_2','Neutrophil_2','Lym_2','GLU0_2',\n",
    "            'Creatinine_2','AST_2','ALT_2','TG_2','LDL_2','Muscle_2','Fat_2_x','FatPercentage_2','WHR_2','SBP_2',\n",
    "            'DBP_2','HR_2','Waist_2','Chol_2','BUN_2']]\n",
    "X_all=np.concatenate((X1, X2), axis=0)\n",
    "\n",
    "#x 배열 생성 (선별)\n",
    "X1=psqi_df[['SEX','AGE','PSQI_TOTAL_1','BMI_1','WBC_1','GLU0_1','ALT_1','TG_1','LDL_1',\n",
    "            'Muscle_1','Fat_1_x','SBP_1','DBP_1','HR_1','Waist_1','PSQI_TOTAL_1']].values\n",
    "X2=psqi_df[['SEX','AGE','PSQI_TOTAL_2','BMI_2','WBC_2','GLU0_2','ALT_2','TG_2','LDL_2',\n",
    "            'Muscle_2','Fat_2_x','SBP_2','DBP_2','HR_2','Waist_2','PSQI_TOTAL_2']].values\n",
    "X_some=np.concatenate((X1, X2), axis=0)\n",
    "\n",
    "#y 배열 생성 (y=HDL)\n",
    "Y1= psqi_df[['HDL_1']].values\n",
    "Y2= psqi_df[['HDL_2']].values\n",
    "Y=np.concatenate((Y1, Y2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 360, 360)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_all), len(X_some), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((360, 25), (360, 16), (360, 1))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all.shape, X_some.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규화 (변수간의 스케일 차이)\n",
    "X_all = StandardScaler().fit_transform(X_all)\n",
    "X_some = StandardScaler().fit_transform(X_some)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all=np.asarray(X_all).astype(np.float)\n",
    "X_some=np.asarray(X_some).astype(np.float)\n",
    "Y=np.asarray(Y).astype(np.float)\n",
    "\n",
    "x_train_all,x_test_all,y_train_all,y_test_all=train_test_split(X_all,Y,train_size=0.8, random_state=100)\n",
    "x_train_some,x_test_some,y_train_some,y_test_some=train_test_split(X_some,Y,train_size=0.8, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 72)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_all), len(x_test_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(288, 72)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_some), len(x_test_some)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 16)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_1=X_all.shape[1]\n",
    "dim_2=X_some.shape[1]\n",
    "dim_1, dim_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "72/72 - 0s - loss: 3451.5381 - mse: 3451.5381\n",
      "Epoch 2/150\n",
      "72/72 - 0s - loss: 2750.6260 - mse: 2750.6260\n",
      "Epoch 3/150\n",
      "72/72 - 0s - loss: 1700.9032 - mse: 1700.9032\n",
      "Epoch 4/150\n",
      "72/72 - 0s - loss: 674.3023 - mse: 674.3023\n",
      "Epoch 5/150\n",
      "72/72 - 0s - loss: 293.7862 - mse: 293.7862\n",
      "Epoch 6/150\n",
      "72/72 - 0s - loss: 236.5044 - mse: 236.5044\n",
      "Epoch 7/150\n",
      "72/72 - 0s - loss: 206.9991 - mse: 206.9991\n",
      "Epoch 8/150\n",
      "72/72 - 0s - loss: 183.4375 - mse: 183.4375\n",
      "Epoch 9/150\n",
      "72/72 - 0s - loss: 166.1641 - mse: 166.1641\n",
      "Epoch 10/150\n",
      "72/72 - 0s - loss: 151.3691 - mse: 151.3691\n",
      "Epoch 11/150\n",
      "72/72 - 0s - loss: 139.3979 - mse: 139.3979\n",
      "Epoch 12/150\n",
      "72/72 - 0s - loss: 128.4589 - mse: 128.4589\n",
      "Epoch 13/150\n",
      "72/72 - 0s - loss: 117.2155 - mse: 117.2155\n",
      "Epoch 14/150\n",
      "72/72 - 0s - loss: 110.5089 - mse: 110.5089\n",
      "Epoch 15/150\n",
      "72/72 - 0s - loss: 103.1648 - mse: 103.1648\n",
      "Epoch 16/150\n",
      "72/72 - 0s - loss: 95.4880 - mse: 95.4880\n",
      "Epoch 17/150\n",
      "72/72 - 0s - loss: 89.8663 - mse: 89.8663\n",
      "Epoch 18/150\n",
      "72/72 - 0s - loss: 82.5243 - mse: 82.5243\n",
      "Epoch 19/150\n",
      "72/72 - 0s - loss: 79.5509 - mse: 79.5509\n",
      "Epoch 20/150\n",
      "72/72 - 0s - loss: 75.1003 - mse: 75.1003\n",
      "Epoch 21/150\n",
      "72/72 - 0s - loss: 71.0287 - mse: 71.0287\n",
      "Epoch 22/150\n",
      "72/72 - 0s - loss: 67.5258 - mse: 67.5258\n",
      "Epoch 23/150\n",
      "72/72 - 0s - loss: 62.5063 - mse: 62.5063\n",
      "Epoch 24/150\n",
      "72/72 - 0s - loss: 59.9002 - mse: 59.9002\n",
      "Epoch 25/150\n",
      "72/72 - 0s - loss: 56.7601 - mse: 56.7601\n",
      "Epoch 26/150\n",
      "72/72 - 0s - loss: 53.9127 - mse: 53.9127\n",
      "Epoch 27/150\n",
      "72/72 - 0s - loss: 51.4513 - mse: 51.4513\n",
      "Epoch 28/150\n",
      "72/72 - 0s - loss: 49.6507 - mse: 49.6507\n",
      "Epoch 29/150\n",
      "72/72 - 0s - loss: 47.8048 - mse: 47.8048\n",
      "Epoch 30/150\n",
      "72/72 - 0s - loss: 44.6667 - mse: 44.6667\n",
      "Epoch 31/150\n",
      "72/72 - 0s - loss: 43.5188 - mse: 43.5188\n",
      "Epoch 32/150\n",
      "72/72 - 0s - loss: 40.8883 - mse: 40.8883\n",
      "Epoch 33/150\n",
      "72/72 - 0s - loss: 39.8315 - mse: 39.8315\n",
      "Epoch 34/150\n",
      "72/72 - 0s - loss: 38.0186 - mse: 38.0186\n",
      "Epoch 35/150\n",
      "72/72 - 0s - loss: 36.2501 - mse: 36.2501\n",
      "Epoch 36/150\n",
      "72/72 - 0s - loss: 35.5688 - mse: 35.5688\n",
      "Epoch 37/150\n",
      "72/72 - 0s - loss: 34.4728 - mse: 34.4728\n",
      "Epoch 38/150\n",
      "72/72 - 0s - loss: 33.4233 - mse: 33.4233\n",
      "Epoch 39/150\n",
      "72/72 - 0s - loss: 31.9184 - mse: 31.9184\n",
      "Epoch 40/150\n",
      "72/72 - 0s - loss: 31.5463 - mse: 31.5463\n",
      "Epoch 41/150\n",
      "72/72 - 0s - loss: 30.2125 - mse: 30.2125\n",
      "Epoch 42/150\n",
      "72/72 - 0s - loss: 29.7756 - mse: 29.7756\n",
      "Epoch 43/150\n",
      "72/72 - 0s - loss: 29.0650 - mse: 29.0650\n",
      "Epoch 44/150\n",
      "72/72 - 0s - loss: 28.6617 - mse: 28.6617\n",
      "Epoch 45/150\n",
      "72/72 - 0s - loss: 27.7837 - mse: 27.7837\n",
      "Epoch 46/150\n",
      "72/72 - 0s - loss: 27.0882 - mse: 27.0882\n",
      "Epoch 47/150\n",
      "72/72 - 0s - loss: 26.6164 - mse: 26.6164\n",
      "Epoch 48/150\n",
      "72/72 - 0s - loss: 25.7966 - mse: 25.7966\n",
      "Epoch 49/150\n",
      "72/72 - 0s - loss: 25.4584 - mse: 25.4584\n",
      "Epoch 50/150\n",
      "72/72 - 0s - loss: 25.1273 - mse: 25.1273\n",
      "Epoch 51/150\n",
      "72/72 - 0s - loss: 24.1328 - mse: 24.1328\n",
      "Epoch 52/150\n",
      "72/72 - 0s - loss: 23.5716 - mse: 23.5716\n",
      "Epoch 53/150\n",
      "72/72 - 0s - loss: 23.6374 - mse: 23.6374\n",
      "Epoch 54/150\n",
      "72/72 - 0s - loss: 23.1803 - mse: 23.1803\n",
      "Epoch 55/150\n",
      "72/72 - 0s - loss: 23.0937 - mse: 23.0937\n",
      "Epoch 56/150\n",
      "72/72 - 0s - loss: 22.3961 - mse: 22.3961\n",
      "Epoch 57/150\n",
      "72/72 - 0s - loss: 22.1055 - mse: 22.1055\n",
      "Epoch 58/150\n",
      "72/72 - 0s - loss: 21.3600 - mse: 21.3600\n",
      "Epoch 59/150\n",
      "72/72 - 0s - loss: 21.5938 - mse: 21.5938\n",
      "Epoch 60/150\n",
      "72/72 - 0s - loss: 21.1805 - mse: 21.1805\n",
      "Epoch 61/150\n",
      "72/72 - 0s - loss: 20.3167 - mse: 20.3167\n",
      "Epoch 62/150\n",
      "72/72 - 0s - loss: 20.1730 - mse: 20.1730\n",
      "Epoch 63/150\n",
      "72/72 - 0s - loss: 19.6893 - mse: 19.6893\n",
      "Epoch 64/150\n",
      "72/72 - 0s - loss: 20.4840 - mse: 20.4840\n",
      "Epoch 65/150\n",
      "72/72 - 0s - loss: 19.2958 - mse: 19.2958\n",
      "Epoch 66/150\n",
      "72/72 - 0s - loss: 19.3232 - mse: 19.3232\n",
      "Epoch 67/150\n",
      "72/72 - 0s - loss: 19.0087 - mse: 19.0087\n",
      "Epoch 68/150\n",
      "72/72 - 0s - loss: 18.6667 - mse: 18.6667\n",
      "Epoch 69/150\n",
      "72/72 - 0s - loss: 18.5433 - mse: 18.5433\n",
      "Epoch 70/150\n",
      "72/72 - 0s - loss: 17.8122 - mse: 17.8122\n",
      "Epoch 71/150\n",
      "72/72 - 0s - loss: 18.2637 - mse: 18.2637\n",
      "Epoch 72/150\n",
      "72/72 - 0s - loss: 17.3486 - mse: 17.3486\n",
      "Epoch 73/150\n",
      "72/72 - 0s - loss: 16.6462 - mse: 16.6462\n",
      "Epoch 74/150\n",
      "72/72 - 0s - loss: 17.0037 - mse: 17.0037\n",
      "Epoch 75/150\n",
      "72/72 - 0s - loss: 17.2108 - mse: 17.2108\n",
      "Epoch 76/150\n",
      "72/72 - 0s - loss: 16.3251 - mse: 16.3251\n",
      "Epoch 77/150\n",
      "72/72 - 0s - loss: 16.2830 - mse: 16.2830\n",
      "Epoch 78/150\n",
      "72/72 - 0s - loss: 15.8305 - mse: 15.8305\n",
      "Epoch 79/150\n",
      "72/72 - 0s - loss: 15.5298 - mse: 15.5298\n",
      "Epoch 80/150\n",
      "72/72 - 0s - loss: 15.5695 - mse: 15.5695\n",
      "Epoch 81/150\n",
      "72/72 - 0s - loss: 15.3350 - mse: 15.3350\n",
      "Epoch 82/150\n",
      "72/72 - 0s - loss: 15.1827 - mse: 15.1827\n",
      "Epoch 83/150\n",
      "72/72 - 0s - loss: 14.9289 - mse: 14.9289\n",
      "Epoch 84/150\n",
      "72/72 - 0s - loss: 15.0121 - mse: 15.0121\n",
      "Epoch 85/150\n",
      "72/72 - 0s - loss: 14.6707 - mse: 14.6707\n",
      "Epoch 86/150\n",
      "72/72 - 0s - loss: 14.4363 - mse: 14.4363\n",
      "Epoch 87/150\n",
      "72/72 - 0s - loss: 13.8586 - mse: 13.8586\n",
      "Epoch 88/150\n",
      "72/72 - 0s - loss: 14.1921 - mse: 14.1921\n",
      "Epoch 89/150\n",
      "72/72 - 0s - loss: 13.9057 - mse: 13.9057\n",
      "Epoch 90/150\n",
      "72/72 - 0s - loss: 13.6405 - mse: 13.6405\n",
      "Epoch 91/150\n",
      "72/72 - 0s - loss: 13.7434 - mse: 13.7434\n",
      "Epoch 92/150\n",
      "72/72 - 0s - loss: 13.2155 - mse: 13.2155\n",
      "Epoch 93/150\n",
      "72/72 - 0s - loss: 13.0878 - mse: 13.0878\n",
      "Epoch 94/150\n",
      "72/72 - 0s - loss: 13.0785 - mse: 13.0785\n",
      "Epoch 95/150\n",
      "72/72 - 0s - loss: 12.5794 - mse: 12.5794\n",
      "Epoch 96/150\n",
      "72/72 - 0s - loss: 12.2908 - mse: 12.2908\n",
      "Epoch 97/150\n",
      "72/72 - 0s - loss: 12.1852 - mse: 12.1852\n",
      "Epoch 98/150\n",
      "72/72 - 0s - loss: 12.2939 - mse: 12.2939\n",
      "Epoch 99/150\n",
      "72/72 - 0s - loss: 11.6993 - mse: 11.6993\n",
      "Epoch 100/150\n",
      "72/72 - 0s - loss: 12.2867 - mse: 12.2867\n",
      "Epoch 101/150\n",
      "72/72 - 0s - loss: 11.8164 - mse: 11.8164\n",
      "Epoch 102/150\n",
      "72/72 - 0s - loss: 10.9541 - mse: 10.9541\n",
      "Epoch 103/150\n",
      "72/72 - 0s - loss: 11.3249 - mse: 11.3249\n",
      "Epoch 104/150\n",
      "72/72 - 0s - loss: 11.3648 - mse: 11.3648\n",
      "Epoch 105/150\n",
      "72/72 - 0s - loss: 11.2071 - mse: 11.2071\n",
      "Epoch 106/150\n",
      "72/72 - 0s - loss: 11.1531 - mse: 11.1531\n",
      "Epoch 107/150\n",
      "72/72 - 0s - loss: 10.4298 - mse: 10.4298\n",
      "Epoch 108/150\n",
      "72/72 - 0s - loss: 10.3692 - mse: 10.3692\n",
      "Epoch 109/150\n",
      "72/72 - 0s - loss: 10.3086 - mse: 10.3086\n",
      "Epoch 110/150\n",
      "72/72 - 0s - loss: 10.5098 - mse: 10.5098\n",
      "Epoch 111/150\n",
      "72/72 - 0s - loss: 9.8770 - mse: 9.8770\n",
      "Epoch 112/150\n",
      "72/72 - 0s - loss: 10.0880 - mse: 10.0880\n",
      "Epoch 113/150\n",
      "72/72 - 0s - loss: 9.8186 - mse: 9.8186\n",
      "Epoch 114/150\n",
      "72/72 - 0s - loss: 9.8712 - mse: 9.8712\n",
      "Epoch 115/150\n",
      "72/72 - 0s - loss: 9.4425 - mse: 9.4425\n",
      "Epoch 116/150\n",
      "72/72 - 0s - loss: 9.2246 - mse: 9.2246\n",
      "Epoch 117/150\n",
      "72/72 - 0s - loss: 9.2191 - mse: 9.2191\n",
      "Epoch 118/150\n",
      "72/72 - 0s - loss: 9.1144 - mse: 9.1144\n",
      "Epoch 119/150\n",
      "72/72 - 0s - loss: 9.1160 - mse: 9.1160\n",
      "Epoch 120/150\n",
      "72/72 - 0s - loss: 8.9009 - mse: 8.9009\n",
      "Epoch 121/150\n",
      "72/72 - 0s - loss: 8.6901 - mse: 8.6901\n",
      "Epoch 122/150\n",
      "72/72 - 0s - loss: 8.4686 - mse: 8.4686\n",
      "Epoch 123/150\n",
      "72/72 - 0s - loss: 8.2478 - mse: 8.2478\n",
      "Epoch 124/150\n",
      "72/72 - 0s - loss: 8.3468 - mse: 8.3468\n",
      "Epoch 125/150\n",
      "72/72 - 0s - loss: 8.2551 - mse: 8.2551\n",
      "Epoch 126/150\n",
      "72/72 - 0s - loss: 7.7953 - mse: 7.7953\n",
      "Epoch 127/150\n",
      "72/72 - 0s - loss: 7.6923 - mse: 7.6923\n",
      "Epoch 128/150\n",
      "72/72 - 0s - loss: 7.8741 - mse: 7.8741\n",
      "Epoch 129/150\n",
      "72/72 - 0s - loss: 7.1707 - mse: 7.1707\n",
      "Epoch 130/150\n",
      "72/72 - 0s - loss: 7.2539 - mse: 7.2539\n",
      "Epoch 131/150\n",
      "72/72 - 0s - loss: 7.5111 - mse: 7.5111\n",
      "Epoch 132/150\n",
      "72/72 - 0s - loss: 7.0031 - mse: 7.0031\n",
      "Epoch 133/150\n",
      "72/72 - 0s - loss: 7.0262 - mse: 7.0262\n",
      "Epoch 134/150\n",
      "72/72 - 0s - loss: 6.4259 - mse: 6.4259\n",
      "Epoch 135/150\n",
      "72/72 - 0s - loss: 6.7629 - mse: 6.7629\n",
      "Epoch 136/150\n",
      "72/72 - 0s - loss: 6.9230 - mse: 6.9230\n",
      "Epoch 137/150\n",
      "72/72 - 0s - loss: 6.1323 - mse: 6.1323\n",
      "Epoch 138/150\n",
      "72/72 - 0s - loss: 6.1200 - mse: 6.1200\n",
      "Epoch 139/150\n",
      "72/72 - 0s - loss: 6.4977 - mse: 6.4977\n",
      "Epoch 140/150\n",
      "72/72 - 0s - loss: 6.0531 - mse: 6.0531\n",
      "Epoch 141/150\n",
      "72/72 - 0s - loss: 5.9381 - mse: 5.9381\n",
      "Epoch 142/150\n",
      "72/72 - 0s - loss: 6.0356 - mse: 6.0356\n",
      "Epoch 143/150\n",
      "72/72 - 0s - loss: 6.0327 - mse: 6.0327\n",
      "Epoch 144/150\n",
      "72/72 - 0s - loss: 5.7057 - mse: 5.7057\n",
      "Epoch 145/150\n",
      "72/72 - 0s - loss: 5.6575 - mse: 5.6575\n",
      "Epoch 146/150\n",
      "72/72 - 0s - loss: 5.4606 - mse: 5.4606\n",
      "Epoch 147/150\n",
      "72/72 - 0s - loss: 5.2243 - mse: 5.2243\n",
      "Epoch 148/150\n",
      "72/72 - 0s - loss: 5.3527 - mse: 5.3527\n",
      "Epoch 149/150\n",
      "72/72 - 0s - loss: 5.2947 - mse: 5.2947\n",
      "Epoch 150/150\n",
      "72/72 - 0s - loss: 5.2664 - mse: 5.2664\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 103.8601 - mse: 103.8601\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[103.86009979248047, 103.86009979248047]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define model\n",
    "model_all=Sequential()\n",
    "model_all.add(Dense(32, activation='relu', input_dim=dim_1))\n",
    "model_all.add(Dense(32, activation='relu'))\n",
    "model_all.add(Dense(1, activation='relu'))\n",
    "\n",
    "model_all.compile(loss='mse',optimizer='rmsprop',metrics=['mse'])\n",
    "\n",
    "#fit model\n",
    "history_all=model_all.fit(x_train_all, y_train_all, epochs=150, batch_size=4, verbose=2)\n",
    "model_all.evaluate(x_test_all, y_test_all) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 32)                832       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,921\n",
      "Trainable params: 1,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAEvCAYAAABolJlEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAApDUlEQVR4nO3de5Bc5Xnn8e9z+j4zuqJBiBk5kokcgyQj2YLgsKuwJjGycQAn5Vq5iC3HF7lc2HE2TrIQV+KkaoldVnyJa4E1sb2I+EKwQ4LWCzGXdUyoYMRAxFUQyUaGEUISErqMZqanu8+zf5wzUkvquUmjOX2mf5+qrj79ntvz6jK/ft8+fcbcHREREWl+QdIFiIiIyPgotEVERFJCoS0iIpISCm0REZGUUGiLiIikhEJbREQkJbJJFzCWefPm+aJFi5IuQ0REZEo8/vjjr7l7Z6N1TR/aixYtoqenJ+kyREREpoSZ/WKkdZoeFxERSQmFtoiISEootEVERFJizM+0zawIPAQU4u1/4O6fM7O/AD4G7I03/VN3vyfe5wbgI0AN+H13/1Hc/jbgNqAE3AN82nXzcxERaaBSqdDb28vg4GDSpZwRxWKR7u5ucrncuPcZz4VoZeAd7t5nZjngYTO7N173FXf/6/qNzewCYC2wFDgXeMDM3uTuNeAWYD3wU6LQXgPci4iIyAl6e3uZMWMGixYtwsySLmdSuTv79u2jt7eXxYsXj3u/MafHPdIXv8zFj9FGx1cDd7h72d1fBLYDF5vZAmCmuz8Sj65vB64Zd6UiItJSBgcHOeuss6ZdYAOYGWedddaEZxHG9Zm2mWXMbAuwB7jf3R+NV33SzJ4ys2+Z2Zy4rQt4uW733ritK14+sV1ERKSh6RjYw06lb+MKbXevufsKoJto1LyMaKr7PGAFsAv40nAdjQ4xSvtJzGy9mfWYWc/evXsbbSIiInLGdXR0JF3CcSZ09bi7HwD+BVjj7rvjMA+BvwUujjfrBRbW7dYNvBK3dzdob3SeW919lbuv6uxseFMYERGRljNmaJtZp5nNjpdLwG8Az8efUQ97L/BMvLwJWGtmBTNbDCwBNrv7LuCwmV1i0ZzAB4G7J68rY9vywPfY8uAdU3lKERGZBtydP/7jP2bZsmUsX76cv//7vwdg165drF69mhUrVrBs2TL+9V//lVqtxoc+9KGj237lK1+ZtDrGc/X4AmCjmWWIQv5Od/+hmf2dma0gmuLeAXw87tizZnYn8BxQBa6LrxwH+ATHvvJ1L1N85Xh+803RfPzla6fytCIiknJ33XUXW7Zs4cknn+S1117joosuYvXq1Xz3u9/liiuu4LOf/Sy1Wo3+/n62bNnCzp07eeaZaCx74MCBSatjzNB296eAlQ3aPzDKPjcCNzZo7wGWTbDGSTOUaadjSJ+Ri4ikzV/+n2d57pVDk3rMC86dyed+a+m4tn344Yd5//vfTyaTYf78+fz6r/86jz32GBdddBEf/vCHqVQqXHPNNaxYsYI3vvGN/PznP+dTn/oUV155Je985zsnreaWuiNaNddBMexPugwREUmZke4Dtnr1ah566CG6urr4wAc+wO23386cOXN48sknueyyy7jpppv46Ec/Oml1NP1v+ZpMtVwHJVdoi4ikzXhHxGfK6tWr+frXv866devYv38/Dz30EBs2bOAXv/gFXV1dfOxjH+PIkSM88cQTvPvd7yafz/M7v/M7nHfeeXzoQx+atDpaKrTDfAftCm0REZmg9773vTzyyCNceOGFmBlf/OIXOeecc9i4cSMbNmwgl8vR0dHB7bffzs6dO/m93/s9wjAE4POf//yk1dFSoU1hBkWrMFQeJF8oJl2NiIg0ub6+6IagZsaGDRvYsGHDcevXrVvHunXrTtrviSeeOCP1tNRn2laYAUD/4QPJFiIiInIKWiq0g+JMQKEtIiLp1FKhnWuLQnug70CyhYiIiJyClgrtbGk2AEP9B5MtRERE5BS0VGgXOmYBMHREoS0iIunTWqHdPhuAyoBCW0RE0qelQrttxmwAav2Teys8ERGRqdCSoe1lhbaIiKRPa4V2+0xCNxg8nHQpIiKSAjt27ODNb34zH/3oR1m2bBnXXnstDzzwAJdeeilLlixh8+bN/OQnP2HFihWsWLGClStXcvhwlDEbNmzgoosu4i1veQuf+9znJqWelrojmgUBfRRhSKEtIiLjs337dr7//e9z6623ctFFF/Hd736Xhx9+mE2bNvFXf/VX1Go1brrpJi699FL6+vooFovcd999bNu2jc2bN+PuXHXVVTz00EOsXr36tGppqdAG6Lc2gqG+pMsQEZGJuPd6ePXpyT3mOcvhXV8Yc7PFixezfPlyAJYuXcrll1+OmbF8+XJ27NjB2rVr+cM//EOuvfZafvu3f5vu7m7uu+8+7rvvPlaujH6zdV9fH9u2bVNoT9RA0Ea2qtAWEZHxKRQKR5eDIDj6OggCqtUq119/PVdeeSX33HMPl1xyCQ888ADuzg033MDHP/7xSa2l5UK7HLSTrR5JugwREZmIcYyIk/Kzn/2M5cuXs3z5ch555BGef/55rrjiCv7sz/6Ma6+9lo6ODnbu3Ekul+Pss88+rXO1Xmhn2ykotEVEZJJ89atf5cc//jGZTIYLLriAd73rXRQKBbZu3crb3/52ADo6Ovj2t7992qFt7j4ZNZ8xq1at8p6enkk73hN//VvM7X+RRX/+zKQdU0REJt/WrVs5//zzky7jjGrURzN73N1XNdq+pb7yBVDNdlAK+5MuQ0REZMJaLrTDfAdtrtAWEZH0abnQJj+DdgYJa7WkKxEREZmQ1gvt4gwCc/qP6FamIiLNrtmvuzodp9K3lgttK8wEoP/wgWQLERGRURWLRfbt2zctg9vd2bdvH8VicUL7tdxXvjJtUWgP9OnXc4qINLPu7m56e3vZu3dv0qWcEcVike7u7gnt03KhnSvNAqB85ECyhYiIyKhyuRyLFy9Ouoym0nLT47l4pF3WSFtERFJmzNA2s6KZbTazJ83sWTP7y7h9rpndb2bb4uc5dfvcYGbbzewFM7uirv1tZvZ0vO5rZmZnplsjK7TPBqA6cGCqTy0iInJaxjPSLgPvcPcLgRXAGjO7BLgeeNDdlwAPxq8xswuAtcBSYA1ws5ll4mPdAqwHlsSPNZPXlfEpdcwGoNKvq8dFRCRdxgxtjwz/Wqxc/HDgamBj3L4RuCZevhq4w93L7v4isB242MwWADPd/RGPLgW8vW6fKdM2YzYA4aBCW0RE0mVcn2mbWcbMtgB7gPvd/VFgvrvvAoifh++C3gW8XLd7b9zWFS+f2D6lhkPbFdoiIpIy4wptd6+5+wqgm2jUvGyUzRt9Tu2jtJ98ALP1ZtZjZj2Tfal/vlBk0HNY+fCkHldERORMm9DV4+5+APgXos+id8dT3sTPe+LNeoGFdbt1A6/E7d0N2hud51Z3X+Xuqzo7OydS4rgcsTasol/PKSIi6TKeq8c7zWx2vFwCfgN4HtgErIs3WwfcHS9vAtaaWcHMFhNdcLY5nkI/bGaXxFeNf7Bunyk1YG1kKn1jbygiItJExnNzlQXAxvgK8AC4091/aGaPAHea2UeAl4D3Abj7s2Z2J/AcUAWuc/fh387xCeA2oATcGz+m3GDQRlahLSIiKTNmaLv7U8DKBu37gMtH2OdG4MYG7T3AaJ+HT4lypo18TdPjIiKSLi13RzSASraDgkJbRERSpiVDu5ptpxgqtEVEJF1aMrRruQ7afCDpMkRERCakJUM7zM+k3fuTLkNERGRCWjK0rTCDvFUpDyq4RUQkPVoytCl0ANB/WL+eU0RE0qMlQztTin6ndv/hA8kWIiIiMgEtGtqzABjsez3hSkRERMavJUM73xaNtMtHND0uIiLp0Zqh3T4bgKF+hbaIiKRHS4Z2sSOaHq8qtEVEJEVaMrTbOuYAUBs4lHAlIiIi49eSoV2aEY20fVChLSIi6dGaod02AwCvDiZciYiIyPi1ZGgHmQxlz0FFd0QTEZH0aMnQBihbnkAjbRERSZHWDW3yWFW/6UtERNKjdUPbCgS1ctJliIiIjFvLhnbFCmRqmh4XEZH0aN3QDhTaIiKSLi0d2tlQ0+MiIpIeLRvataCo0BYRkVRp2dCuZorkQ02Pi4hIerRsaIeZAjnXSFtERNKjhUO7SEGhLSIiKdK6oZ0tkWco6TJERETGrWVD27Mliq7QFhGR9BgztM1soZn92My2mtmzZvbpuP0vzGynmW2JH++u2+cGM9tuZi+Y2RV17W8zs6fjdV8zMzsz3RqHXJG8ValVq4mVICIiMhHjGWlXgc+4+/nAJcB1ZnZBvO4r7r4iftwDEK9bCywF1gA3m1km3v4WYD2wJH6smbyuTIzlSgAMDvQlVYKIiMiEjBna7r7L3Z+Ilw8DW4GuUXa5GrjD3cvu/iKwHbjYzBYAM939EXd34HbgmtPtwKkaDu3ywJGkShAREZmQCX2mbWaLgJXAo3HTJ83sKTP7lpnNidu6gJfrduuN27ri5RPbExHk2wAoa6QtIiIpMe7QNrMO4B+AP3D3Q0RT3ecBK4BdwJeGN22wu4/S3uhc682sx8x69u7dO94SJ8Ty0Uh7aFC/nlNERNJhXKFtZjmiwP6Ou98F4O673b3m7iHwt8DF8ea9wMK63buBV+L27gbtJ3H3W919lbuv6uzsnEh/xi0Tj7Qrg5oeFxGRdBjP1eMGfBPY6u5frmtfULfZe4Fn4uVNwFozK5jZYqILzja7+y7gsJldEh/zg8Ddk9SPCcsWotCuKrRFRCQlsuPY5lLgA8DTZrYlbvtT4P1mtoJoinsH8HEAd3/WzO4EniO68vw6d6/F+30CuA0oAffGj0Rk4tCulDU9LiIi6TBmaLv7wzT+PPqeUfa5EbixQXsPsGwiBZ4puTi0a2WNtEVEJB1a9o5o+WI7ALWh/oQrERERGZ+WDe3c0dDW9LiIiKRDy4Z2vhRNj7tG2iIikhItG9qFUgcAXtFIW0RE0qFlQ7tYiqbHFdoiIpIWLRvauXyBimdAoS0iIinRsqENUCaPVQeTLkNERGRcWju0LY/VFNoiIpIOLR7aBYKqpsdFRCQdWjq0K1Ygo5G2iIikRIuHdl6hLSIiqdHaoR0UydbKSZchIiIyLi0d2tWgQDZUaIuISDq0dmhniuRCTY+LiEg6tHRoh5kiOR9KugwREZFxaenQrmWK5F3T4yIikg4tHdqeLVJAoS0iIunQ4qFdoqDpcRERSYmWDm2yRUo2hIdh0pWIiIiMqaVD23MlAMqD/QlXIiIiMraWDm0bDu2BIwlXIiIiMjaFNjA40JdwJSIiImNr6dDOFNoAGNL0uIiIpEBLh3aQj0baFYW2iIikQEuHdqbQDsDQoKbHRUSk+bV0aGfz0fR4tayRtoiINL/WDu1CND1eKw8kXImIiMjYxgxtM1toZj82s61m9qyZfTpun2tm95vZtvh5Tt0+N5jZdjN7wcyuqGt/m5k9Ha/7mpnZmenW+ORL0fR4rayvfImISPMbz0i7CnzG3c8HLgGuM7MLgOuBB919CfBg/Jp43VpgKbAGuNnMMvGxbgHWA0vix5pJ7MuE5eLPtGtDGmmLiEjzGzO03X2Xuz8RLx8GtgJdwNXAxnizjcA18fLVwB3uXnb3F4HtwMVmtgCY6e6PuLsDt9ftk4jhkXao0BYRkRSY0GfaZrYIWAk8Csx3910QBTtwdrxZF/By3W69cVtXvHxie6PzrDezHjPr2bt370RKnJBCqQMAr+hCNBERaX7jDm0z6wD+AfgDdz802qYN2nyU9pMb3W9191Xuvqqzs3O8JU5YoRRdPe4VjbRFRKT5jSu0zSxHFNjfcfe74ubd8ZQ38fOeuL0XWFi3ezfwStze3aA9MYVCidANFNoiIpIC47l63IBvAlvd/ct1qzYB6+LldcDdde1rzaxgZouJLjjbHE+hHzazS+JjfrBun0RYEDBIHlNoi4hICmTHsc2lwAeAp81sS9z2p8AXgDvN7CPAS8D7ANz9WTO7E3iO6Mrz69y9Fu/3CeA2oATcGz8SVbY8VisnXYaIiMiYxgxtd3+Yxp9HA1w+wj43Ajc2aO8Blk2kwDOtTIGgqpG2iIg0v5a+IxpAxfIEtcGkyxARERlTy4f2UFAg0PS4iIikQMuHdsWKZGuaHhcRkebX8qFdzRTIhhppi4hI81NoB0VyCm0REUmBlg/tMFMg5wptERFpfi0f2rVMkbxG2iIikgItH9phtkQBhbaIiDS/lg9tzxYp+FDSZYiIiIxJoZ0tUmAID8OkSxERERlVy4c2uRJZC6lUNNoWEZHm1vKhbbkSAIMDRxKuREREZHQK7Ti0hwYV2iIi0txaPrSDfBsA5X6FtoiINDeFdj4aaVfKCm0REWluLR/amUI7AEP6TFtERJpcy4d2Nh5pV8v9CVciIiIyOoV2IfpMW6EtIiLNruVDO1eMpscV2iIi0uwU2sVopB0OKbRFRKS5tXxo5+ORdjg0kHAlIiIio2v50C6UOgCNtEVEpPkptEvR9DgVjbRFRKS5tXxoF+ORtlcHE65ERERkdC0f2kEmQ9lzUNH0uIiINLeWD22AsuUJNNIWEZEmN2Zom9m3zGyPmT1T1/YXZrbTzLbEj3fXrbvBzLab2QtmdkVd+9vM7Ol43dfMzCa/O6dmkAJW1WfaIiLS3MYz0r4NWNOg/SvuviJ+3ANgZhcAa4Gl8T43m1km3v4WYD2wJH40OmYihixPUCsnXYaIiMioxgxtd38I2D/O410N3OHuZXd/EdgOXGxmC4CZ7v6IuztwO3DNKdY86SpWIFPT9LiIiDS30/lM+5Nm9lQ8fT4nbusCXq7bpjdu64qXT2xvCpVAoS0iIs3vVEP7FuA8YAWwC/hS3N7oc2ofpb0hM1tvZj1m1rN3795TLHH8KkGBbKjpcRERaW6nFNruvtvda+4eAn8LXByv6gUW1m3aDbwSt3c3aB/p+Le6+yp3X9XZ2XkqJU5ILSgqtEVEpOmdUmjHn1EPey8wfGX5JmCtmRXMbDHRBWeb3X0XcNjMLomvGv8gcPdp1D2pqpki+VDT4yIi0tyyY21gZt8DLgPmmVkv8DngMjNbQTTFvQP4OIC7P2tmdwLPAVXgOnevxYf6BNGV6CXg3vjRFMJMgZxrpC0iIs1tzNB29/c3aP7mKNvfCNzYoL0HWDah6qZImC2R96GkyxARERmV7ogGhJkiBTTSFhGR5qbQBjxboqiRtoiINDmFNkCuRN6q1KrVpCsREREZkUIbsFwRgMGBvoQrERERGZlCG7BcCYDywJGEKxERERmZQhsI8m0AlDXSFhGRJqbQBiwfjbSHBvXrOUVEpHkptIFMPNKuDGp6XEREmpdCG8gWotCuKrRFRKSJKbSBTBzalbKmx0VEpHkptIFcsR2A2lB/wpWIiIiMTKEN5OORdq2s6XEREWleCm3qR9qaHhcRkeal0AbypSi0XdPjIiLSxBTaQGE4tCsaaYuISPNSaANFhbaIiKSAQhvI5QtUPAMKbRERaWIK7ViZPFYdTLoMERGRESm0Y4NWwGoKbRERaV4K7diQ5Qmqmh4XEZHmpdCOVaxARiNtERFpYgrt2JAVyNTKSZchIiIyIoV2rBoUyGqkLSIiTUyhHasGBbKhRtoiItK8FNqxaqZILtRIW0REmpdCOxZmiuR8KOkyRERERqTQjtUyRfKu6XEREWleY4a2mX3LzPaY2TN1bXPN7H4z2xY/z6lbd4OZbTezF8zsirr2t5nZ0/G6r5mZTX53Tp1nixRQaIuISPMaz0j7NmDNCW3XAw+6+xLgwfg1ZnYBsBZYGu9zs5ll4n1uAdYDS+LHicdMlGdLFDQ9LiIiTWzM0Hb3h4D9JzRfDWyMlzcC19S13+HuZXd/EdgOXGxmC4CZ7v6Iuztwe90+TcFzJUo2hIdh0qWIiIg0dKqfac93910A8fPZcXsX8HLddr1xW1e8fGJ788gWASgP9idciIiISGOTfSFao8+pfZT2xgcxW29mPWbWs3fv3kkrbjSWKwFQHjgyJecTERGZqFMN7d3xlDfx8564vRdYWLddN/BK3N7doL0hd7/V3Ve5+6rOzs5TLHFignwbAIMDfVNyPhERkYk61dDeBKyLl9cBd9e1rzWzgpktJrrgbHM8hX7YzC6Jrxr/YN0+TSHIRyPtIU2Pi4hIk8qOtYGZfQ+4DJhnZr3A54AvAHea2UeAl4D3Abj7s2Z2J/AcUAWuc/dafKhPEF2JXgLujR9NI1OIRtoVhbaIiDSpMUPb3d8/wqrLR9j+RuDGBu09wLIJVTeFhqfHhwY1PS4iIs1Jd0SLZePQrpY10hYRkeak0I7lSu0A1MoDCVciIiLSmEI7litEF6LVyvrKl4iINCeFdixXiEfaQxppi4hIc1JoxwptHQCECm0REWlSCu1YvhiNtL2iC9FERKQ5KbRjhVJ09bhXNNIWEZHmpNCOFQolQjdQaIuISJNSaMcsCBgkjym0RUSkSSm065StgNXKSZchIiLSkEK7Tpk8QVUjbRERaU4K7ToVyxPUBpMuQ0REpCGFdp2hoEig6XEREWlSCu06FSuQrWl6XEREmpNCu041UyAbaqQtIiLNSaFdpxoUySm0RUSkSSm064SZAjlXaIuISHNSaNepZUrkNdIWEZEmpdCuE+baKKEL0UREpDkptOuEbZ3Mpo+hsr6rLSIizUehXSczawEA+3e/nHAlIiIiJ1No1ynMiUL74B6FtoiINB+Fdp32sxYCcGTfzoQrEREROZlCu87s+VFoVw4otEVEpPkotOvMmXcuVQ8ID72adCkiIiInUWjXyWSz7LfZZI7sTroUERGRkyi0T3AwexbFwb1JlyEiInKS0wptM9thZk+b2RYz64nb5prZ/Wa2LX6eU7f9DWa23cxeMLMrTrf4M6Ev30nHkEJbRESaz2SMtP+Lu69w91Xx6+uBB919CfBg/BozuwBYCywF1gA3m1lmEs4/qYZKncwO9yddhoiIyEnOxPT41cDGeHkjcE1d+x3uXnb3F4HtwMVn4PynJWw/h7kc0l3RRESk6ZxuaDtwn5k9bmbr47b57r4LIH4+O27vAurvWtIbtzWVzMxzANi/pzfhSkRERI6XPc39L3X3V8zsbOB+M3t+lG2tQZs33DB6A7Ae4A1veMNpljgxhbnnAnBw90ucs/CXp/TcIiIiozmtkba7vxI/7wH+kWi6e7eZLQCIn/fEm/cCC+t27wZeGeG4t7r7Kndf1dnZeTolTlj7Wd2A7oomIiLN55RD28zazWzG8DLwTuAZYBOwLt5sHXB3vLwJWGtmBTNbDCwBNp/q+c+U2fOjkX3lQMP3EyIiIok5nenx+cA/mtnwcb7r7v9sZo8Bd5rZR4CXgPcBuPuzZnYn8BxQBa5z99ppVX8GHLsr2q6kSxERETnOKYe2u/8cuLBB+z7g8hH2uRG48VTPORUy2Sx7dFc0ERFpQrojWgO6K5qIiDQjhXYDR/LzaK+8lnQZIiIix1FoN1Aunc2c2r6kyxARETmOQruBsH0+czlEZaicdCkiIiJHKbQbyMxcAMC+3S+PsaWIiMjUUWg3UH9XNBERkWah0G5g+K5o/ft1gxUREWkeCu0Ghu+KNvS6bmUqIiLNQ6HdwJx551JzIzz8atKliIiIHKXQbiCTzbLP5lDcP9ovLRMREZlaCu0R/KzrKlb2/xtP/fgHSZciIiICKLRH9NYPfJ4dwULm/+RPOHRAN1oREZHkKbRHUCi2MfSe/8k838/zG38/6XJEREQU2qN501svY/O513Lx6z/k0Ts34GGYdEkiItLCFNpjWPnBL/JUcRW/+tz/4LGvXctgf1/SJYmISItSaI+hWGpn6R/9iJ92f4SLD9zDzi/9J7Y/+XDSZYmISAtSaI9DJpvlko9+mSdXf51ZtddZfNd7+OnNH6Pv0OtJlyYiIi1EoT0BF75jLblPP07PvKu5ePf3qXx5OY988zPs36M7p4mIyJmn0J6gWXPm8auf2sj2q+9mR9ty3v7yN2i76UIe++pann/0Pl2sJiIiZ4y5e9I1jGrVqlXe09OTdBkj+sULW3j1R19i2b77aLdBXgq62LngN5mz8iretPIygkwm6RJFRCRFzOxxd1/VcJ1Ce3IcOXyAZx+4nfat3+dXys+QtZB9zOJnsy8ld8GV/Mqv/RZtHbOSLlNERJqcQnuKHdy3m23/9k/Yf/wzSw7/lJn0U/YcL5RWMLDocs46/z/T/aaVFEvtSZcqIiJNRqGdoMpQmRc2/4i+p/8v3Xt+QrfvAqDqAb2Zbl7reBPVzqW0/9JK3rD015h11vyEKxYRkSQptJvIzp9vZfcLP6Xc+ySl/VtZMLCN+Ry7t/lOm8+e0nmUZy4mmPfLzH7jW3nDm1dRbOtIsGoREZkqo4V2dqqLaXVdbzyfrjeef1zbgdde5eXnHqVvRw+FPVuY27+DBUceo/BqBZ6B6t0Bv8icy+HcPAYL86i2zYcZ88nOWkBpbjczz17I3PkLaZ8xO5lOiYjIlNBIu0nVqlVefWkbe7ZtZvClf6dwYDtt5deYWdvHvHA/eauetE+fl3g9mMOh3DwGimdTLXXCzAVkZ59LaU4X7XPOpmPO2cyaO59cvpBAr0REZCwaaadQJputG5WvO26dhyEHX9/L63te5vDenQzu76V66FXs8C7yA3solfdy7uGnmXtwP8XdlYbH7/MSh4IZHAlmMpCbxVBuNrX8DMJ8BxRmYIUZBMUZZEszyZVmkmufRbF9FsWO2ZQ6ZjNj5hx9nU1EZIpNeWib2Rrgb4AM8A13/8JU15B2FgTMOmt+dNHa+Q3fjAFxuB/Yx4HdL3Foby/lw3uo9e0n7N+PDewnO/g6uaEDlKoHmVveSbv30+YDFKxx0NcL3ThobfRZBxUrULE81aBANchTyxQJgzy1TIEwU8QzBTxbgmwBsgUsV8JyRYJckSBfIsgWyOSKBPkC2VyRbKFENl+Ml4tkcwUymSxBNkc2myWTzZHN5shkslig+wOJSOuY0tA2swxwE/CbQC/wmJltcvfnprKOVmFBwKy5ncya2wnnv23c+w2VB+k/fID+voMM9h1g6MhByv0HqQ4cojZwmHDwIAwcxAYPkBk6SFAbIhOWyYRlcmGZUvUQOR8i50PkvUyeCnkfojiONwMTVXOjRoYqGWoE1CxDjQwhQfQ8/NoyhGQILahbjh5uAaFlcQtwy8SP+uUMWHC0DQvi1xkIMoDhQQaG1x1dNggymGUgCKK2IIPVbWNBcHQbC+LtwpCwNgS1KliAZbJYkI2fMxBkMTMsyGCBAQFmQXQsy2AGBEF8zCDa1gIIAoK43qg9wIIMQWBgAUFc19HXR49hR48fBHHNGMHR18fOE9RtE73OxI9Ab7BEJsFUj7QvBra7+88BzOwO4GpAod1E8oUi+cI5zJ53zqQeN6zVGBoapDzQT2Wwn6HyAJXBI1SGBqkODVAbKlOrDBJWytQqZcLKAF4t47UKXquC1yCsRsthFcLoNV7D4mU7+roKHmJexcIagVcxrx33COJHJhzCfCB6TUjgUeQHXsMIyXgNw6N1Jz48as8QRtsSkrHmvk4kSaFb/CcV/Yk6EBLgw21mOMceJzu5rf4Y0XHiZQsIMRhut+FthtcfW3fsWAZ2rM3r9o/aafB6uK7hY9btF58j2ofojVz9Oa3+/PXntrirx8514n7D+9Sf47jj2QltdftET8Gx13b8eY4+H7cd8bmCo7XVb2/1tVgQtVkGj99gHn/ME2s5/s8ADLMT67AR15sdO7cdtx3x6wbnsrptTqojOKGsY8fyo+uO/buZ3bWExUt/lakw1aHdBbxc97oXmJqeSuKCTIZiqX3a31TGwxB3p1arUqtV8TCMl2sQ1qjVqoRh7Wh7WKvhYZUwDAlrNYJMhkw2Tyabxd0Jq1VqtQphtUJYq0Tbu+MeRucKazgOYXiszR332tE23OP2+tfRcfAaHkbR5x5CGIKH8bq6NqJj4Mcew9scfdTVYe440bmG11tcN3EN5rXhP7Rj2xBvP/4/cMCx444bAmHc5kePaRx/3qj9WBQTvx6O3ui48fNwZMevORrh1NUd7zt8zvr9jq6vj/pj5zgW98P7HXttx9VY1z68fML+x7+9OP58duKxGxxntP3q66tvD1r4zepPz/6v0za0G711Pulv2szWA+sB3vCGN5zpmkQmlQVB9EMskyGHrtKX1jL8pjEMa/Eb0erRZYDhbyyd+MzRN4l+7E3pcds4hI5zwvqjy8TL0RvQqJZ4/+GY8WM1HP3i1PAbRD9+22N1hUdXn9wWvV581uTOSo5mqkO7F1hY97obeOXEjdz9VuBWiL7yNTWliYjI6ap/0yqTb6qvDHkMWGJmi80sD6wFNk1xDSIiIqk0pSNtd6+a2SeBHxF95etb7v7sVNYgIiKSVlP+PW13vwe4Z6rPKyIiknb64qSIiEhKKLRFRERSQqEtIiKSEgptERGRlFBoi4iIpIRCW0REJCUU2iIiIilhR++l2qTMbC/wi0k85DzgtUk8XrOYrv2C6du36dovmL59m679gunbtzT265fcvbPRiqYP7clmZj3uvirpOibbdO0XTN++Tdd+wfTt23TtF0zfvk23fml6XEREJCUU2iIiIinRiqF9a9IFnCHTtV8wffs2XfsF07dv07VfMH37Nq361XKfaYuIiKRVK460RUREUqllQtvM1pjZC2a23cyuT7qe02FmC83sx2a21cyeNbNPx+1zzex+M9sWP89JutZTYWYZM/t3M/th/Hq69Gu2mf3AzJ6P/+7ePh36Zmb/Lf53+IyZfc/Mimntl5l9y8z2mNkzdW0j9sXMboh/prxgZlckU/XYRujXhvjf4lNm9o9mNrtuXSr6BY37Vrfuj8zMzWxeXVtq+tZIS4S2mWWAm4B3ARcA7zezC5Kt6rRUgc+4+/nAJcB1cX+uBx509yXAg/HrNPo0sLXu9XTp198A/+zubwYuJOpjqvtmZl3A7wOr3H0ZkAHWkt5+3QasOaGtYV/i/3NrgaXxPjfHP2ua0W2c3K/7gWXu/hbgP4AbIHX9gsZ9w8wWAr8JvFTXlra+naQlQhu4GNju7j939yHgDuDqhGs6Ze6+y92fiJcPE/3w7yLq08Z4s43ANYkUeBrMrBu4EvhGXfN06NdMYDXwTQB3H3L3A0yDvgFZoGRmWaANeIWU9svdHwL2n9A8Ul+uBu5w97K7vwhsJ/pZ03Qa9cvd73P3avzyp0B3vJyafsGIf2cAXwH+BKi/cCtVfWukVUK7C3i57nVv3JZ6ZrYIWAk8Csx3910QBTtwdoKlnaqvEv1HC+vapkO/3gjsBf53PPX/DTNrJ+V9c/edwF8TjWZ2AQfd/T5S3q8TjNSX6fRz5cPAvfFy6vtlZlcBO939yRNWpb5vrRLa1qAt9ZfNm1kH8A/AH7j7oaTrOV1m9h5gj7s/nnQtZ0AWeCtwi7uvBI6QninjEcWf714NLAbOBdrN7HeTrWrKTIufK2b2WaKP3L4z3NRgs9T0y8zagM8Cf95odYO21PQNWie0e4GFda+7iabwUsvMckSB/R13vytu3m1mC+L1C4A9SdV3ii4FrjKzHUQfYbzDzL5N+vsF0b/BXnd/NH79A6IQT3vffgN40d33unsFuAv4NdLfr3oj9SX1P1fMbB3wHuBaP/b937T36zyiN5FPxj9LuoEnzOwc0t+3lgntx4AlZrbYzPJEFyJsSrimU2ZmRvTZ6FZ3/3Ldqk3Aunh5HXD3VNd2Otz9BnfvdvdFRH9H/8/df5eU9wvA3V8FXjazX4mbLgeeI/19ewm4xMza4n+XlxNdY5H2ftUbqS+bgLVmVjCzxcASYHMC9Z0SM1sD/HfgKnfvr1uV6n65+9Pufra7L4p/lvQCb43/D6a6bwC4e0s8gHcTXSH5M+CzSddzmn35T0RTOk8BW+LHu4GziK5u3RY/z0261tPo42XAD+PladEvYAXQE/+9/RMwZzr0DfhL4HngGeDvgEJa+wV8j+iz+QrRD/uPjNYXomnYnwEvAO9Kuv4J9ms70ee7wz9D/lfa+jVS305YvwOYl8a+NXrojmgiIiIp0SrT4yIiIqmn0BYREUkJhbaIiEhKKLRFRERSQqEtIiKSEgptERGRlFBoi4iIpIRCW0REJCX+P3VENpn5CetmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history_all.history).plot(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_p_all = model_all.predict(x_train_all)\n",
    "y_test_p_all = model_all.predict(x_test_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_train,y_train_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_test,y_test_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list_all=np.array(y_train_all).flatten().tolist() #y_train_all 리스트\n",
    "y_test_list_all=np.array(y_test_all).flatten().tolist() #y_test_all 리스트\n",
    "y_p_train_list_all=np.array(y_train_p_all).flatten().tolist() #y_train_all 예측 리스트\n",
    "y_p_test_list_all=np.array(y_test_p_all).flatten().tolist() #y_test_all 예측 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#오차 범위 3 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_all set prediction accuracy: 50.35 %\n",
      "test_all set prediction accuracy: 56.94 %\n"
     ]
    }
   ],
   "source": [
    "#평균 성능 테스트\n",
    "scores = 0\n",
    "mean=np.mean(Y, axis=0)\n",
    "for i in range(len(y_train_all)):\n",
    "    if  y_train_list_all[i]-10 <= mean <= y_train_list_all[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train_all)\n",
    "print(\"train_all set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test_all)):\n",
    "    if  y_test_list_all[i]-10 <= mean <= y_test_list_all[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test_all)\n",
    "print(\"test_all set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### <오차범위 3>\n",
      "- train_all set prediction accuracy(+-3): 88.89 % <br>\n",
      "- test_all set prediction accuracy(+-3): 31.94 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 5>\n",
      "- train_all set prediction accuracy(+-5): 96.53 % <br>\n",
      "- test_all set prediction accuracy(+-5): 55.56 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 10>\n",
      "- train_all set prediction accuracy(+-10): 100.00 % <br>\n",
      "- test_all set prediction accuracy(+-10): 83.33 % <br>\n",
      "<br>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######입력용#######\n",
    "\n",
    "#오차 범위 3 설정\n",
    "print('### <오차범위 3>')\n",
    "scores = 0\n",
    "for i in range(len(y_train_all)):\n",
    "    if  y_train_list_all[i]-3 <= y_p_train_list_all[i] <= y_train_list_all[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train_all)\n",
    "print(\"- train_all set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test_all)):\n",
    "    if  y_test_list_all[i]-3 <= y_p_test_list_all[i] <= y_test_list_all[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test_all)\n",
    "print(\"- test_all set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "print('### <오차범위 5>')\n",
    "scores = 0\n",
    "for i in range(len(y_train_all)):\n",
    "    if  y_train_list_all[i]-5 <= y_p_train_list_all[i] <= y_train_list_all[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train_all)\n",
    "print(\"- train_all set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test_all)):\n",
    "    if  y_test_list_all[i]-5 <= y_p_test_list_all[i] <= y_test_list_all[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test_all)\n",
    "print(\"- test_all set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "print('### <오차범위 10>')\n",
    "scores = 0\n",
    "for i in range(len(y_train_all)):\n",
    "    if  y_train_list_all[i]-10 <= y_p_train_list_all[i] <= y_train_list_all[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train_all)\n",
    "print(\"- train_all set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test_all)):\n",
    "    if  y_test_list_all[i]-10 <= y_p_test_list_all[i] <= y_test_list_all[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test_all)\n",
    "\n",
    "print(\"- test_all set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - SOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "72/72 - 0s - loss: 3576.9927 - mse: 3576.9927\n",
      "Epoch 2/150\n",
      "72/72 - 0s - loss: 3145.0154 - mse: 3145.0154\n",
      "Epoch 3/150\n",
      "72/72 - 0s - loss: 2510.0107 - mse: 2510.0107\n",
      "Epoch 4/150\n",
      "72/72 - 0s - loss: 1648.3110 - mse: 1648.3110\n",
      "Epoch 5/150\n",
      "72/72 - 0s - loss: 822.8356 - mse: 822.8356\n",
      "Epoch 6/150\n",
      "72/72 - 0s - loss: 405.4231 - mse: 405.4231\n",
      "Epoch 7/150\n",
      "72/72 - 0s - loss: 324.4249 - mse: 324.4249\n",
      "Epoch 8/150\n",
      "72/72 - 0s - loss: 293.2901 - mse: 293.2901\n",
      "Epoch 9/150\n",
      "72/72 - 0s - loss: 266.3015 - mse: 266.3015\n",
      "Epoch 10/150\n",
      "72/72 - 0s - loss: 247.4260 - mse: 247.4260\n",
      "Epoch 11/150\n",
      "72/72 - 0s - loss: 231.6475 - mse: 231.6475\n",
      "Epoch 12/150\n",
      "72/72 - 0s - loss: 219.5000 - mse: 219.5000\n",
      "Epoch 13/150\n",
      "72/72 - 0s - loss: 206.3285 - mse: 206.3285\n",
      "Epoch 14/150\n",
      "72/72 - 0s - loss: 197.0539 - mse: 197.0539\n",
      "Epoch 15/150\n",
      "72/72 - 0s - loss: 188.6428 - mse: 188.6428\n",
      "Epoch 16/150\n",
      "72/72 - 0s - loss: 180.3251 - mse: 180.3251\n",
      "Epoch 17/150\n",
      "72/72 - 0s - loss: 176.6274 - mse: 176.6274\n",
      "Epoch 18/150\n",
      "72/72 - 0s - loss: 169.2346 - mse: 169.2346\n",
      "Epoch 19/150\n",
      "72/72 - 0s - loss: 164.9213 - mse: 164.9213\n",
      "Epoch 20/150\n",
      "72/72 - 0s - loss: 159.7558 - mse: 159.7558\n",
      "Epoch 21/150\n",
      "72/72 - 0s - loss: 155.2324 - mse: 155.2324\n",
      "Epoch 22/150\n",
      "72/72 - 0s - loss: 151.1742 - mse: 151.1742\n",
      "Epoch 23/150\n",
      "72/72 - 0s - loss: 147.9948 - mse: 147.9948\n",
      "Epoch 24/150\n",
      "72/72 - 0s - loss: 142.6464 - mse: 142.6464\n",
      "Epoch 25/150\n",
      "72/72 - 0s - loss: 141.7563 - mse: 141.7563\n",
      "Epoch 26/150\n",
      "72/72 - 0s - loss: 138.7980 - mse: 138.7980\n",
      "Epoch 27/150\n",
      "72/72 - 0s - loss: 137.0904 - mse: 137.0904\n",
      "Epoch 28/150\n",
      "72/72 - 0s - loss: 134.7589 - mse: 134.7589\n",
      "Epoch 29/150\n",
      "72/72 - 0s - loss: 132.7550 - mse: 132.7550\n",
      "Epoch 30/150\n",
      "72/72 - 0s - loss: 130.8533 - mse: 130.8533\n",
      "Epoch 31/150\n",
      "72/72 - 0s - loss: 129.8415 - mse: 129.8415\n",
      "Epoch 32/150\n",
      "72/72 - 0s - loss: 127.0538 - mse: 127.0538\n",
      "Epoch 33/150\n",
      "72/72 - 0s - loss: 126.2510 - mse: 126.2510\n",
      "Epoch 34/150\n",
      "72/72 - 0s - loss: 125.7934 - mse: 125.7934\n",
      "Epoch 35/150\n",
      "72/72 - 0s - loss: 122.8467 - mse: 122.8467\n",
      "Epoch 36/150\n",
      "72/72 - 0s - loss: 121.5266 - mse: 121.5266\n",
      "Epoch 37/150\n",
      "72/72 - 0s - loss: 122.2812 - mse: 122.2812\n",
      "Epoch 38/150\n",
      "72/72 - 0s - loss: 120.9410 - mse: 120.9410\n",
      "Epoch 39/150\n",
      "72/72 - 0s - loss: 120.0614 - mse: 120.0614\n",
      "Epoch 40/150\n",
      "72/72 - 0s - loss: 118.1798 - mse: 118.1798\n",
      "Epoch 41/150\n",
      "72/72 - 0s - loss: 117.8926 - mse: 117.8926\n",
      "Epoch 42/150\n",
      "72/72 - 0s - loss: 116.6275 - mse: 116.6275\n",
      "Epoch 43/150\n",
      "72/72 - 0s - loss: 114.9330 - mse: 114.9330\n",
      "Epoch 44/150\n",
      "72/72 - 0s - loss: 115.4059 - mse: 115.4059\n",
      "Epoch 45/150\n",
      "72/72 - 0s - loss: 114.5233 - mse: 114.5233\n",
      "Epoch 46/150\n",
      "72/72 - 0s - loss: 113.3335 - mse: 113.3335\n",
      "Epoch 47/150\n",
      "72/72 - 0s - loss: 112.3860 - mse: 112.3860\n",
      "Epoch 48/150\n",
      "72/72 - 0s - loss: 112.2603 - mse: 112.2603\n",
      "Epoch 49/150\n",
      "72/72 - 0s - loss: 111.6998 - mse: 111.6998\n",
      "Epoch 50/150\n",
      "72/72 - 0s - loss: 111.3359 - mse: 111.3359\n",
      "Epoch 51/150\n",
      "72/72 - 0s - loss: 109.8036 - mse: 109.8036\n",
      "Epoch 52/150\n",
      "72/72 - 0s - loss: 109.4020 - mse: 109.4020\n",
      "Epoch 53/150\n",
      "72/72 - 0s - loss: 109.0250 - mse: 109.0250\n",
      "Epoch 54/150\n",
      "72/72 - 0s - loss: 107.9056 - mse: 107.9056\n",
      "Epoch 55/150\n",
      "72/72 - 0s - loss: 107.9837 - mse: 107.9837\n",
      "Epoch 56/150\n",
      "72/72 - 0s - loss: 107.7430 - mse: 107.7430\n",
      "Epoch 57/150\n",
      "72/72 - 0s - loss: 106.4314 - mse: 106.4314\n",
      "Epoch 58/150\n",
      "72/72 - 0s - loss: 105.8147 - mse: 105.8147\n",
      "Epoch 59/150\n",
      "72/72 - 0s - loss: 105.5053 - mse: 105.5053\n",
      "Epoch 60/150\n",
      "72/72 - 0s - loss: 104.5646 - mse: 104.5646\n",
      "Epoch 61/150\n",
      "72/72 - 0s - loss: 103.9708 - mse: 103.9708\n",
      "Epoch 62/150\n",
      "72/72 - 0s - loss: 104.4398 - mse: 104.4398\n",
      "Epoch 63/150\n",
      "72/72 - 0s - loss: 104.4899 - mse: 104.4899\n",
      "Epoch 64/150\n",
      "72/72 - 0s - loss: 102.7783 - mse: 102.7783\n",
      "Epoch 65/150\n",
      "72/72 - 0s - loss: 101.9061 - mse: 101.9061\n",
      "Epoch 66/150\n",
      "72/72 - 0s - loss: 102.2615 - mse: 102.2615\n",
      "Epoch 67/150\n",
      "72/72 - 0s - loss: 102.2416 - mse: 102.2416\n",
      "Epoch 68/150\n",
      "72/72 - 0s - loss: 100.7687 - mse: 100.7687\n",
      "Epoch 69/150\n",
      "72/72 - 0s - loss: 100.9471 - mse: 100.9471\n",
      "Epoch 70/150\n",
      "72/72 - 0s - loss: 100.1726 - mse: 100.1726\n",
      "Epoch 71/150\n",
      "72/72 - 0s - loss: 99.9858 - mse: 99.9858\n",
      "Epoch 72/150\n",
      "72/72 - 0s - loss: 99.0420 - mse: 99.0420\n",
      "Epoch 73/150\n",
      "72/72 - 0s - loss: 98.4286 - mse: 98.4286\n",
      "Epoch 74/150\n",
      "72/72 - 0s - loss: 98.7924 - mse: 98.7924\n",
      "Epoch 75/150\n",
      "72/72 - 0s - loss: 96.5854 - mse: 96.5854\n",
      "Epoch 76/150\n",
      "72/72 - 0s - loss: 98.5806 - mse: 98.5806\n",
      "Epoch 77/150\n",
      "72/72 - 0s - loss: 97.5714 - mse: 97.5714\n",
      "Epoch 78/150\n",
      "72/72 - 0s - loss: 96.6650 - mse: 96.6650\n",
      "Epoch 79/150\n",
      "72/72 - 0s - loss: 96.8570 - mse: 96.8570\n",
      "Epoch 80/150\n",
      "72/72 - 0s - loss: 96.2174 - mse: 96.2174\n",
      "Epoch 81/150\n",
      "72/72 - 0s - loss: 95.8775 - mse: 95.8775\n",
      "Epoch 82/150\n",
      "72/72 - 0s - loss: 93.3841 - mse: 93.3841\n",
      "Epoch 83/150\n",
      "72/72 - 0s - loss: 95.3081 - mse: 95.3081\n",
      "Epoch 84/150\n",
      "72/72 - 0s - loss: 94.5679 - mse: 94.5679\n",
      "Epoch 85/150\n",
      "72/72 - 0s - loss: 93.7137 - mse: 93.7137\n",
      "Epoch 86/150\n",
      "72/72 - 0s - loss: 90.9681 - mse: 90.9681\n",
      "Epoch 87/150\n",
      "72/72 - 0s - loss: 93.9804 - mse: 93.9804\n",
      "Epoch 88/150\n",
      "72/72 - 0s - loss: 92.9965 - mse: 92.9965\n",
      "Epoch 89/150\n",
      "72/72 - 0s - loss: 92.6543 - mse: 92.6543\n",
      "Epoch 90/150\n",
      "72/72 - 0s - loss: 92.4854 - mse: 92.4854\n",
      "Epoch 91/150\n",
      "72/72 - 0s - loss: 91.9696 - mse: 91.9696\n",
      "Epoch 92/150\n",
      "72/72 - 0s - loss: 91.8919 - mse: 91.8919\n",
      "Epoch 93/150\n",
      "72/72 - 0s - loss: 91.3552 - mse: 91.3552\n",
      "Epoch 94/150\n",
      "72/72 - 0s - loss: 90.2723 - mse: 90.2723\n",
      "Epoch 95/150\n",
      "72/72 - 0s - loss: 90.2312 - mse: 90.2312\n",
      "Epoch 96/150\n",
      "72/72 - 0s - loss: 89.2665 - mse: 89.2665\n",
      "Epoch 97/150\n",
      "72/72 - 0s - loss: 89.0129 - mse: 89.0129\n",
      "Epoch 98/150\n",
      "72/72 - 0s - loss: 89.1923 - mse: 89.1923\n",
      "Epoch 99/150\n",
      "72/72 - 0s - loss: 88.2676 - mse: 88.2676\n",
      "Epoch 100/150\n",
      "72/72 - 0s - loss: 88.2124 - mse: 88.2124\n",
      "Epoch 101/150\n",
      "72/72 - 0s - loss: 86.4827 - mse: 86.4827\n",
      "Epoch 102/150\n",
      "72/72 - 0s - loss: 87.5992 - mse: 87.5992\n",
      "Epoch 103/150\n",
      "72/72 - 0s - loss: 87.8894 - mse: 87.8894\n",
      "Epoch 104/150\n",
      "72/72 - 0s - loss: 86.8484 - mse: 86.8484\n",
      "Epoch 105/150\n",
      "72/72 - 0s - loss: 86.2584 - mse: 86.2584\n",
      "Epoch 106/150\n",
      "72/72 - 0s - loss: 85.2796 - mse: 85.2796\n",
      "Epoch 107/150\n",
      "72/72 - 0s - loss: 85.2536 - mse: 85.2536\n",
      "Epoch 108/150\n",
      "72/72 - 0s - loss: 85.3926 - mse: 85.3926\n",
      "Epoch 109/150\n",
      "72/72 - 0s - loss: 83.9705 - mse: 83.9705\n",
      "Epoch 110/150\n",
      "72/72 - 0s - loss: 83.9537 - mse: 83.9537\n",
      "Epoch 111/150\n",
      "72/72 - 0s - loss: 83.2662 - mse: 83.2662\n",
      "Epoch 112/150\n",
      "72/72 - 0s - loss: 83.1834 - mse: 83.1834\n",
      "Epoch 113/150\n",
      "72/72 - 0s - loss: 82.9230 - mse: 82.9230\n",
      "Epoch 114/150\n",
      "72/72 - 0s - loss: 82.6534 - mse: 82.6534\n",
      "Epoch 115/150\n",
      "72/72 - 0s - loss: 81.5369 - mse: 81.5369\n",
      "Epoch 116/150\n",
      "72/72 - 0s - loss: 81.7247 - mse: 81.7247\n",
      "Epoch 117/150\n",
      "72/72 - 0s - loss: 81.4969 - mse: 81.4969\n",
      "Epoch 118/150\n",
      "72/72 - 0s - loss: 80.6195 - mse: 80.6195\n",
      "Epoch 119/150\n",
      "72/72 - 0s - loss: 80.6168 - mse: 80.6168\n",
      "Epoch 120/150\n",
      "72/72 - 0s - loss: 80.3595 - mse: 80.3595\n",
      "Epoch 121/150\n",
      "72/72 - 0s - loss: 78.7996 - mse: 78.7996\n",
      "Epoch 122/150\n",
      "72/72 - 0s - loss: 79.4508 - mse: 79.4508\n",
      "Epoch 123/150\n",
      "72/72 - 0s - loss: 77.9850 - mse: 77.9850\n",
      "Epoch 124/150\n",
      "72/72 - 0s - loss: 78.4009 - mse: 78.4009\n",
      "Epoch 125/150\n",
      "72/72 - 0s - loss: 78.0800 - mse: 78.0800\n",
      "Epoch 126/150\n",
      "72/72 - 0s - loss: 76.8754 - mse: 76.8754\n",
      "Epoch 127/150\n",
      "72/72 - 0s - loss: 77.6143 - mse: 77.6143\n",
      "Epoch 128/150\n",
      "72/72 - 0s - loss: 74.6563 - mse: 74.6563\n",
      "Epoch 129/150\n",
      "72/72 - 0s - loss: 77.0808 - mse: 77.0808\n",
      "Epoch 130/150\n",
      "72/72 - 0s - loss: 75.3081 - mse: 75.3081\n",
      "Epoch 131/150\n",
      "72/72 - 0s - loss: 75.7943 - mse: 75.7943\n",
      "Epoch 132/150\n",
      "72/72 - 0s - loss: 75.2192 - mse: 75.2192\n",
      "Epoch 133/150\n",
      "72/72 - 0s - loss: 74.8218 - mse: 74.8218\n",
      "Epoch 134/150\n",
      "72/72 - 0s - loss: 74.1899 - mse: 74.1899\n",
      "Epoch 135/150\n",
      "72/72 - 0s - loss: 73.5093 - mse: 73.5093\n",
      "Epoch 136/150\n",
      "72/72 - 0s - loss: 71.0903 - mse: 71.0903\n",
      "Epoch 137/150\n",
      "72/72 - 0s - loss: 71.5356 - mse: 71.5356\n",
      "Epoch 138/150\n",
      "72/72 - 0s - loss: 72.1858 - mse: 72.1858\n",
      "Epoch 139/150\n",
      "72/72 - 0s - loss: 71.2960 - mse: 71.2960\n",
      "Epoch 140/150\n",
      "72/72 - 0s - loss: 71.4109 - mse: 71.4109\n",
      "Epoch 141/150\n",
      "72/72 - 0s - loss: 71.2580 - mse: 71.2580\n",
      "Epoch 142/150\n",
      "72/72 - 0s - loss: 70.9272 - mse: 70.9272\n",
      "Epoch 143/150\n",
      "72/72 - 0s - loss: 69.5151 - mse: 69.5151\n",
      "Epoch 144/150\n",
      "72/72 - 0s - loss: 69.6804 - mse: 69.6804\n",
      "Epoch 145/150\n",
      "72/72 - 0s - loss: 69.7072 - mse: 69.7072\n",
      "Epoch 146/150\n",
      "72/72 - 0s - loss: 68.5138 - mse: 68.5138\n",
      "Epoch 147/150\n",
      "72/72 - 0s - loss: 68.1196 - mse: 68.1196\n",
      "Epoch 148/150\n",
      "72/72 - 0s - loss: 67.4209 - mse: 67.4209\n",
      "Epoch 149/150\n",
      "72/72 - 0s - loss: 66.6903 - mse: 66.6903\n",
      "Epoch 150/150\n",
      "72/72 - 0s - loss: 67.2002 - mse: 67.2002\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 130.0268 - mse: 130.0268\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[130.02679443359375, 130.02679443359375]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define model\n",
    "model_some=Sequential()\n",
    "model_some.add(Dense(32, activation='relu', input_dim=dim_2))\n",
    "model_some.add(Dense(32, activation='relu'))\n",
    "model_some.add(Dense(1, activation='relu'))\n",
    "\n",
    "model_some.compile(loss='mse',optimizer='rmsprop',metrics=['mse'])\n",
    "\n",
    "#fit model\n",
    "history_some=model_some.fit(x_train_some, y_train_some, epochs=150, batch_size=4, verbose=2)\n",
    "model_some.evaluate(x_test_some, y_test_some) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,633\n",
      "Trainable params: 1,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_some.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history_some.history).plot(figsize=(8,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_p_some = model_some.predict(x_train_some)\n",
    "y_test_p_some = model_some.predict(x_test_some)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_train,y_train_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t,p in zip(y_test,y_test_p):\n",
    "    print(t,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_list_some=np.array(y_train_some).flatten().tolist() #y_train_some 리스트\n",
    "y_test_list_some=np.array(y_test_some).flatten().tolist() #y_test_some 리스트\n",
    "y_p_train_list_some=np.array(y_train_p_some).flatten().tolist() #y_train_some 예측 리스트\n",
    "y_p_test_list_some=np.array(y_test_p_some).flatten().tolist() #y_test_some 예측 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#오차 범위 3 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-3 <= y_p_train_list[i] <= y_train_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-3 <= y_p_test_list[i] <= y_test_list[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-3): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-5 <= y_p_train_list[i] <= y_train_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-5 <= y_p_test_list[i] <= y_test_list[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-5): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-10 <= y_p_train_list[i] <= y_train_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-10 <= y_p_test_list[i] <= y_test_list[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-10): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 20 설정\n",
    "scores = 0\n",
    "for i in range(len(y_train)):\n",
    "    if  y_train_list[i]-20 <= y_p_train_list[i] <= y_train_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train)\n",
    "print(\"- train set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test)):\n",
    "    if  y_test_list[i]-20 <= y_p_test_list[i] <= y_test_list[i]+20:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test)\n",
    "print(\"- test set prediction accuracy(+-20): {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_some set prediction accuracy: 50.35 %\n",
      "test_some set prediction accuracy: 56.94 %\n"
     ]
    }
   ],
   "source": [
    "#평균 성능 테스트\n",
    "scores = 0\n",
    "mean=np.mean(Y, axis=0)\n",
    "for i in range(len(y_train_some)):\n",
    "    if  y_train_list_some[i]-10 <= mean <= y_train_list_some[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train_some)\n",
    "print(\"train_some set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "#======================================================================================\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test_some)):\n",
    "    if  y_test_list_some[i]-10 <= mean <= y_test_list_some[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test_some)\n",
    "print(\"test_some set prediction accuracy: {:.2f} %\".format(accuracy*100)) # 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### <오차범위 3>\n",
      "- train_some set prediction accuracy(+-3): 32.99 % <br>\n",
      "- test_some set prediction accuracy(+-3): 27.78 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 5>\n",
      "- train_some set prediction accuracy(+-5): 54.86 % <br>\n",
      "- test_some set prediction accuracy(+-5): 40.28 % <br>\n",
      "<br>\n",
      "\n",
      "### <오차범위 10>\n",
      "- train_some set prediction accuracy(+-10): 82.29 % <br>\n",
      "- test_some set prediction accuracy(+-10): 59.72 % <br>\n",
      "<br>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######입력용#######\n",
    "\n",
    "#오차 범위 3 설정\n",
    "print('### <오차범위 3>')\n",
    "scores = 0\n",
    "for i in range(len(y_train_some)):\n",
    "    if  y_train_list_some[i]-3 <= y_p_train_list_some[i] <= y_train_list_some[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train_some)\n",
    "print(\"- train_some set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test_some)):\n",
    "    if  y_test_list_some[i]-3 <= y_p_test_list_some[i] <= y_test_list_some[i]+3:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test_some)\n",
    "print(\"- test_some set prediction accuracy(+-3): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 5 설정\n",
    "print('### <오차범위 5>')\n",
    "scores = 0\n",
    "for i in range(len(y_train_some)):\n",
    "    if  y_train_list_some[i]-5 <= y_p_train_list_some[i] <= y_train_list_some[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train_some)\n",
    "print(\"- train_some set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test_some)):\n",
    "    if  y_test_list_some[i]-5 <= y_p_test_list_some[i] <= y_test_list_some[i]+5:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test_some)\n",
    "print(\"- test_some set prediction accuracy(+-5): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()\n",
    "#======================================================================================\n",
    "\n",
    "\n",
    "#오차 범위 10 설정\n",
    "print('### <오차범위 10>')\n",
    "scores = 0\n",
    "for i in range(len(y_train_some)):\n",
    "    if  y_train_list_some[i]-10 <= y_p_train_list_some[i] <= y_train_list_some[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_train_some)\n",
    "print(\"- train_some set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "\n",
    "\n",
    "scores = 0\n",
    "for i in range(len(y_test_some)):\n",
    "    if  y_test_list_some[i]-10 <= y_p_test_list_some[i] <= y_test_list_some[i]+10:\n",
    "        scores+=1\n",
    "\n",
    "accuracy=scores/len(y_test_some)\n",
    "print(\"- test_some set prediction accuracy(+-10): {:.2f} % <br>\".format(accuracy*100)) # 예측 정확도\n",
    "print('<br>')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다쓴거\n",
    "### <오차범위 3>\n",
    "- train_all set prediction accuracy(+-3): 88.89 % <br>\n",
    "- test_all set prediction accuracy(+-3): 31.94 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 5>\n",
    "- train_all set prediction accuracy(+-5): 96.53 % <br>\n",
    "- test_all set prediction accuracy(+-5): 55.56 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 10>\n",
    "- train_all set prediction accuracy(+-10): 100.00 % <br>\n",
    "- test_all set prediction accuracy(+-10): 83.33 % <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다안쓴거\n",
    "### <오차범위 3>\n",
    "- train_some set prediction accuracy(+-3): 32.99 % <br>\n",
    "- test_some set prediction accuracy(+-3): 27.78 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 5>\n",
    "- train_some set prediction accuracy(+-5): 54.86 % <br>\n",
    "- test_some set prediction accuracy(+-5): 40.28 % <br>\n",
    "<br>\n",
    "\n",
    "### <오차범위 10>\n",
    "- train_some set prediction accuracy(+-10): 82.29 % <br>\n",
    "- test_some set prediction accuracy(+-10): 59.72 % <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
