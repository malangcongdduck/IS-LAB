{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "import mglearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FONT 깨질때 폰트깨질때\n",
    "from matplotlib import font_manager, rc\n",
    "font_name = font_manager.FontProperties(fname = \"C:/Windows/Fonts/malgun.ttf\").get_name()\n",
    "rc('font',family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\minta\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3343: DtypeWarning: Columns (14,15,2279,2757,3282) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('./최종 데이터 그래프그리기용.xlsx') \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "psqi_df=df[['Sample_ID','GROUP','COHORT','SEX','AGE','TG_1','FatPercentage _1','BMI_1','PSQI_TOTAL_1','TG_2','FatPercentage_2','BMI_2','PSQI_TOTAL_2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "psqi_df=df[['Sample_ID','GROUP','COHORT','SEX','AGE','Insulin _1','FatPercentage _1','TG_1','BMI_1','AST_1','BUN_1',\n",
    "            'PSQI_TOTAL_1','PSQI_C1_1','PSQI_C2_1','PSQI_C3_1','PSQI_C4_1','PSQI_C5_1','PSQI_C6_1','PSQI_C7_1','PSQI_Q1_1','PSQI_Q2_1','PSQI_Q3_1','PSQI_Q4_1',\n",
    "            'PSQI_Q5a_1','PSQI_Q5b_1','PSQI_Q5c_1','PSQI_Q5d_1','PSQI_Q5e_1','PSQI_Q5f_1','PSQI_Q5g_1','PSQI_Q5h_1','PSQI_Q5i_1','PSQI_Q5j_1','PSQI_Q6_1','PSQI_Q7_1','PSQI_Q8_1','PSQI_Q9_1',\n",
    "              'Insulin _2','FatPercentage_2','TG_2','BMI_2','AST_2','BUN_2','PSQI_TOTAL_2','PSQI_C1_2','PSQI_C2_2','PSQI_C3_2','PSQI_C4_2','PSQI_C5_2','PSQI_C6_2','PSQI_C7_2','PSQI_Q1_2','PSQI_Q2_2','PSQI_Q3_2','PSQI_Q4_2',\n",
    "            'PSQI_Q5a_2','PSQI_Q5b_2','PSQI_Q5c_2','PSQI_Q5d_2','PSQI_Q5e_2','PSQI_Q5f_2','PSQI_Q5g_2','PSQI_Q5h_2','PSQI_Q5i_2','PSQI_Q5j_2','PSQI_Q6_2','PSQI_Q7_2','PSQI_Q8_2','PSQI_Q9_2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "psqi_df=df[['Sample_ID','GROUP','COHORT','SEX','AGE','HDL_1','DBP_1','Waist_1','SBP_1','BMI_1','Fat_1_x','PSQI_TOTAL_1','TG_1',\n",
    "           'HDL_2','DBP_2','Waist_2','SBP_2','BMI_2','Fat_2_x','PSQI_TOTAL_2','TG_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df=df[['Sample_ID','GROUP','COHORT','SEX','AGE','Insulin _1','FatPercentage _1','TG_1','BMI_1','AST_1','BUN_1','HDL_1','DBP_1','Waist_1','SBP_1','Fat_1_x','LDL_1',\n",
    "            'PSQI_TOTAL_1','PSQI_C1_1','PSQI_C2_1','PSQI_C3_1','PSQI_C4_1','PSQI_C5_1','PSQI_C6_1','PSQI_C7_1','PSQI_Q1_1','PSQI_Q2_1','PSQI_Q3_1','PSQI_Q4_1',\n",
    "            'PSQI_Q5a_1','PSQI_Q5b_1','PSQI_Q5c_1','PSQI_Q5d_1','PSQI_Q5e_1','PSQI_Q5f_1','PSQI_Q5g_1','PSQI_Q5h_1','PSQI_Q5i_1','PSQI_Q5j_1','PSQI_Q6_1','PSQI_Q7_1','PSQI_Q8_1','PSQI_Q9_1',\n",
    "              'Insulin _2','FatPercentage_2','TG_2','BMI_2','AST_2','BUN_2','HDL_2','DBP_2','Waist_2','SBP_2','Fat_2_x','LDL_2',\n",
    "            'PSQI_TOTAL_2','PSQI_C1_2','PSQI_C2_2','PSQI_C3_2','PSQI_C4_2','PSQI_C5_2','PSQI_C6_2','PSQI_C7_2','PSQI_Q1_2','PSQI_Q2_2','PSQI_Q3_2','PSQI_Q4_2',\n",
    "            'PSQI_Q5a_2','PSQI_Q5b_2','PSQI_Q5c_2','PSQI_Q5d_2','PSQI_Q5e_2','PSQI_Q5f_2','PSQI_Q5g_2','PSQI_Q5h_2','PSQI_Q5i_2','PSQI_Q5j_2','PSQI_Q6_2','PSQI_Q7_2','PSQI_Q8_2','PSQI_Q9_2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>COHORT</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>FatPercentage _1</th>\n",
       "      <th>TG_1</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>AST_1</th>\n",
       "      <th>...</th>\n",
       "      <th>PSQI_Q5e_2</th>\n",
       "      <th>PSQI_Q5f_2</th>\n",
       "      <th>PSQI_Q5g_2</th>\n",
       "      <th>PSQI_Q5h_2</th>\n",
       "      <th>PSQI_Q5i_2</th>\n",
       "      <th>PSQI_Q5j_2</th>\n",
       "      <th>PSQI_Q6_2</th>\n",
       "      <th>PSQI_Q7_2</th>\n",
       "      <th>PSQI_Q8_2</th>\n",
       "      <th>PSQI_Q9_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0001</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>60</td>\n",
       "      <td>7.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>81</td>\n",
       "      <td>21.110190</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0002</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>61</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.5</td>\n",
       "      <td>106</td>\n",
       "      <td>27.782064</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0003</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>52</td>\n",
       "      <td>5.1</td>\n",
       "      <td>39.1</td>\n",
       "      <td>231</td>\n",
       "      <td>24.944742</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>41</td>\n",
       "      <td>4.2</td>\n",
       "      <td>29.1</td>\n",
       "      <td>94</td>\n",
       "      <td>22.620489</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0005</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>41</td>\n",
       "      <td>3.2</td>\n",
       "      <td>24.6</td>\n",
       "      <td>70</td>\n",
       "      <td>20.524157</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>MetS_S0280</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>24</td>\n",
       "      <td>11.3</td>\n",
       "      <td>34.4</td>\n",
       "      <td>51</td>\n",
       "      <td>34.803410</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>MetS_S0281</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>44</td>\n",
       "      <td>10.6</td>\n",
       "      <td>43.8</td>\n",
       "      <td>104</td>\n",
       "      <td>30.903615</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>MetS_S0282</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>37</td>\n",
       "      <td>12.2</td>\n",
       "      <td>35.8</td>\n",
       "      <td>128</td>\n",
       "      <td>28.676533</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>MetS_S0283</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>10.4</td>\n",
       "      <td>26.8</td>\n",
       "      <td>163</td>\n",
       "      <td>24.549738</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>MetS_S0284</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>42</td>\n",
       "      <td>10.1</td>\n",
       "      <td>32.6</td>\n",
       "      <td>90</td>\n",
       "      <td>24.605921</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample_ID GROUP  COHORT SEX  AGE Insulin _1  FatPercentage _1  TG_1  \\\n",
       "0         S0001   SMI       2   M   60        7.7              15.0    81   \n",
       "1         S0002   SMI       2   M   61        5.4              29.5   106   \n",
       "2         S0003   SMI       2   F   52        5.1              39.1   231   \n",
       "3         S0004   SMI       2   F   41        4.2              29.1    94   \n",
       "4         S0005   SMI       2   F   41        3.2              24.6    70   \n",
       "..          ...   ...     ...  ..  ...        ...               ...   ...   \n",
       "383  MetS_S0280  MetS       1   F   24       11.3              34.4    51   \n",
       "384  MetS_S0281  MetS       1   F   44       10.6              43.8   104   \n",
       "385  MetS_S0282  MetS       1   F   37       12.2              35.8   128   \n",
       "386  MetS_S0283  MetS       1   M   51       10.4              26.8   163   \n",
       "387  MetS_S0284  MetS       1   F   42       10.1              32.6    90   \n",
       "\n",
       "         BMI_1  AST_1  ...  PSQI_Q5e_2  PSQI_Q5f_2  PSQI_Q5g_2  PSQI_Q5h_2  \\\n",
       "0    21.110190   21.0  ...         0.0         0.0         0.0         1.0   \n",
       "1    27.782064   29.0  ...         3.0         0.0         2.0         0.0   \n",
       "2    24.944742   16.0  ...         3.0         0.0         3.0         0.0   \n",
       "3    22.620489   16.0  ...         1.0         0.0         0.0         0.0   \n",
       "4    20.524157   26.0  ...         0.0         0.0         0.0         1.0   \n",
       "..         ...    ...  ...         ...         ...         ...         ...   \n",
       "383  34.803410   14.0  ...         NaN         NaN         NaN         NaN   \n",
       "384  30.903615   27.0  ...         NaN         NaN         NaN         NaN   \n",
       "385  28.676533   61.0  ...         NaN         NaN         NaN         NaN   \n",
       "386  24.549738   81.0  ...         NaN         NaN         NaN         NaN   \n",
       "387  24.605921   32.0  ...         NaN         NaN         NaN         NaN   \n",
       "\n",
       "     PSQI_Q5i_2  PSQI_Q5j_2  PSQI_Q6_2  PSQI_Q7_2  PSQI_Q8_2  PSQI_Q9_2  \n",
       "0           0.0         0.0        1.0        0.0        1.0        0.0  \n",
       "1           0.0         0.0        1.0        0.0        2.0        1.0  \n",
       "2           0.0         0.0        1.0        0.0        2.0        0.0  \n",
       "3           0.0         0.0        1.0        0.0        2.0        1.0  \n",
       "4           1.0         1.0        3.0        0.0        2.0        1.0  \n",
       "..          ...         ...        ...        ...        ...        ...  \n",
       "383         NaN         NaN        NaN        NaN        NaN        NaN  \n",
       "384         NaN         NaN        NaN        NaN        NaN        NaN  \n",
       "385         NaN         NaN        NaN        NaN        NaN        NaN  \n",
       "386         NaN         NaN        NaN        NaN        NaN        NaN  \n",
       "387         NaN         NaN        NaN        NaN        NaN        NaN  \n",
       "\n",
       "[388 rows x 81 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_ID</th>\n",
       "      <th>GROUP</th>\n",
       "      <th>COHORT</th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>FatPercentage _1</th>\n",
       "      <th>TG_1</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>AST_1</th>\n",
       "      <th>...</th>\n",
       "      <th>PSQI_Q5e_2</th>\n",
       "      <th>PSQI_Q5f_2</th>\n",
       "      <th>PSQI_Q5g_2</th>\n",
       "      <th>PSQI_Q5h_2</th>\n",
       "      <th>PSQI_Q5i_2</th>\n",
       "      <th>PSQI_Q5j_2</th>\n",
       "      <th>PSQI_Q6_2</th>\n",
       "      <th>PSQI_Q7_2</th>\n",
       "      <th>PSQI_Q8_2</th>\n",
       "      <th>PSQI_Q9_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S0001</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>60</td>\n",
       "      <td>7.7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>81</td>\n",
       "      <td>21.110190</td>\n",
       "      <td>21.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0002</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>61</td>\n",
       "      <td>5.4</td>\n",
       "      <td>29.5</td>\n",
       "      <td>106</td>\n",
       "      <td>27.782064</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S0003</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>52</td>\n",
       "      <td>5.1</td>\n",
       "      <td>39.1</td>\n",
       "      <td>231</td>\n",
       "      <td>24.944742</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0004</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>41</td>\n",
       "      <td>4.2</td>\n",
       "      <td>29.1</td>\n",
       "      <td>94</td>\n",
       "      <td>22.620489</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S0005</td>\n",
       "      <td>SMI</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>41</td>\n",
       "      <td>3.2</td>\n",
       "      <td>24.6</td>\n",
       "      <td>70</td>\n",
       "      <td>20.524157</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>MetS_S0280</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>24</td>\n",
       "      <td>11.3</td>\n",
       "      <td>34.4</td>\n",
       "      <td>51</td>\n",
       "      <td>34.803410</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>MetS_S0281</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>44</td>\n",
       "      <td>10.6</td>\n",
       "      <td>43.8</td>\n",
       "      <td>104</td>\n",
       "      <td>30.903615</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>MetS_S0282</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>37</td>\n",
       "      <td>12.2</td>\n",
       "      <td>35.8</td>\n",
       "      <td>128</td>\n",
       "      <td>28.676533</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>MetS_S0283</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>51</td>\n",
       "      <td>10.4</td>\n",
       "      <td>26.8</td>\n",
       "      <td>163</td>\n",
       "      <td>24.549738</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>MetS_S0284</td>\n",
       "      <td>MetS</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>42</td>\n",
       "      <td>10.1</td>\n",
       "      <td>32.6</td>\n",
       "      <td>90</td>\n",
       "      <td>24.605921</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>317 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample_ID GROUP  COHORT SEX  AGE Insulin _1  FatPercentage _1  TG_1  \\\n",
       "0         S0001   SMI       2   M   60        7.7              15.0    81   \n",
       "1         S0002   SMI       2   M   61        5.4              29.5   106   \n",
       "2         S0003   SMI       2   F   52        5.1              39.1   231   \n",
       "3         S0004   SMI       2   F   41        4.2              29.1    94   \n",
       "4         S0005   SMI       2   F   41        3.2              24.6    70   \n",
       "..          ...   ...     ...  ..  ...        ...               ...   ...   \n",
       "383  MetS_S0280  MetS       1   F   24       11.3              34.4    51   \n",
       "384  MetS_S0281  MetS       1   F   44       10.6              43.8   104   \n",
       "385  MetS_S0282  MetS       1   F   37       12.2              35.8   128   \n",
       "386  MetS_S0283  MetS       1   M   51       10.4              26.8   163   \n",
       "387  MetS_S0284  MetS       1   F   42       10.1              32.6    90   \n",
       "\n",
       "         BMI_1  AST_1  ...  PSQI_Q5e_2  PSQI_Q5f_2  PSQI_Q5g_2  PSQI_Q5h_2  \\\n",
       "0    21.110190   21.0  ...         0.0         0.0         0.0         1.0   \n",
       "1    27.782064   29.0  ...         3.0         0.0         2.0         0.0   \n",
       "2    24.944742   16.0  ...         3.0         0.0         3.0         0.0   \n",
       "3    22.620489   16.0  ...         1.0         0.0         0.0         0.0   \n",
       "4    20.524157   26.0  ...         0.0         0.0         0.0         1.0   \n",
       "..         ...    ...  ...         ...         ...         ...         ...   \n",
       "383  34.803410   14.0  ...         NaN         NaN         NaN         NaN   \n",
       "384  30.903615   27.0  ...         NaN         NaN         NaN         NaN   \n",
       "385  28.676533   61.0  ...         NaN         NaN         NaN         NaN   \n",
       "386  24.549738   81.0  ...         NaN         NaN         NaN         NaN   \n",
       "387  24.605921   32.0  ...         NaN         NaN         NaN         NaN   \n",
       "\n",
       "     PSQI_Q5i_2  PSQI_Q5j_2  PSQI_Q6_2  PSQI_Q7_2  PSQI_Q8_2  PSQI_Q9_2  \n",
       "0           0.0         0.0        1.0        0.0        1.0        0.0  \n",
       "1           0.0         0.0        1.0        0.0        2.0        1.0  \n",
       "2           0.0         0.0        1.0        0.0        2.0        0.0  \n",
       "3           0.0         0.0        1.0        0.0        2.0        1.0  \n",
       "4           1.0         1.0        3.0        0.0        2.0        1.0  \n",
       "..          ...         ...        ...        ...        ...        ...  \n",
       "383         NaN         NaN        NaN        NaN        NaN        NaN  \n",
       "384         NaN         NaN        NaN        NaN        NaN        NaN  \n",
       "385         NaN         NaN        NaN        NaN        NaN        NaN  \n",
       "386         NaN         NaN        NaN        NaN        NaN        NaN  \n",
       "387         NaN         NaN        NaN        NaN        NaN        NaN  \n",
       "\n",
       "[317 rows x 81 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#보조 호르몬 요법을 받고 있는 Cohort 3 제거 Filter 적용\n",
    "psqi_df = psqi_df[(psqi_df['COHORT'] != 3)]\n",
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "psqi_df=psqi_df.dropna()\n",
    "psqi_df.reset_index(drop=True, inplace=True)\n",
    "psqi_df.isnull().sum()\n",
    "psqi_df=psqi_df.drop([\"Sample_ID\", \"GROUP\", \"COHORT\"],axis=1)\n",
    "#1분, 매일다름, 정해진간이없음 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<150 (정상) : 0 -> 중간 값인 80으로 조정\n",
    "#한단계씩 상향 조정\n",
    "#150~199 (약간 높음 - 체중감량과 운동요법) : 1 -> 80~150 (정상)\n",
    "#200~499 (높음 - 생활요법과 함께 약물치료 고려) : 2 -> 150~199 (약간 높음)\n",
    "#>500 (아주 높음 - 즉시 약물치료) : 3 한명도 해당 X -> 200~ (높음~아주높음)\n",
    "\n",
    "psqi_df[\"TG_1\"] = psqi_df[\"TG_1\"].apply(lambda x: 0. if x<80 else 1. if 80<=x<150 else 2. if 150<=x<200 else 3.)\n",
    "psqi_df[\"TG_2\"] = psqi_df[\"TG_2\"].apply(lambda x: 0. if x<80 else 1. if 80<=x<150 else 2. if 150<=x<200 else 3.)\n",
    "psqi_df[\"Insulin _1\"] = psqi_df[\"Insulin _1\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)\n",
    "psqi_df[\"Insulin _2\"] = psqi_df[\"Insulin _1\"].apply(lambda x: 0.1 if x=='<0.2' else 0. if x=='<0.1' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SEX</th>\n",
       "      <th>AGE</th>\n",
       "      <th>Insulin _1</th>\n",
       "      <th>FatPercentage _1</th>\n",
       "      <th>TG_1</th>\n",
       "      <th>BMI_1</th>\n",
       "      <th>AST_1</th>\n",
       "      <th>BUN_1</th>\n",
       "      <th>HDL_1</th>\n",
       "      <th>DBP_1</th>\n",
       "      <th>...</th>\n",
       "      <th>PSQI_Q5e_2</th>\n",
       "      <th>PSQI_Q5f_2</th>\n",
       "      <th>PSQI_Q5g_2</th>\n",
       "      <th>PSQI_Q5h_2</th>\n",
       "      <th>PSQI_Q5i_2</th>\n",
       "      <th>PSQI_Q5j_2</th>\n",
       "      <th>PSQI_Q6_2</th>\n",
       "      <th>PSQI_Q7_2</th>\n",
       "      <th>PSQI_Q8_2</th>\n",
       "      <th>PSQI_Q9_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>35</td>\n",
       "      <td>5.57</td>\n",
       "      <td>23.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.097789</td>\n",
       "      <td>25.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>57</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>46</td>\n",
       "      <td>7.35</td>\n",
       "      <td>20.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.472213</td>\n",
       "      <td>31.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>68</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "      <td>9.26</td>\n",
       "      <td>24.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.744827</td>\n",
       "      <td>30.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>46</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33</td>\n",
       "      <td>3.52</td>\n",
       "      <td>22.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.616175</td>\n",
       "      <td>25.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>96</td>\n",
       "      <td>69.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>2.86</td>\n",
       "      <td>22.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.437500</td>\n",
       "      <td>21.0</td>\n",
       "      <td>9.1</td>\n",
       "      <td>62</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.0</td>\n",
       "      <td>63</td>\n",
       "      <td>4.2</td>\n",
       "      <td>40.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.259585</td>\n",
       "      <td>26.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>55</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>1.0</td>\n",
       "      <td>57</td>\n",
       "      <td>8.8</td>\n",
       "      <td>30.44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.630719</td>\n",
       "      <td>91.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>51</td>\n",
       "      <td>81.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35</td>\n",
       "      <td>6.3</td>\n",
       "      <td>31.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.641274</td>\n",
       "      <td>18.0</td>\n",
       "      <td>11.8</td>\n",
       "      <td>58</td>\n",
       "      <td>75.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.0</td>\n",
       "      <td>61</td>\n",
       "      <td>4.8</td>\n",
       "      <td>27.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.421366</td>\n",
       "      <td>18.0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>58</td>\n",
       "      <td>85.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>1.0</td>\n",
       "      <td>56</td>\n",
       "      <td>9</td>\n",
       "      <td>16.90</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.271653</td>\n",
       "      <td>17.0</td>\n",
       "      <td>38.9</td>\n",
       "      <td>28</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SEX  AGE Insulin _1  FatPercentage _1  TG_1      BMI_1  AST_1  BUN_1  \\\n",
       "0    1.0   35       5.57             23.80   1.0  24.097789   25.0   15.5   \n",
       "1    1.0   46       7.35             20.00   1.0  23.472213   31.0   12.5   \n",
       "2    1.0   32       9.26             24.00   0.0  23.744827   30.0   16.0   \n",
       "3    0.0   33       3.52             22.00   0.0  20.616175   25.0   13.5   \n",
       "4    0.0   28       2.86             22.30   0.0  18.437500   21.0    9.1   \n",
       "..   ...  ...        ...               ...   ...        ...    ...    ...   \n",
       "171  0.0   63        4.2             40.30   0.0  26.259585   26.0   12.0   \n",
       "172  1.0   57        8.8             30.44   0.0  28.630719   91.0   15.0   \n",
       "173  0.0   35        6.3             31.00   1.0  21.641274   18.0   11.8   \n",
       "174  0.0   61        4.8             27.30   0.0  20.421366   18.0   16.2   \n",
       "175  1.0   56          9             16.90   2.0  22.271653   17.0   38.9   \n",
       "\n",
       "     HDL_1  DBP_1  ...  PSQI_Q5e_2  PSQI_Q5f_2  PSQI_Q5g_2  PSQI_Q5h_2  \\\n",
       "0       57   77.0  ...         2.0         0.0         0.0         1.0   \n",
       "1       68   73.0  ...         2.0         2.0         0.0         0.0   \n",
       "2       46   87.0  ...         0.0         0.0         0.0         0.0   \n",
       "3       96   69.0  ...         0.0         0.0         0.0         0.0   \n",
       "4       62   75.0  ...         0.0         0.0         0.0         0.0   \n",
       "..     ...    ...  ...         ...         ...         ...         ...   \n",
       "171     55   74.0  ...         0.0         0.0         0.0         0.0   \n",
       "172     51   81.0  ...         0.0         0.0         0.0         0.0   \n",
       "173     58   75.0  ...         0.0         0.0         0.0         0.0   \n",
       "174     58   85.0  ...         0.0         0.0         0.0         0.0   \n",
       "175     28   83.0  ...         0.0         0.0         0.0         0.0   \n",
       "\n",
       "     PSQI_Q5i_2  PSQI_Q5j_2  PSQI_Q6_2  PSQI_Q7_2  PSQI_Q8_2  PSQI_Q9_2  \n",
       "0           0.0         1.0        1.0        0.0        1.0        0.0  \n",
       "1           0.0         0.0        2.0        0.0        2.0        0.0  \n",
       "2           0.0         0.0        1.0        0.0        0.0        0.0  \n",
       "3           0.0         0.0        1.0        0.0        1.0        1.0  \n",
       "4           1.0         1.0        1.0        0.0        0.0        1.0  \n",
       "..          ...         ...        ...        ...        ...        ...  \n",
       "171         0.0         0.0        1.0        0.0        0.0        0.0  \n",
       "172         1.0         0.0        1.0        0.0        0.0        0.0  \n",
       "173         0.0         0.0        0.0        0.0        0.0        0.0  \n",
       "174         0.0         0.0        2.0        0.0        0.0        0.0  \n",
       "175         0.0         0.0        0.0        0.0        0.0        0.0  \n",
       "\n",
       "[176 rows x 78 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    80\n",
       "1.0    69\n",
       "2.0    14\n",
       "3.0    13\n",
       "Name: TG_1, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df['TG_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    82\n",
       "1.0    71\n",
       "3.0    13\n",
       "2.0    10\n",
       "Name: TG_2, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psqi_df['TG_2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#x 배열 생성 (x=psqi)\n",
    "X1=psqi_df[['PSQI_TOTAL_1','PSQI_C1_1','PSQI_C2_1','PSQI_C3_1','PSQI_C4_1','PSQI_C5_1','PSQI_C6_1','PSQI_C7_1',\n",
    "                           'PSQI_Q1_1','PSQI_Q2_1','PSQI_Q3_1','PSQI_Q4_1','PSQI_Q5a_1','PSQI_Q5b_1','PSQI_Q5c_1','PSQI_Q5d_1','PSQI_Q5e_1','PSQI_Q5f_1','PSQI_Q5g_1','PSQI_Q5h_1','PSQI_Q5i_1','PSQI_Q5j_1',\n",
    "                           'PSQI_Q6_1','PSQI_Q7_1','PSQI_Q8_1','PSQI_Q9_1']].values\n",
    "X2=psqi_df[['PSQI_TOTAL_2','PSQI_C1_2','PSQI_C2_2','PSQI_C3_2','PSQI_C4_2','PSQI_C5_2','PSQI_C6_2','PSQI_C7_2',\n",
    "                           'PSQI_Q1_2','PSQI_Q2_2','PSQI_Q3_2','PSQI_Q4_2','PSQI_Q5a_2','PSQI_Q5b_2','PSQI_Q5c_2','PSQI_Q5d_2','PSQI_Q5e_2','PSQI_Q5f_2','PSQI_Q5g_2','PSQI_Q5h_2','PSQI_Q5i_2','PSQI_Q5j_2',\n",
    "                           'PSQI_Q6_2','PSQI_Q7_2','PSQI_Q8_2','PSQI_Q9_2']].values\n",
    "\n",
    "#y 배열 생성 (y=tg)\n",
    "Y1 = psqi_df['TG_1'].tolist()\n",
    "Y2 = psqi_df['TG_2'].tolist()\n",
    "Y=Y1+Y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#x 배열 생성 (x=임의)\n",
    "X1=psqi_df[['SEX','AGE','Insulin _1','FatPercentage _1','BMI_1','AST_1','BUN_1','PSQI_TOTAL_1','PSQI_C1_1','PSQI_C2_1','PSQI_C3_1','PSQI_C4_1','PSQI_C5_1','PSQI_C6_1','PSQI_C7_1',\n",
    "            'PSQI_Q1_1','PSQI_Q2_1','PSQI_Q3_1','PSQI_Q4_1','PSQI_Q5a_1','PSQI_Q5b_1','PSQI_Q5c_1','PSQI_Q5d_1','PSQI_Q5e_1','PSQI_Q5f_1','PSQI_Q5g_1','PSQI_Q5h_1','PSQI_Q5i_1','PSQI_Q5j_1',\n",
    "            'PSQI_Q6_1','PSQI_Q7_1','PSQI_Q8_1','PSQI_Q9_1']].values\n",
    "X2=psqi_df[['SEX','AGE','Insulin _2','FatPercentage_2','BMI_2','AST_2','BUN_2','PSQI_TOTAL_2','PSQI_C1_2','PSQI_C2_2','PSQI_C3_2','PSQI_C4_2','PSQI_C5_2','PSQI_C6_2','PSQI_C7_2',\n",
    "            'PSQI_Q1_2','PSQI_Q2_2','PSQI_Q3_2','PSQI_Q4_2','PSQI_Q5a_2','PSQI_Q5b_2','PSQI_Q5c_2','PSQI_Q5d_2','PSQI_Q5e_2','PSQI_Q5f_2','PSQI_Q5g_2','PSQI_Q5h_2','PSQI_Q5i_2','PSQI_Q5j_2',\n",
    "            'PSQI_Q6_2','PSQI_Q7_2','PSQI_Q8_2','PSQI_Q9_2']].values\n",
    "\n",
    "#y 배열 생성 (y=tg)\n",
    "Y1 = psqi_df['TG_1'].tolist()\n",
    "Y2 = psqi_df['TG_2'].tolist()\n",
    "Y=Y1+Y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#x 배열 생성 (x=선별)\n",
    "X1=psqi_df[['AGE','HDL_1','DBP_1','Waist_1','SBP_1','BMI_1','Fat_1_x','PSQI_TOTAL_1']].values\n",
    "X2=psqi_df[['AGE','HDL_2','DBP_2','Waist_2','SBP_2','BMI_2','Fat_2_x','PSQI_TOTAL_2']].values\n",
    "X=np.concatenate((X1, X2), axis=0)\n",
    "\n",
    "#y 배열 생성 (y=tg)\n",
    "Y1 = psqi_df['TG_1'].tolist()\n",
    "Y2 = psqi_df['TG_2'].tolist()\n",
    "Y=Y1+Y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#x 배열 생성 (x=임의+선별)\n",
    "X1=psqi_df[['HDL_1','DBP_1','Waist_1','SBP_1','Fat_1_x','Insulin _1','FatPercentage _1','BMI_1','AST_1','BUN_1','LDL_1',\n",
    "            'PSQI_TOTAL_1','PSQI_C1_1','PSQI_C2_1','PSQI_C3_1','PSQI_C4_1','PSQI_C5_1','PSQI_C6_1','PSQI_C7_1','PSQI_Q1_1','PSQI_Q2_1','PSQI_Q3_1','PSQI_Q4_1',\n",
    "            'PSQI_Q5a_1','PSQI_Q5b_1','PSQI_Q5c_1','PSQI_Q5d_1','PSQI_Q5e_1','PSQI_Q5f_1','PSQI_Q5g_1','PSQI_Q5h_1','PSQI_Q5i_1','PSQI_Q5j_1','PSQI_Q6_1','PSQI_Q7_1','PSQI_Q8_1','PSQI_Q9_1']].values\n",
    "\n",
    "X2=psqi_df[['HDL_2','DBP_2','Waist_2','SBP_2','Fat_2_x','Insulin _2','FatPercentage_2','BMI_2','AST_2','BUN_2','LDL_2',\n",
    "            'PSQI_TOTAL_2','PSQI_C1_2','PSQI_C2_2','PSQI_C3_2','PSQI_C4_2','PSQI_C5_2','PSQI_C6_2','PSQI_C7_2','PSQI_Q1_2','PSQI_Q2_2','PSQI_Q3_2','PSQI_Q4_2',\n",
    "            'PSQI_Q5a_2','PSQI_Q5b_2','PSQI_Q5c_2','PSQI_Q5d_2','PSQI_Q5e_2','PSQI_Q5f_2','PSQI_Q5g_2','PSQI_Q5h_2','PSQI_Q5i_2','PSQI_Q5j_2','PSQI_Q6_2','PSQI_Q7_2','PSQI_Q8_2','PSQI_Q9_2']].values\n",
    "X=np.concatenate((X1, X2), axis=0)\n",
    "\n",
    "#y 배열 생성 (y=tg)\n",
    "Y1 = psqi_df['TG_1'].tolist()\n",
    "Y2 = psqi_df['TG_2'].tolist()\n",
    "Y=Y1+Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x 배열 생성 (x=임의+선별)\n",
    "X1=psqi_df[['HDL_1','DBP_1','Waist_1','SBP_1','Fat_1_x','Insulin _1','FatPercentage _1','BMI_1','AST_1','BUN_1','LDL_1',\n",
    "            'PSQI_TOTAL_1','PSQI_C1_1','PSQI_C2_1','PSQI_C3_1','PSQI_C4_1','PSQI_C5_1','PSQI_C6_1','PSQI_C7_1','PSQI_Q1_1','PSQI_Q2_1','PSQI_Q3_1','PSQI_Q4_1',\n",
    "            'PSQI_Q5a_1','PSQI_Q5b_1','PSQI_Q5c_1','PSQI_Q5d_1','PSQI_Q5e_1','PSQI_Q5f_1','PSQI_Q5g_1','PSQI_Q5h_1','PSQI_Q5i_1','PSQI_Q5j_1','PSQI_Q6_1','PSQI_Q7_1','PSQI_Q8_1','PSQI_Q9_1']].values\n",
    "\n",
    "X2=psqi_df[['HDL_2','DBP_2','Waist_2','SBP_2','Fat_2_x','Insulin _2','FatPercentage_2','BMI_2','AST_2','BUN_2','LDL_2',\n",
    "            'PSQI_TOTAL_2','PSQI_C1_2','PSQI_C2_2','PSQI_C3_2','PSQI_C4_2','PSQI_C5_2','PSQI_C6_2','PSQI_C7_2','PSQI_Q1_2','PSQI_Q2_2','PSQI_Q3_2','PSQI_Q4_2',\n",
    "            'PSQI_Q5a_2','PSQI_Q5b_2','PSQI_Q5c_2','PSQI_Q5d_2','PSQI_Q5e_2','PSQI_Q5f_2','PSQI_Q5g_2','PSQI_Q5h_2','PSQI_Q5i_2','PSQI_Q5j_2','PSQI_Q6_2','PSQI_Q7_2','PSQI_Q8_2','PSQI_Q9_2']].values\n",
    "X=np.concatenate((X1, X2), axis=0)\n",
    "\n",
    "#y 배열 생성 (y=tg, sex, age)\n",
    "Y1= psqi_df[['TG_1']].values\n",
    "Y2= psqi_df[['TG_2']].values\n",
    "Y=np.concatenate((Y1, Y2), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#x df 생성 (x=임의+선별) ~수정작업중~\n",
    "X1=psqi_df[['HDL_1','DBP_1','Waist_1','SBP_1','Fat_1_x','Insulin _1','FatPercentage _1','BMI_1','AST_1','BUN_1','LDL_1',\n",
    "            'PSQI_TOTAL_1','PSQI_C1_1','PSQI_C2_1','PSQI_C3_1','PSQI_C4_1','PSQI_C5_1','PSQI_C6_1','PSQI_C7_1','PSQI_Q1_1','PSQI_Q2_1','PSQI_Q3_1','PSQI_Q4_1',\n",
    "            'PSQI_Q5a_1','PSQI_Q5b_1','PSQI_Q5c_1','PSQI_Q5d_1','PSQI_Q5e_1','PSQI_Q5f_1','PSQI_Q5g_1','PSQI_Q5h_1','PSQI_Q5i_1','PSQI_Q5j_1','PSQI_Q6_1','PSQI_Q7_1','PSQI_Q8_1','PSQI_Q9_1']]\n",
    "#예외처리\n",
    "X1=X1.rename({ 'Fat_1_x' : 'Fat_x_1'}, axis=1)\n",
    "X1=X1.rename({ 'FatPercentage _1' : 'FatPercentage_1'}, axis=1)\n",
    "X1=X1.rename({ 'Insulin _1' : 'Insulin_1'}, axis=1)\n",
    "for i in X1.columns:\n",
    "    X1=X1.rename({ i : i[:-2]}, axis=1)\n",
    "\n",
    "X2=psqi_df[['HDL_2','DBP_2','Waist_2','SBP_2','Fat_2_x','Insulin _2','FatPercentage_2','BMI_2','AST_2','BUN_2','LDL_2',\n",
    "            'PSQI_TOTAL_2','PSQI_C1_2','PSQI_C2_2','PSQI_C3_2','PSQI_C4_2','PSQI_C5_2','PSQI_C6_2','PSQI_C7_2','PSQI_Q1_2','PSQI_Q2_2','PSQI_Q3_2','PSQI_Q4_2',\n",
    "            'PSQI_Q5a_2','PSQI_Q5b_2','PSQI_Q5c_2','PSQI_Q5d_2','PSQI_Q5e_2','PSQI_Q5f_2','PSQI_Q5g_2','PSQI_Q5h_2','PSQI_Q5i_2','PSQI_Q5j_2','PSQI_Q6_2','PSQI_Q7_2','PSQI_Q8_2','PSQI_Q9_2']]\n",
    "#예외처리\n",
    "X2=X2.rename({ 'Fat_2_x' : 'Fat_x_2'}, axis=1)\n",
    "X2=X2.rename({ 'Insulin _2' : 'Insulin_2'}, axis=1)\n",
    "for i in X2.columns:\n",
    "    X2=X2.rename({ i : i[:-2]}, axis=1)\n",
    "\n",
    "X=pd.concat((X1, X2), axis=0)\n",
    "\n",
    "\n",
    "#y df 생성 (y=tg, sex, age)\n",
    "Y1= psqi_df[['TG_1', 'SEX', 'AGE']]\n",
    "Y2= psqi_df[['TG_2', 'SEX', 'AGE']]\n",
    "\n",
    "Y1=Y1.rename({'TG_1' : 'TG'}, axis=1)\n",
    "Y2=Y2.rename({'TG_2' : 'TG'}, axis=1)\n",
    "Y=pd.concat((Y1, Y2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "oh_encoder = OneHotEncoder()\n",
    "oh_encoder.fit(Y)\n",
    "oh_labels = oh_encoder.transform(Y)\n",
    "Y = oh_labels.toarray()\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352, 352)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((352, 37), (352, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규화 (변수간의 스케일 차이)\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X=np.asarray(X).astype(np.float)\n",
    "Y=np.asarray(Y).astype(np.float)\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((352, 37), (352, 4))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 2.8202 - accuracy: 0.3879\n",
      "Epoch 2/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.8267 - accuracy: 0.5125\n",
      "Epoch 3/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.5585 - accuracy: 0.4947\n",
      "Epoch 4/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.5170 - accuracy: 0.4947\n",
      "Epoch 5/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.4784 - accuracy: 0.4626\n",
      "Epoch 6/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.3103 - accuracy: 0.5089\n",
      "Epoch 7/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.2766 - accuracy: 0.5125\n",
      "Epoch 8/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.2244 - accuracy: 0.5160\n",
      "Epoch 9/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.2633 - accuracy: 0.5445\n",
      "Epoch 10/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.2055 - accuracy: 0.4982\n",
      "Epoch 11/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.1828 - accuracy: 0.5409\n",
      "Epoch 12/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.1398 - accuracy: 0.4911\n",
      "Epoch 13/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.1651 - accuracy: 0.5338\n",
      "Epoch 14/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.1026 - accuracy: 0.5231\n",
      "Epoch 15/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.1055 - accuracy: 0.5196\n",
      "Epoch 16/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.0598 - accuracy: 0.5516\n",
      "Epoch 17/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0671 - accuracy: 0.6157\n",
      "Epoch 18/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0309 - accuracy: 0.5338\n",
      "Epoch 19/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0445 - accuracy: 0.5516\n",
      "Epoch 20/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.0601 - accuracy: 0.5872\n",
      "Epoch 21/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.0174 - accuracy: 0.5409\n",
      "Epoch 22/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.0105 - accuracy: 0.5801\n",
      "Epoch 23/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 1.0362 - accuracy: 0.5979\n",
      "Epoch 24/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9735 - accuracy: 0.5267\n",
      "Epoch 25/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9905 - accuracy: 0.5409\n",
      "Epoch 26/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9581 - accuracy: 0.5979\n",
      "Epoch 27/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9380 - accuracy: 0.5801\n",
      "Epoch 28/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9453 - accuracy: 0.5872\n",
      "Epoch 29/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9493 - accuracy: 0.6121\n",
      "Epoch 30/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9254 - accuracy: 0.5836\n",
      "Epoch 31/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9270 - accuracy: 0.5872\n",
      "Epoch 32/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9161 - accuracy: 0.6157\n",
      "Epoch 33/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8831 - accuracy: 0.6263\n",
      "Epoch 34/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9240 - accuracy: 0.6014\n",
      "Epoch 35/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9115 - accuracy: 0.6014\n",
      "Epoch 36/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8983 - accuracy: 0.6121\n",
      "Epoch 37/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9029 - accuracy: 0.6050\n",
      "Epoch 38/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8364 - accuracy: 0.6192\n",
      "Epoch 39/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8991 - accuracy: 0.6192\n",
      "Epoch 40/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.9204 - accuracy: 0.5943\n",
      "Epoch 41/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8368 - accuracy: 0.6406\n",
      "Epoch 42/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.8464 - accuracy: 0.6299\n",
      "Epoch 43/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.8705 - accuracy: 0.6014\n",
      "Epoch 44/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.8298 - accuracy: 0.6477\n",
      "Epoch 45/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.8305 - accuracy: 0.6299\n",
      "Epoch 46/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.8217 - accuracy: 0.6263\n",
      "Epoch 47/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.8538 - accuracy: 0.6157\n",
      "Epoch 48/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.7860 - accuracy: 0.6584\n",
      "Epoch 49/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.8217 - accuracy: 0.6512\n",
      "Epoch 50/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.8016 - accuracy: 0.6477\n",
      "Epoch 51/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.7814 - accuracy: 0.6335\n",
      "Epoch 52/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7625 - accuracy: 0.6690\n",
      "Epoch 53/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7700 - accuracy: 0.6406\n",
      "Epoch 54/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7298 - accuracy: 0.6655\n",
      "Epoch 55/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7863 - accuracy: 0.6406\n",
      "Epoch 56/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7384 - accuracy: 0.6655\n",
      "Epoch 57/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.7335 - accuracy: 0.6975\n",
      "Epoch 58/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.7798 - accuracy: 0.6584\n",
      "Epoch 59/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.7482 - accuracy: 0.6762\n",
      "Epoch 60/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7659 - accuracy: 0.6655\n",
      "Epoch 61/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7099 - accuracy: 0.6940\n",
      "Epoch 62/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7095 - accuracy: 0.6833\n",
      "Epoch 63/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6957 - accuracy: 0.6868\n",
      "Epoch 64/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6817 - accuracy: 0.7082\n",
      "Epoch 65/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7309 - accuracy: 0.7082\n",
      "Epoch 66/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6813 - accuracy: 0.7011\n",
      "Epoch 67/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.7270 - accuracy: 0.6868\n",
      "Epoch 68/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6883 - accuracy: 0.7224\n",
      "Epoch 69/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6603 - accuracy: 0.7153\n",
      "Epoch 70/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6687 - accuracy: 0.6904\n",
      "Epoch 71/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6730 - accuracy: 0.7295\n",
      "Epoch 72/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6602 - accuracy: 0.7117\n",
      "Epoch 73/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6212 - accuracy: 0.7331\n",
      "Epoch 74/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6727 - accuracy: 0.6940\n",
      "Epoch 75/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6245 - accuracy: 0.7402\n",
      "Epoch 76/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6495 - accuracy: 0.7224\n",
      "Epoch 77/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6363 - accuracy: 0.7473\n",
      "Epoch 78/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6469 - accuracy: 0.7331\n",
      "Epoch 79/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6358 - accuracy: 0.7260\n",
      "Epoch 80/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.6123 - accuracy: 0.7402\n",
      "Epoch 81/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6091 - accuracy: 0.7367\n",
      "Epoch 82/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6041 - accuracy: 0.7367\n",
      "Epoch 83/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6062 - accuracy: 0.7367\n",
      "Epoch 84/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5759 - accuracy: 0.7438\n",
      "Epoch 85/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6179 - accuracy: 0.7260\n",
      "Epoch 86/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5856 - accuracy: 0.7473\n",
      "Epoch 87/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5756 - accuracy: 0.7544\n",
      "Epoch 88/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.7580\n",
      "Epoch 89/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5496 - accuracy: 0.7794\n",
      "Epoch 90/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5910 - accuracy: 0.7616\n",
      "Epoch 91/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5719 - accuracy: 0.7616\n",
      "Epoch 92/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5566 - accuracy: 0.7722\n",
      "Epoch 93/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5238 - accuracy: 0.7972\n",
      "Epoch 94/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5326 - accuracy: 0.7936\n",
      "Epoch 95/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5534 - accuracy: 0.7687\n",
      "Epoch 96/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5521 - accuracy: 0.7473\n",
      "Epoch 97/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5287 - accuracy: 0.7972\n",
      "Epoch 98/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5307 - accuracy: 0.7794\n",
      "Epoch 99/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5130 - accuracy: 0.7509\n",
      "Epoch 100/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5247 - accuracy: 0.7758\n",
      "Epoch 101/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5365 - accuracy: 0.7936\n",
      "Epoch 102/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7936\n",
      "Epoch 103/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.5069 - accuracy: 0.7865\n",
      "Epoch 104/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.7865\n",
      "Epoch 105/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.7936\n",
      "Epoch 106/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4903 - accuracy: 0.7936\n",
      "Epoch 107/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.8078\n",
      "Epoch 108/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5105 - accuracy: 0.8149\n",
      "Epoch 109/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5021 - accuracy: 0.7865\n",
      "Epoch 110/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4643 - accuracy: 0.8149\n",
      "Epoch 111/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4899 - accuracy: 0.7936\n",
      "Epoch 112/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4770 - accuracy: 0.8043\n",
      "Epoch 113/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.8149\n",
      "Epoch 114/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4805 - accuracy: 0.7829\n",
      "Epoch 115/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4647 - accuracy: 0.8292\n",
      "Epoch 116/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4393 - accuracy: 0.8292\n",
      "Epoch 117/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4516 - accuracy: 0.8292\n",
      "Epoch 118/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4461 - accuracy: 0.8327\n",
      "Epoch 119/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.8434\n",
      "Epoch 120/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4217 - accuracy: 0.8327\n",
      "Epoch 121/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.8292\n",
      "Epoch 122/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3947 - accuracy: 0.8434\n",
      "Epoch 123/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4461 - accuracy: 0.8256\n",
      "Epoch 124/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4320 - accuracy: 0.8292\n",
      "Epoch 125/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4326 - accuracy: 0.8185\n",
      "Epoch 126/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3928 - accuracy: 0.8434\n",
      "Epoch 127/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3972 - accuracy: 0.8541\n",
      "Epoch 128/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.8256\n",
      "Epoch 129/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4053 - accuracy: 0.8577\n",
      "Epoch 130/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3836 - accuracy: 0.8541\n",
      "Epoch 131/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8470\n",
      "Epoch 132/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3972 - accuracy: 0.8399\n",
      "Epoch 133/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3980 - accuracy: 0.8363\n",
      "Epoch 134/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3661 - accuracy: 0.8683\n",
      "Epoch 135/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3782 - accuracy: 0.8505\n",
      "Epoch 136/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3716 - accuracy: 0.8612\n",
      "Epoch 137/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3711 - accuracy: 0.8719\n",
      "Epoch 138/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8541\n",
      "Epoch 139/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3369 - accuracy: 0.8826\n",
      "Epoch 140/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3704 - accuracy: 0.8541\n",
      "Epoch 141/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3529 - accuracy: 0.8612\n",
      "Epoch 142/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3531 - accuracy: 0.8541\n",
      "Epoch 143/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3786 - accuracy: 0.8292\n",
      "Epoch 144/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3320 - accuracy: 0.8612\n",
      "Epoch 145/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8577\n",
      "Epoch 146/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3408 - accuracy: 0.8754\n",
      "Epoch 147/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3266 - accuracy: 0.8612\n",
      "Epoch 148/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3170 - accuracy: 0.8719\n",
      "Epoch 149/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3213 - accuracy: 0.8648\n",
      "Epoch 150/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3081 - accuracy: 0.8826\n",
      "Epoch 151/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3354 - accuracy: 0.8683\n",
      "Epoch 152/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3323 - accuracy: 0.8790\n",
      "Epoch 153/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3399 - accuracy: 0.8861\n",
      "Epoch 154/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3233 - accuracy: 0.8754\n",
      "Epoch 155/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2981 - accuracy: 0.8826\n",
      "Epoch 156/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3054 - accuracy: 0.8897\n",
      "Epoch 157/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3272 - accuracy: 0.8719\n",
      "Epoch 158/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3049 - accuracy: 0.8719\n",
      "Epoch 159/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2871 - accuracy: 0.9004\n",
      "Epoch 160/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.8648\n",
      "Epoch 161/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3005 - accuracy: 0.8826\n",
      "Epoch 162/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.8932\n",
      "Epoch 163/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2763 - accuracy: 0.9004\n",
      "Epoch 164/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8932\n",
      "Epoch 165/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2995 - accuracy: 0.8790\n",
      "Epoch 166/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2655 - accuracy: 0.8754\n",
      "Epoch 167/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2588 - accuracy: 0.8754\n",
      "Epoch 168/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2921 - accuracy: 0.8754\n",
      "Epoch 169/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2831 - accuracy: 0.8968\n",
      "Epoch 170/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2860 - accuracy: 0.8719\n",
      "Epoch 171/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.8968\n",
      "Epoch 172/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2313 - accuracy: 0.9217\n",
      "Epoch 173/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2364 - accuracy: 0.9110\n",
      "Epoch 174/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2345 - accuracy: 0.8861\n",
      "Epoch 175/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.9075\n",
      "Epoch 176/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2582 - accuracy: 0.8897\n",
      "Epoch 177/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2502 - accuracy: 0.9039\n",
      "Epoch 178/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2546 - accuracy: 0.8861\n",
      "Epoch 179/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9181\n",
      "Epoch 180/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2201 - accuracy: 0.9075\n",
      "Epoch 181/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2325 - accuracy: 0.9039\n",
      "Epoch 182/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2230 - accuracy: 0.9181\n",
      "Epoch 183/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2358 - accuracy: 0.9004\n",
      "Epoch 184/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2110 - accuracy: 0.9181\n",
      "Epoch 185/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2379 - accuracy: 0.9110\n",
      "Epoch 186/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.8897\n",
      "Epoch 187/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2453 - accuracy: 0.9146\n",
      "Epoch 188/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2363 - accuracy: 0.9181\n",
      "Epoch 189/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.1999 - accuracy: 0.9253\n",
      "Epoch 190/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2161 - accuracy: 0.9181\n",
      "Epoch 191/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2275 - accuracy: 0.9110\n",
      "Epoch 192/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2012 - accuracy: 0.9146\n",
      "Epoch 193/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2373 - accuracy: 0.9075\n",
      "Epoch 194/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2045 - accuracy: 0.9217\n",
      "Epoch 195/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2334 - accuracy: 0.9181\n",
      "Epoch 196/200\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2087 - accuracy: 0.9288\n",
      "Epoch 197/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2088 - accuracy: 0.9217\n",
      "Epoch 198/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1888 - accuracy: 0.9359\n",
      "Epoch 199/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9181\n",
      "Epoch 200/200\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.9431\n"
     ]
    }
   ],
   "source": [
    "#define model\n",
    "model=Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=37))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "#fit model\n",
    "history=model.fit(x_train, y_train, epochs=200, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step - loss: 2.8134 - accuracy: 0.5070\n",
      "accuracy: 50.70%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측 정확도\n",
    "- <epoch 200, batch_size=4, optimizer=rmsprop> <br>\n",
    "train: 94.31%<br>\n",
    "test: 50.7% <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
